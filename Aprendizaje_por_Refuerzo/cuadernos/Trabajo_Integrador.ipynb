{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìö **Tarea Integradora: An√°lisis y Desarrollo de un Agente de Aprendizaje por Refuerzo**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ **Informaci√≥n General**\n",
        "\n",
        "**Curso**: Introducci√≥n a la Programaci√≥n en Python para Aprendizaje por Refuerzo  \n",
        "**Modalidad**: Google Colaboratory  \n",
        "**Tiempo estimado**: 4-6 horas  \n",
        "**Puntuaci√≥n total**: 100 puntos  \n",
        "\n",
        "---\n",
        "\n",
        "## üìã **Objetivos de la Actividad**\n",
        "\n",
        "- **Integrar conceptos fundamentales** de Python: estructuras de datos, POO, manipulaci√≥n con Pandas/NumPy\n",
        "- **Implementar algoritmos b√°sicos de RL**: Desarrollar un agente Q-Learning simplificado\n",
        "- **Analizar resultados experimentales**: Procesar m√©tricas de rendimiento y generar visualizaciones\n",
        "- **Demostrar competencias** en el ecosistema cient√≠fico de Python\n",
        "\n",
        "---\n",
        "\n",
        "## üîß **Configuraci√≥n del Entorno**"
      ],
      "metadata": {
        "id": "yBVRvJXSPqZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVlY2ssnOwJt"
      },
      "outputs": [],
      "source": [
        "# Importaci√≥n de bibliotecas necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, deque\n",
        "import random\n",
        "from typing import Tuple, List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de visualizaci√≥n\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Semilla para reproducibilidad\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"‚úÖ Bibliotecas importadas correctamente\")\n",
        "print(\"üìä Configuraci√≥n de visualizaci√≥n establecida\")\n",
        "print(\"üé≤ Semilla aleatoria fijada para reproducibilidad\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìù **PARTE A: Implementaci√≥n del Entorno y Agente (40 puntos)**\n",
        "\n",
        "### üåê **Paso 1: Implementar GridWorldEnvironment**\n",
        "\n",
        "**Especificaciones t√©cnicas:**\n",
        "- Cuadr√≠cula 4√ó4 (16 estados: 0-15)\n",
        "- Estado inicial: 0 (esquina superior izquierda)\n",
        "- Objetivo: estado 15 (esquina inferior derecha)\n",
        "- Obst√°culos: estados [5, 7, 11]\n",
        "- Acciones: 0=‚Üë, 1=‚Üí, 2=‚Üì, 3=‚Üê\n",
        "- Recompensas: +10 (objetivo), -1 (obst√°culo), -0.1 (movimiento normal)\n",
        "\n",
        "**Representaci√≥n del grid:**"
      ],
      "metadata": {
        "id": "XZXg675HPwEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementaci√≥n del entorno GridWorld\n",
        "class GridWorldEnvironment:\n",
        "    \"\"\"\n",
        "    Entorno de cuadr√≠cula 4x4 para aprendizaje por refuerzo.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # TODO: Implementar inicializaci√≥n\n",
        "        # Definir: grid_size, n_states, n_actions, obstacles, goal_state, current_state\n",
        "        self.grid_size = 4\n",
        "        self.n_states = 16\n",
        "        self.n_actions = 4\n",
        "        self.obstacles = [5, 7, 11]\n",
        "        self.goal_state = 15\n",
        "        self.current_state = 0\n",
        "\n",
        "        # Definir movimientos: 0=‚Üë, 1=‚Üí, 2=‚Üì, 3=‚Üê\n",
        "        self.actions = {\n",
        "            0: (-1, 0),  # arriba\n",
        "            1: (0, 1),   # derecha\n",
        "            2: (1, 0),   # abajo\n",
        "            3: (0, -1)   # izquierda\n",
        "        }\n",
        "\n",
        "        print(f\"üåê GridWorld 4x4 inicializado\")\n",
        "        print(f\"   Estados: {self.n_states}\")\n",
        "        print(f\"   Obst√°culos: {self.obstacles}\")\n",
        "        print(f\"   Objetivo: {self.goal_state}\")\n",
        "\n",
        "    def reset(self) -> int:\n",
        "        \"\"\"\n",
        "        Reinicia el entorno al estado inicial.\n",
        "        Returns:\n",
        "            int: Estado inicial (0)\n",
        "        \"\"\"\n",
        "        # TODO: Implementar reset\n",
        "        self.current_state = 0\n",
        "        return self.current_state\n",
        "\n",
        "    def step(self, action: int) -> Tuple[int, float, bool, dict]:\n",
        "        \"\"\"\n",
        "        Ejecuta una acci√≥n en el entorno.\n",
        "\n",
        "        Args:\n",
        "            action (int): Acci√≥n a ejecutar (0=‚Üë, 1=‚Üí, 2=‚Üì, 3=‚Üê)\n",
        "\n",
        "        Returns:\n",
        "            Tuple: (next_state, reward, done, info)\n",
        "        \"\"\"\n",
        "        # TODO: Implementar l√≥gica de transici√≥n\n",
        "        # Considerar: validaci√≥n de fronteras, detecci√≥n de obst√°culos, c√°lculo de recompensas\n",
        "\n",
        "        # Obtener coordenadas actuales\n",
        "        row, col = self.state_to_coordinates(self.current_state)\n",
        "\n",
        "        # Calcular nueva posici√≥n\n",
        "        delta_row, delta_col = self.actions[action]\n",
        "        new_row = row + delta_row\n",
        "        new_col = col + delta_col\n",
        "\n",
        "        # Validar fronteras\n",
        "        if new_row < 0 or new_row >= self.grid_size or new_col < 0 or new_col >= self.grid_size:\n",
        "            # Fuera de fronteras, mantener posici√≥n actual\n",
        "            next_state = self.current_state\n",
        "        else:\n",
        "            next_state = self.coordinates_to_state(new_row, new_col)\n",
        "\n",
        "        # Calcular recompensa\n",
        "        if next_state == self.goal_state:\n",
        "            reward = 10.0\n",
        "            done = True\n",
        "        elif next_state in self.obstacles:\n",
        "            reward = -1.0\n",
        "            done = True\n",
        "        else:\n",
        "            reward = -0.1\n",
        "            done = False\n",
        "\n",
        "        # Actualizar estado actual\n",
        "        self.current_state = next_state\n",
        "\n",
        "        info = {\n",
        "            'coordinates': self.state_to_coordinates(next_state),\n",
        "            'action_name': ['‚Üë', '‚Üí', '‚Üì', '‚Üê'][action]\n",
        "        }\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    def get_valid_actions(self, state: int) -> List[int]:\n",
        "        \"\"\"\n",
        "        Obtiene acciones v√°lidas desde un estado dado.\n",
        "\n",
        "        Args:\n",
        "            state (int): Estado actual\n",
        "\n",
        "        Returns:\n",
        "            List[int]: Lista de acciones v√°lidas\n",
        "        \"\"\"\n",
        "        # TODO: Implementar validaci√≥n de acciones\n",
        "        valid_actions = []\n",
        "        row, col = self.state_to_coordinates(state)\n",
        "\n",
        "        for action in range(self.n_actions):\n",
        "            delta_row, delta_col = self.actions[action]\n",
        "            new_row = row + delta_row\n",
        "            new_col = col + delta_col\n",
        "\n",
        "            # Verificar si la nueva posici√≥n est√° dentro de fronteras\n",
        "            if 0 <= new_row < self.grid_size and 0 <= new_col < self.grid_size:\n",
        "                valid_actions.append(action)\n",
        "\n",
        "        return valid_actions\n",
        "\n",
        "    def state_to_coordinates(self, state: int) -> Tuple[int, int]:\n",
        "        \"\"\"Convierte estado a coordenadas (fila, columna).\"\"\"\n",
        "        # TODO: Implementar conversi√≥n estado -> coordenadas\n",
        "        row = state // self.grid_size\n",
        "        col = state % self.grid_size\n",
        "        return row, col\n",
        "\n",
        "    def coordinates_to_state(self, row: int, col: int) -> int:\n",
        "        \"\"\"Convierte coordenadas a estado.\"\"\"\n",
        "        # TODO: Implementar conversi√≥n coordenadas -> estado\n",
        "        return row * self.grid_size + col\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Visualiza el estado actual del entorno.\"\"\"\n",
        "        # TODO: Implementar visualizaci√≥n opcional\n",
        "        print(\"\\nGrid actual:\")\n",
        "        for row in range(self.grid_size):\n",
        "            line = \"\"\n",
        "            for col in range(self.grid_size):\n",
        "                state = self.coordinates_to_state(row, col)\n",
        "                if state == self.current_state:\n",
        "                    line += \"[A] \"  # Agente\n",
        "                elif state == self.goal_state:\n",
        "                    line += \"[G] \"  # Goal\n",
        "                elif state in self.obstacles:\n",
        "                    line += \"[X] \"  # Obst√°culo\n",
        "                else:\n",
        "                    line += \"[ ] \"\n",
        "            print(line)\n",
        "        print()\n",
        "\n",
        "# Prueba inicial del entorno\n",
        "env = GridWorldEnvironment()\n",
        "print(\"Estado inicial:\", env.reset())\n",
        "print(\"Acciones v√°lidas desde estado 0:\", env.get_valid_actions(0))\n",
        "env.render()"
      ],
      "metadata": {
        "id": "KJvlHAwMPyhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ñ **Paso 2: Implementar SimpleQLearningAgent**\n",
        "\n",
        "**Especificaciones del agente:**\n",
        "- Algoritmo: Q-Learning con tabla tabular\n",
        "- Exploraci√≥n: Estrategia epsilon-greedy con decaimiento\n",
        "- Actualizaci√≥n: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]\n",
        "- Par√°metros configurables: Œ±, Œ≥, Œµ, decaimiento de Œµ"
      ],
      "metadata": {
        "id": "dk6ZVlsWP2MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementaci√≥n del agente Q-Learning\n",
        "class SimpleQLearningAgent:\n",
        "    \"\"\"\n",
        "    Agente Q-Learning con exploraci√≥n epsilon-greedy.\n",
        "\n",
        "    Implementa la ecuaci√≥n de actualizaci√≥n:\n",
        "    Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_states: int, n_actions: int,\n",
        "                 learning_rate: float = 0.1, discount_factor: float = 0.95,\n",
        "                 epsilon: float = 0.1, epsilon_decay: float = 0.995,\n",
        "                 epsilon_min: float = 0.01):\n",
        "        \"\"\"\n",
        "        Inicializa el agente Q-Learning.\n",
        "\n",
        "        Args:\n",
        "            n_states: N√∫mero de estados en el entorno\n",
        "            n_actions: N√∫mero de acciones disponibles\n",
        "            learning_rate: Tasa de aprendizaje (Œ±)\n",
        "            discount_factor: Factor de descuento (Œ≥)\n",
        "            epsilon: Probabilidad de exploraci√≥n inicial\n",
        "            epsilon_decay: Factor de decaimiento de epsilon\n",
        "            epsilon_min: Valor m√≠nimo de epsilon\n",
        "        \"\"\"\n",
        "        # TODO: Implementar inicializaci√≥n\n",
        "        # Inicializar: tabla Q, par√°metros de aprendizaje, m√©tricas de seguimiento\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "        # Inicializar tabla Q con ceros\n",
        "        self.q_table = np.zeros((n_states, n_actions))\n",
        "\n",
        "        # M√©tricas de seguimiento\n",
        "        self.td_errors = []\n",
        "        self.training_step = 0\n",
        "\n",
        "        print(f\"ü§ñ Agente Q-Learning inicializado\")\n",
        "        print(f\"   Estados: {n_states}, Acciones: {n_actions}\")\n",
        "        print(f\"   Œ±: {learning_rate}, Œ≥: {discount_factor}\")\n",
        "        print(f\"   Œµ inicial: {epsilon}, decaimiento: {epsilon_decay}\")\n",
        "\n",
        "    def choose_action(self, state: int, training: bool = True) -> int:\n",
        "        \"\"\"\n",
        "        Selecciona acci√≥n usando estrategia epsilon-greedy.\n",
        "\n",
        "        Args:\n",
        "            state: Estado actual\n",
        "            training: Si True, aplica exploraci√≥n; si False, pol√≠tica greedy pura\n",
        "\n",
        "        Returns:\n",
        "            int: Acci√≥n seleccionada\n",
        "        \"\"\"\n",
        "        # TODO: Implementar selecci√≥n de acci√≥n epsilon-greedy\n",
        "        if training and np.random.random() < self.epsilon:\n",
        "            # Exploraci√≥n: acci√≥n aleatoria\n",
        "            return np.random.randint(0, self.n_actions)\n",
        "        else:\n",
        "            # Explotaci√≥n: mejor acci√≥n seg√∫n tabla Q\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "    def update_q_table(self, state: int, action: int, reward: float,\n",
        "                      next_state: int, done: bool) -> float:\n",
        "        \"\"\"\n",
        "        Actualiza la tabla Q usando la regla de Q-Learning.\n",
        "\n",
        "        Args:\n",
        "            state: Estado actual\n",
        "            action: Acci√≥n ejecutada\n",
        "            reward: Recompensa observada\n",
        "            next_state: Estado siguiente\n",
        "            done: Si el episodio termin√≥\n",
        "\n",
        "        Returns:\n",
        "            float: Error de diferencia temporal (TD error)\n",
        "        \"\"\"\n",
        "        # TODO: Implementar actualizaci√≥n Q-Learning\n",
        "        # Calcular: Q_target, TD_error, actualizar Q(s,a)\n",
        "\n",
        "        # Valor Q actual\n",
        "        current_q = self.q_table[state, action]\n",
        "\n",
        "        # Calcular target\n",
        "        if done:\n",
        "            target_q = reward\n",
        "        else:\n",
        "            target_q = reward + self.discount_factor * np.max(self.q_table[next_state])\n",
        "\n",
        "        # Error de diferencia temporal\n",
        "        td_error = target_q - current_q\n",
        "\n",
        "        # Actualizar tabla Q\n",
        "        self.q_table[state, action] += self.learning_rate * td_error\n",
        "\n",
        "        # Registrar m√©tricas\n",
        "        self.td_errors.append(abs(td_error))\n",
        "        self.training_step += 1\n",
        "\n",
        "        return abs(td_error)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"Reduce epsilon gradualmente.\"\"\"\n",
        "        # TODO: Implementar decaimiento de epsilon\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def get_policy(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Extrae pol√≠tica determin√≠stica de la tabla Q.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Matriz de pol√≠tica [n_states x n_actions]\n",
        "        \"\"\"\n",
        "        # TODO: Implementar extracci√≥n de pol√≠tica\n",
        "        policy = np.zeros((self.n_states, self.n_actions))\n",
        "        for state in range(self.n_states):\n",
        "            best_action = np.argmax(self.q_table[state])\n",
        "            policy[state, best_action] = 1.0\n",
        "        return policy\n",
        "\n",
        "    def get_value_function(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Calcula funci√≥n de valor V(s) = max_a Q(s,a).\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Vector de funci√≥n de valor\n",
        "        \"\"\"\n",
        "        # TODO: Implementar c√°lculo de funci√≥n de valor\n",
        "        return np.max(self.q_table, axis=1)\n",
        "\n",
        "# Prueba inicial del agente\n",
        "agent = SimpleQLearningAgent(n_states=16, n_actions=4, epsilon=0.3)\n",
        "print(\"Acci√≥n inicial desde estado 0:\", agent.choose_action(0))\n",
        "print(\"Forma de la tabla Q:\", agent.q_table.shape)\n",
        "print(\"Funci√≥n de valor inicial:\", agent.get_value_function()[:5])"
      ],
      "metadata": {
        "id": "abiE8SoLP5OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üß™ **PARTE B: Experimentaci√≥n y An√°lisis de Datos (35 puntos)**\n",
        "\n",
        "### üìä **Paso 3: Script de Entrenamiento y Recolecci√≥n de Datos**\n",
        "\n",
        "En esta secci√≥n implementaremos el loop de entrenamiento que:\n",
        "- Ejecuta 1000 episodios de entrenamiento\n",
        "- Registra m√©tricas de rendimiento por episodio\n",
        "- Aplica decaimiento gradual de epsilon\n",
        "- Genera reportes de progreso peri√≥dicos"
      ],
      "metadata": {
        "id": "Y_jsrnR3P72o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci√≥n de entrenamiento\n",
        "def train_agent(env, agent, n_episodes: int = 1000, max_steps: int = 100) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Entrena el agente y recolecta m√©tricas de rendimiento.\n",
        "\n",
        "    Args:\n",
        "        env: Entorno de entrenamiento\n",
        "        agent: Agente a entrenar\n",
        "        n_episodes: N√∫mero de episodios\n",
        "        max_steps: M√°ximo de pasos por episodio\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: M√©tricas de entrenamiento\n",
        "    \"\"\"\n",
        "\n",
        "    # Contenedores para m√©tricas\n",
        "    metrics = {\n",
        "        'episode': [],\n",
        "        'reward': [],\n",
        "        'steps': [],\n",
        "        'epsilon': [],\n",
        "        'states_visited': [],\n",
        "        'td_errors': []\n",
        "    }\n",
        "\n",
        "    print(f\"üöÄ Iniciando entrenamiento: {n_episodes} episodios\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for episode in range(n_episodes):\n",
        "        # TODO: Implementar loop de entrenamiento\n",
        "        # 1. Reset del entorno\n",
        "        state = env.reset()\n",
        "\n",
        "        # 2. Loop del episodio hasta terminar o max_steps\n",
        "        episode_reward = 0\n",
        "        episode_steps = 0\n",
        "        episode_states = set()\n",
        "        episode_td_errors = []\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            # Seleccionar acci√≥n\n",
        "            action = agent.choose_action(state, training=True)\n",
        "\n",
        "            # Ejecutar acci√≥n\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Actualizar agente\n",
        "            td_error = agent.update_q_table(state, action, reward, next_state, done)\n",
        "\n",
        "            # Registrar m√©tricas del paso\n",
        "            episode_reward += reward\n",
        "            episode_steps += 1\n",
        "            episode_states.add(state)\n",
        "            episode_td_errors.append(td_error)\n",
        "\n",
        "            # Preparar siguiente iteraci√≥n\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # 3. Registrar m√©tricas del episodio\n",
        "        metrics['episode'].append(episode)\n",
        "        metrics['reward'].append(episode_reward)\n",
        "        metrics['steps'].append(episode_steps)\n",
        "        metrics['epsilon'].append(agent.epsilon)\n",
        "        metrics['states_visited'].append(len(episode_states))\n",
        "        metrics['td_errors'].append(np.mean(episode_td_errors) if episode_td_errors else 0)\n",
        "\n",
        "        # 4. Aplicar decaimiento de epsilon\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        # 5. Reportar progreso cada 100 episodios\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(metrics['reward'][-100:])\n",
        "            avg_steps = np.mean(metrics['steps'][-100:])\n",
        "            print(f\"Episodio {episode + 1:4d} | \"\n",
        "                  f\"Recompensa promedio: {avg_reward:6.2f} | \"\n",
        "                  f\"Pasos promedio: {avg_steps:5.1f} | \"\n",
        "                  f\"Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "    # TODO: Crear DataFrame con m√©tricas\n",
        "    df = pd.DataFrame(metrics)\n",
        "    df['moving_average'] = df['reward'].rolling(window=50, min_periods=1).mean()\n",
        "\n",
        "    print(f\"\\n‚úÖ Entrenamiento completado\")\n",
        "    print(f\"üìä Datos recolectados: {len(df)} episodios\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Ejecuci√≥n del entrenamiento\n",
        "print(\"üéÆ Creando nuevo entorno y agente para entrenamiento...\")\n",
        "env = GridWorldEnvironment()\n",
        "agent = SimpleQLearningAgent(n_states=16, n_actions=4,\n",
        "                           learning_rate=0.1,\n",
        "                           epsilon=0.3,\n",
        "                           epsilon_decay=0.995)\n",
        "\n",
        "# Ejecutar entrenamiento\n",
        "training_data = train_agent(env, agent, n_episodes=1000)"
      ],
      "metadata": {
        "id": "nnxhanczP-6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìà **Paso 4: Procesamiento y An√°lisis de Datos**\n",
        "\n",
        "Ahora analizaremos estad√≠sticamente los datos recolectados durante el entrenamiento:\n",
        "- Estad√≠sticas descriptivas b√°sicas\n",
        "- Detecci√≥n de convergencia\n",
        "- An√°lisis de distribuciones\n",
        "- Correlaciones entre variables"
      ],
      "metadata": {
        "id": "9vz6DTPGQB5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An√°lisis estad√≠stico de los datos\n",
        "def analyze_training_data(df: pd.DataFrame) -> Dict:\n",
        "    \"\"\"\n",
        "    Realiza an√°lisis estad√≠stico comprehensivo de los datos de entrenamiento.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame con m√©tricas de entrenamiento\n",
        "\n",
        "    Returns:\n",
        "        Dict: Estad√≠sticas de an√°lisis\n",
        "    \"\"\"\n",
        "\n",
        "    analysis = {}\n",
        "\n",
        "    # TODO: Implementar an√°lisis estad√≠stico\n",
        "    # 1. Estad√≠sticas descriptivas b√°sicas\n",
        "    analysis['total_episodes'] = len(df)\n",
        "    analysis['final_average_reward'] = df['reward'].tail(100).mean()\n",
        "    analysis['best_episode_reward'] = df['reward'].max()\n",
        "    analysis['worst_episode_reward'] = df['reward'].min()\n",
        "\n",
        "    # 2. Tasa de convergencia (episodio donde se estabiliza)\n",
        "    # Considerar convergencia cuando la media m√≥vil se estabiliza\n",
        "    moving_avg = df['moving_average'].values\n",
        "    differences = np.abs(np.diff(moving_avg))\n",
        "    convergence_threshold = 0.1\n",
        "\n",
        "    convergence_episode = None\n",
        "    for i in range(100, len(differences)):\n",
        "        if np.all(differences[i-20:i] < convergence_threshold):\n",
        "            convergence_episode = i\n",
        "            break\n",
        "\n",
        "    analysis['convergence_episode'] = convergence_episode if convergence_episode else \"No detectada\"\n",
        "\n",
        "    # 3. An√°lisis de cuartiles para longitud de episodios\n",
        "    steps_quartiles = np.percentile(df['steps'], [25, 50, 75])\n",
        "    analysis['steps_q1'] = steps_quartiles[0]\n",
        "    analysis['steps_median'] = steps_quartiles[1]\n",
        "    analysis['steps_q3'] = steps_quartiles[2]\n",
        "\n",
        "    # 4. Correlaciones entre variables\n",
        "    correlation_matrix = df[['reward', 'steps', 'epsilon', 'states_visited']].corr()\n",
        "    analysis['reward_steps_correlation'] = correlation_matrix.loc['reward', 'steps']\n",
        "    analysis['reward_epsilon_correlation'] = correlation_matrix.loc['reward', 'epsilon']\n",
        "\n",
        "    # 5. Estad√≠sticas de exploraci√≥n\n",
        "    analysis['initial_epsilon'] = df['epsilon'].iloc[0]\n",
        "    analysis['final_epsilon'] = df['epsilon'].iloc[-1]\n",
        "    analysis['average_states_visited'] = df['states_visited'].mean()\n",
        "\n",
        "    # 6. Tendencias de mejora\n",
        "    first_half_reward = df['reward'].iloc[:len(df)//2].mean()\n",
        "    second_half_reward = df['reward'].iloc[len(df)//2:].mean()\n",
        "    analysis['improvement_percentage'] = ((second_half_reward - first_half_reward) / abs(first_half_reward)) * 100\n",
        "\n",
        "    return analysis\n",
        "\n",
        "# Ejecutar an√°lisis\n",
        "analysis_results = analyze_training_data(training_data)\n",
        "\n",
        "print(\"üìä AN√ÅLISIS ESTAD√çSTICO DE ENTRENAMIENTO\")\n",
        "print(\"=\" * 50)\n",
        "for key, value in analysis_results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key}: {value:.3f}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "# Mostrar estad√≠sticas descriptivas del DataFrame\n",
        "print(\"\\nüìà ESTAD√çSTICAS DESCRIPTIVAS\")\n",
        "print(\"=\" * 30)\n",
        "print(training_data[['reward', 'steps', 'epsilon', 'states_visited']].describe())"
      ],
      "metadata": {
        "id": "QU9ARJ06QEmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìä **PARTE C: Visualizaci√≥n y Interpretaci√≥n (25 puntos)**\n",
        "\n",
        "### üé® **Paso 5: Generaci√≥n de Visualizaciones**\n",
        "\n",
        "Crearemos un conjunto comprehensivo de visualizaciones que incluye:\n",
        "1. **Curva de aprendizaje**: Recompensa por episodio + media m√≥vil\n",
        "2. **Eficiencia del agente**: Evoluci√≥n de pasos por episodio\n",
        "3. **Exploraci√≥n**: Decaimiento de epsilon durante entrenamiento\n",
        "4. **Mapa de calor**: Pol√≠tica final aprendida en la cuadr√≠cula 4x4"
      ],
      "metadata": {
        "id": "AaShPiLtQHEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funci√≥n de visualizaci√≥n comprehensiva\n",
        "def create_comprehensive_plots(df: pd.DataFrame, agent, env):\n",
        "    \"\"\"\n",
        "    Genera visualizaciones comprehensivas del proceso de entrenamiento.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame con m√©tricas de entrenamiento\n",
        "        agent: Agente entrenado\n",
        "        env: Entorno utilizado\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # TODO: Implementar cada subgr√°fico\n",
        "\n",
        "    # 1. Curva de aprendizaje (superior izquierdo)\n",
        "    axes[0,0].plot(df['episode'], df['reward'], alpha=0.3, color='lightblue', label='Recompensa por episodio')\n",
        "    axes[0,0].plot(df['episode'], df['moving_average'], color='darkblue', linewidth=2, label='Media m√≥vil (50 ep.)')\n",
        "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='L√≠nea base')\n",
        "    axes[0,0].set_title('Curva de Aprendizaje', fontweight='bold', fontsize=12)\n",
        "    axes[0,0].set_xlabel('Episodio')\n",
        "    axes[0,0].set_ylabel('Recompensa Acumulada')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Eficiencia del agente (superior derecho)\n",
        "    axes[0,1].plot(df['episode'], df['steps'], color='green', alpha=0.6, label='Pasos por episodio')\n",
        "    # Agregar l√≠nea de tendencia\n",
        "    z = np.polyfit(df['episode'], df['steps'], 1)\n",
        "    p = np.poly1d(z)\n",
        "    axes[0,1].plot(df['episode'], p(df['episode']), color='darkgreen', linewidth=2, linestyle='--', label='Tendencia')\n",
        "    axes[0,1].set_title('Eficiencia del Agente', fontweight='bold', fontsize=12)\n",
        "    axes[0,1].set_xlabel('Episodio')\n",
        "    axes[0,1].set_ylabel('Pasos hasta Terminaci√≥n')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Exploraci√≥n (inferior izquierdo)\n",
        "    axes[1,0].plot(df['episode'], df['epsilon'], color='orange', linewidth=2)\n",
        "    axes[1,0].axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='Exploraci√≥n baja')\n",
        "    axes[1,0].axhline(y=0.3, color='yellow', linestyle='--', alpha=0.7, label='Exploraci√≥n alta')\n",
        "    axes[1,0].set_title('Evoluci√≥n de la Exploraci√≥n (Œµ)', fontweight='bold', fontsize=12)\n",
        "    axes[1,0].set_xlabel('Episodio')\n",
        "    axes[1,0].set_ylabel('Epsilon')\n",
        "    axes[1,0].legend()\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Mapa de calor de pol√≠tica (inferior derecho)\n",
        "    policy = agent.get_policy()\n",
        "    policy_grid = np.zeros((4, 4))\n",
        "\n",
        "    # Convertir pol√≠tica a grid 4x4 con direcciones\n",
        "    action_arrows = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']\n",
        "    policy_arrows = np.empty((4, 4), dtype=object)\n",
        "\n",
        "    for state in range(16):\n",
        "        row, col = env.state_to_coordinates(state)\n",
        "        best_action = np.argmax(policy[state])\n",
        "        policy_grid[row, col] = best_action\n",
        "        policy_arrows[row, col] = action_arrows[best_action]\n",
        "\n",
        "    # Crear mapa de calor\n",
        "    im = axes[1,1].imshow(policy_grid, cmap='viridis', alpha=0.7)\n",
        "\n",
        "    # Agregar texto con direcciones y estados especiales\n",
        "    for row in range(4):\n",
        "        for col in range(4):\n",
        "            state = env.coordinates_to_state(row, col)\n",
        "            if state == 0:  # Estado inicial\n",
        "                text = f\"START\\n{policy_arrows[row, col]}\"\n",
        "                color = 'white'\n",
        "            elif state == 15:  # Objetivo\n",
        "                text = \"GOAL\"\n",
        "                color = 'yellow'\n",
        "            elif state in env.obstacles:  # Obst√°culos\n",
        "                text = \"XXX\"\n",
        "                color = 'red'\n",
        "            else:\n",
        "                text = policy_arrows[row, col]\n",
        "                color = 'white'\n",
        "\n",
        "            axes[1,1].text(col, row, text, ha='center', va='center',\n",
        "                         color=color, fontweight='bold', fontsize=10)\n",
        "\n",
        "    axes[1,1].set_title('Pol√≠tica Final Aprendida', fontweight='bold', fontsize=12)\n",
        "    axes[1,1].set_xlabel('Columna')\n",
        "    axes[1,1].set_ylabel('Fila')\n",
        "\n",
        "    # Configurar ticks\n",
        "    axes[1,1].set_xticks(range(4))\n",
        "    axes[1,1].set_yticks(range(4))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('An√°lisis Comprehensivo: Entrenamiento Q-Learning GridWorld 4x4',\n",
        "                 fontsize=16, fontweight='bold', y=0.98)\n",
        "    plt.show()\n",
        "\n",
        "# Ejecutar visualizaci√≥n\n",
        "create_comprehensive_plots(training_data, agent, env)"
      ],
      "metadata": {
        "id": "lK3EGC8EQJTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç **Paso 6: An√°lisis Interpretativo**\n",
        "\n",
        "**INSTRUCCIONES**: Complete el siguiente an√°lisis con un m√≠nimo de 200 palabras, abordando cada punto solicitado. Sustente sus observaciones con los resultados obtenidos en su entrenamiento.\n",
        "\n",
        "---\n",
        "\n",
        "## üìù **AN√ÅLISIS E INTERPRETACI√ìN DE RESULTADOS**\n",
        "\n",
        "### **1. Patrones de Convergencia Observados:**\n",
        "[TODO: Describa c√≥mo evolucion√≥ la recompensa a lo largo del entrenamiento. ¬øEn qu√© episodio aproximado observa estabilizaci√≥n? ¬øHubo fluctuaciones significativas? Mencione valores espec√≠ficos de su entrenamiento.]\n",
        "\n",
        "### **2. Efectividad de la Estrategia Epsilon-Greedy:**\n",
        "[TODO: Analice c√≥mo el balance entre exploraci√≥n y explotaci√≥n afect√≥ el aprendizaje. ¬øFue apropiado el decaimiento de epsilon? ¬øQu√© mejoras propondr√≠a? Considere los valores inicial y final de epsilon.]\n",
        "\n",
        "### **3. Calidad de la Pol√≠tica Aprendida:**\n",
        "[TODO: Eval√∫e la pol√≠tica final mostrada en el mapa de calor. ¬øEl agente aprendi√≥ una estrategia √≥ptima para llegar al objetivo? ¬øEvita obst√°culos eficientemente? ¬øHay direcciones que parecen sub√≥ptimas?]\n",
        "\n",
        "### **4. Limitaciones del Enfoque Tabular:**\n",
        "[TODO: Reflexione sobre las limitaciones de Q-Learning tabular. ¬øC√≥mo escalar√≠a este enfoque a problemas con espacios de estados m√°s grandes (ej: 100x100)? ¬øQu√© alternativas conoce (aproximaci√≥n funcional, redes neuronales)?]\n",
        "\n",
        "### **5. Observaciones Adicionales:**\n",
        "[TODO: Incluya cualquier insight adicional sobre el comportamiento del agente, patrones inesperados, relaci√≥n entre m√©tricas, o mejoras potenciales al algoritmo implementado.]\n",
        "\n",
        "---\n",
        "\n",
        "**Extensi√≥n m√≠nima requerida**: 200 palabras  \n",
        "**Criterios de evaluaci√≥n**: Profundidad del an√°lisis, conexi√≥n con resultados obtenidos, pensamiento cr√≠tico, propuestas de mejora"
      ],
      "metadata": {
        "id": "_jTxsSAOQLqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda para que los estudiantes escriban su an√°lisis interpretativo\n",
        "analysis_text = \"\"\"\n",
        "# üìã MI AN√ÅLISIS INTERPRETATIVO\n",
        "\n",
        "## 1. Patrones de Convergencia Observados\n",
        "[Escriba aqu√≠ su an√°lisis basado en los gr√°ficos generados...]\n",
        "\n",
        "## 2. Efectividad de la Estrategia Epsilon-Greedy\n",
        "[Escriba aqu√≠ su an√°lisis sobre la exploraci√≥n vs explotaci√≥n...]\n",
        "\n",
        "## 3. Calidad de la Pol√≠tica Aprendida\n",
        "[Escriba aqu√≠ su evaluaci√≥n de la pol√≠tica final...]\n",
        "\n",
        "## 4. Limitaciones del Enfoque Tabular\n",
        "[Escriba aqu√≠ sus reflexiones sobre escalabilidad...]\n",
        "\n",
        "## 5. Observaciones Adicionales\n",
        "[Escriba aqu√≠ insights adicionales...]\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìù Secci√≥n de an√°lisis preparada\")\n",
        "print(\"‚úèÔ∏è Complete su an√°lisis interpretativo arriba\")\n",
        "print(\"üí° Recuerde: m√≠nimo 200 palabras, conectar con sus resultados espec√≠ficos\")"
      ],
      "metadata": {
        "id": "5KQxe9CdQOXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìä **Evaluaci√≥n y M√©tricas Finales**\n",
        "\n",
        "### **Paso 7: Evaluaci√≥n del Rendimiento Final**\n",
        "\n",
        "Evaluaremos el agente entrenado ejecutando episodios sin exploraci√≥n (Œµ=0) para medir su rendimiento real."
      ],
      "metadata": {
        "id": "J2-1FlL3QQsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# M√©tricas de evaluaci√≥n final\n",
        "def evaluate_final_performance(agent, env, n_test_episodes: int = 100) -> Dict:\n",
        "    \"\"\"\n",
        "    Eval√∫a el rendimiento final del agente entrenado.\n",
        "\n",
        "    Args:\n",
        "        agent: Agente entrenado\n",
        "        env: Entorno de evaluaci√≥n\n",
        "        n_test_episodes: N√∫mero de episodios de prueba\n",
        "\n",
        "    Returns:\n",
        "        Dict: M√©tricas de rendimiento final\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Implementar evaluaci√≥n final\n",
        "    success_count = 0\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "    outcomes = []\n",
        "\n",
        "    print(f\"üéØ Evaluando rendimiento final: {n_test_episodes} episodios sin exploraci√≥n\")\n",
        "\n",
        "    for episode in range(n_test_episodes):\n",
        "        # Reiniciar entorno\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_steps = 0\n",
        "\n",
        "        # Ejecutar episodio sin exploraci√≥n\n",
        "        for step in range(100):  # M√°ximo 100 pasos\n",
        "            action = agent.choose_action(state, training=False)  # Sin exploraci√≥n\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            episode_reward += reward\n",
        "            episode_steps += 1\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                if reward > 0:  # Lleg√≥ al objetivo\n",
        "                    success_count += 1\n",
        "                    outcomes.append(\"√âxito\")\n",
        "                else:  # Cay√≥ en obst√°culo\n",
        "                    outcomes.append(\"Obst√°culo\")\n",
        "                break\n",
        "        else:\n",
        "            outcomes.append(\"Timeout\")  # No termin√≥ en tiempo l√≠mite\n",
        "\n",
        "        total_rewards.append(episode_reward)\n",
        "        total_steps.append(episode_steps)\n",
        "\n",
        "    # Calcular m√©tricas\n",
        "    metrics = {\n",
        "        'success_rate': success_count / n_test_episodes,\n",
        "        'average_reward': np.mean(total_rewards),\n",
        "        'std_reward': np.std(total_rewards),\n",
        "        'average_steps': np.mean(total_steps),\n",
        "        'std_steps': np.std(total_steps),\n",
        "        'best_reward': np.max(total_rewards),\n",
        "        'worst_reward': np.min(total_rewards),\n",
        "        'success_episodes': success_count,\n",
        "        'total_episodes': n_test_episodes\n",
        "    }\n",
        "\n",
        "    # Mostrar distribuci√≥n de resultados\n",
        "    outcome_counts = {outcome: outcomes.count(outcome) for outcome in set(outcomes)}\n",
        "    metrics['outcome_distribution'] = outcome_counts\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Ejecutar evaluaci√≥n final\n",
        "final_metrics = evaluate_final_performance(agent, env, n_test_episodes=100)\n",
        "\n",
        "print(\"\\nüéØ M√âTRICAS DE RENDIMIENTO FINAL\")\n",
        "print(\"=\" * 40)\n",
        "for metric, value in final_metrics.items():\n",
        "    if isinstance(value, float):\n",
        "        if metric.endswith('_rate'):\n",
        "            print(f\"{metric}: {value:.1%}\")\n",
        "        else:\n",
        "            print(f\"{metric}: {value:.3f}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value}\")\n",
        "\n",
        "# Visualizaci√≥n adicional: distribuci√≥n de recompensas finales\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "rewards_eval = []\n",
        "steps_eval = []\n",
        "for _ in range(100):\n",
        "    state = env.reset()\n",
        "    ep_reward = 0\n",
        "    ep_steps = 0\n",
        "    for step in range(100):\n",
        "        action = agent.choose_action(state, training=False)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "        ep_steps += 1\n",
        "        if done:\n",
        "            break\n",
        "    rewards_eval.append(ep_reward)\n",
        "    steps_eval.append(ep_steps)\n",
        "\n",
        "plt.hist(rewards_eval, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.title('Distribuci√≥n de Recompensas (Evaluaci√≥n Final)')\n",
        "plt.xlabel('Recompensa por Episodio')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(steps_eval, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "plt.title('Distribuci√≥n de Pasos (Evaluaci√≥n Final)')\n",
        "plt.xlabel('Pasos por Episodio')\n",
        "plt.ylabel('Frecuencia')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìà Recompensa promedio en evaluaci√≥n: {np.mean(rewards_eval):.2f}\")\n",
        "print(f\"üìä Pasos promedio en evaluaci√≥n: {np.mean(steps_eval):.1f}\")"
      ],
      "metadata": {
        "id": "w0LrDuw7QSrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üèÜ **Criterios de Evaluaci√≥n y Lista de Verificaci√≥n**\n",
        "\n",
        "### **Criterios de Evaluaci√≥n**\n",
        "\n",
        "| **Componente** | **Puntuaci√≥n** | **Criterios de Evaluaci√≥n** |\n",
        "|----------------|----------------|------------------------------|\n",
        "| **Implementaci√≥n POO** | 20 pts | Clases bien estructuradas, m√©todos apropiados, documentaci√≥n clara |\n",
        "| **Algoritmo Q-Learning** | 20 pts | Implementaci√≥n matem√°ticamente correcta, manejo apropiado de par√°metros |\n",
        "| **Procesamiento de Datos** | 15 pts | Uso efectivo de Pandas, an√°lisis estad√≠stico completo y apropiado |\n",
        "| **Visualizaci√≥n** | 20 pts | Gr√°ficos informativos, etiquetado apropiado, est√©tica profesional |\n",
        "| **An√°lisis Interpretativo** | 15 pts | Insights profundos, conexiones te√≥ricas, pensamiento cr√≠tico |\n",
        "| **C√≥digo y Documentaci√≥n** | 10 pts | Estilo PEP8, comentarios explicativos, c√≥digo ejecutable |\n",
        "\n",
        "### **Escala de Calificaci√≥n por Componente**\n",
        "\n",
        "- **Excelente (90-100%)**: Implementaci√≥n completa y correcta, an√°lisis profundo, c√≥digo profesional\n",
        "- **Bueno (80-89%)**: Implementaci√≥n correcta con peque√±as mejoras posibles, an√°lisis s√≥lido\n",
        "- **Satisfactorio (70-79%)**: Funcionalidad b√°sica correcta, an√°lisis superficial pero presente\n",
        "- **Insuficiente (<70%)**: Errores significativos, an√°lisis incompleto o incorrecto\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Lista de Verificaci√≥n Pre-Entrega**\n",
        "\n",
        "Marque cada item antes de entregar su trabajo:\n",
        "\n",
        "- [ ] **C√≥digo ejecutable**: Todas las celdas se ejecutan sin errores\n",
        "- [ ] **GridWorldEnvironment**: Implementaci√≥n completa con todos los m√©todos requeridos\n",
        "- [ ] **SimpleQLearningAgent**: Implementaci√≥n correcta del algoritmo Q-Learning\n",
        "- [ ] **Entrenamiento**: 1000 episodios ejecutados exitosamente\n",
        "- [ ] **An√°lisis de datos**: Procesamiento completo con Pandas, estad√≠sticas calculadas\n",
        "- [ ] **Visualizaciones**: Las 4 gr√°ficas requeridas generadas correctamente\n",
        "- [ ] **An√°lisis interpretativo**: M√≠nimo 200 palabras, todos los puntos abordados\n",
        "- [ ] **Evaluaci√≥n final**: M√©tricas de rendimiento calculadas sin exploraci√≥n\n",
        "- [ ] **Comentarios**: C√≥digo apropiadamente comentado y documentado\n",
        "- [ ] **Nomenclatura**: Archivo nombrado como `RL_Python_[Apellido]_[Nombre].ipynb`\n",
        "\n",
        "---\n",
        "\n",
        "## üìö **Recursos de Referencia Adicionales**\n",
        "\n",
        "- **Sutton & Barto (2018)**: Cap√≠tulos 1-6 para fundamentos te√≥ricos de RL\n",
        "- **Documentaci√≥n NumPy**: [numpy.org/doc](https://numpy.org/doc/) - Para operaciones con arrays\n",
        "- **Documentaci√≥n Pandas**: [pandas.pydata.org/docs](https://pandas.pydata.org/docs/) - Para manipulaci√≥n de datos\n",
        "- **Documentaci√≥n Matplotlib**: [matplotlib.org](https://matplotlib.org/) - Para visualizaci√≥n\n",
        "- **PEP 8**: [pep8.org](https://pep8.org/) - Gu√≠a de estilo para Python\n",
        "\n",
        "---\n",
        "\n",
        "## üéì **Mensaje Final**\n",
        "\n",
        "**¬°Felicitaciones por completar esta tarea integradora!**\n",
        "\n",
        "Esta actividad representa la culminaci√≥n de sus aprendizajes en el curso, integrando programaci√≥n orientada a objetos, manipulaci√≥n de datos, visualizaci√≥n y fundamentos de aprendizaje por refuerzo.\n",
        "\n",
        "El agente Q-Learning que han implementado constituye uno de los algoritmos fundamentales del campo, y las habilidades desarrolladas les proporcionan una base s√≥lida para abordar problemas m√°s complejos en inteligencia artificial y ciencia de datos.\n",
        "\n",
        "**Recuerden**: La excelencia no radica √∫nicamente en obtener resultados correctos, sino en comprender profundamente los procesos subyacentes y ser capaces de articular insights significativos sobre el comportamiento observado.\n",
        "\n",
        "¬°√âxito en su implementaci√≥n! üöÄ"
      ],
      "metadata": {
        "id": "qx4BOvRbQUqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda final de verificaci√≥n y resumen\n",
        "print(\"üéâ TAREA INTEGRADORA COMPLETADA\")\n",
        "print(\"=\" * 50)\n",
        "print(\"‚úÖ Configuraci√≥n del entorno\")\n",
        "print(\"‚úÖ Implementaci√≥n de clases (GridWorld + Q-Learning)\")\n",
        "print(\"‚úÖ Entrenamiento y recolecci√≥n de datos\")\n",
        "print(\"‚úÖ An√°lisis estad√≠stico\")\n",
        "print(\"‚úÖ Visualizaciones comprehensivas\")\n",
        "print(\"‚úÖ Evaluaci√≥n final del rendimiento\")\n",
        "print(\"\\nüìù PENDIENTE POR COMPLETAR:\")\n",
        "print(\"‚è≥ An√°lisis interpretativo (m√≠nimo 200 palabras)\")\n",
        "print(\"‚è≥ Verificaci√≥n de lista de control\")\n",
        "print(\"‚è≥ Nomenclatura correcta del archivo\")\n",
        "print(\"\\nüèÜ ¬°Buena suerte con su entrega!\")\n",
        "\n",
        "# Informaci√≥n del estudiante (completar)\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"INFORMACI√ìN DEL ESTUDIANTE\")\n",
        "print(\"=\"*30)\n",
        "print(\"Nombre: [COMPLETAR]\")\n",
        "print(\"Apellido: [COMPLETAR]\")\n",
        "print(\"Fecha de entrega: [COMPLETAR]\")\n",
        "print(\"Nombre del archivo: RL_Python_[Apellido]_[Nombre].ipynb\")"
      ],
      "metadata": {
        "id": "heXMLRX4QZRo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}