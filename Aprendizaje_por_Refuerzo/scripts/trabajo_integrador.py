# -*- coding: utf-8 -*-
"""Trabajo_Integrador.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sX4dlnahY2qG0cMSoLLhITDPAQMB4rzk

# üìö **Tarea Integradora: An√°lisis y Desarrollo de un Agente de Aprendizaje por Refuerzo**

---

## üéØ **Informaci√≥n General**

**Curso**: Introducci√≥n a la Programaci√≥n en Python para Aprendizaje por Refuerzo  
**Modalidad**: Google Colaboratory  
**Tiempo estimado**: 4-6 horas  
**Puntuaci√≥n total**: 100 puntos  

---

## üìã **Objetivos de la Actividad**

- **Integrar conceptos fundamentales** de Python: estructuras de datos, POO, manipulaci√≥n con Pandas/NumPy
- **Implementar algoritmos b√°sicos de RL**: Desarrollar un agente Q-Learning simplificado
- **Analizar resultados experimentales**: Procesar m√©tricas de rendimiento y generar visualizaciones
- **Demostrar competencias** en el ecosistema cient√≠fico de Python

---

## üîß **Configuraci√≥n del Entorno**
"""

# Importaci√≥n de bibliotecas necesarias
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict, deque
import random
from typing import Tuple, List, Dict
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n de visualizaci√≥n
plt.style.use('default')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 10
sns.set_palette("husl")

# Semilla para reproducibilidad
np.random.seed(42)
random.seed(42)

print("‚úÖ Bibliotecas importadas correctamente")
print("üìä Configuraci√≥n de visualizaci√≥n establecida")
print("üé≤ Semilla aleatoria fijada para reproducibilidad")

"""---

## üìù **PARTE A: Implementaci√≥n del Entorno y Agente (40 puntos)**

### üåê **Paso 1: Implementar GridWorldEnvironment**

**Especificaciones t√©cnicas:**
- Cuadr√≠cula 4√ó4 (16 estados: 0-15)
- Estado inicial: 0 (esquina superior izquierda)
- Objetivo: estado 15 (esquina inferior derecha)
- Obst√°culos: estados [5, 7, 11]
- Acciones: 0=‚Üë, 1=‚Üí, 2=‚Üì, 3=‚Üê
- Recompensas: +10 (objetivo), -1 (obst√°culo), -0.1 (movimiento normal)

**Representaci√≥n del grid:**
"""

# Implementaci√≥n del entorno GridWorld
class GridWorldEnvironment:
    """
    Entorno de cuadr√≠cula 4x4 para aprendizaje por refuerzo.
    """

    def __init__(self):
        # TODO: Implementar inicializaci√≥n
        # Definir: grid_size, n_states, n_actions, obstacles, goal_state, current_state
        self.grid_size = 4
        self.n_states = 16
        self.n_actions = 4
        self.obstacles = [5, 7, 11]
        self.goal_state = 15
        self.current_state = 0

        # Definir movimientos: 0=‚Üë, 1=‚Üí, 2=‚Üì, 3=‚Üê
        self.actions = {
            0: (-1, 0),  # arriba
            1: (0, 1),   # derecha
            2: (1, 0),   # abajo
            3: (0, -1)   # izquierda
        }

        print(f"üåê GridWorld 4x4 inicializado")
        print(f"   Estados: {self.n_states}")
        print(f"   Obst√°culos: {self.obstacles}")
        print(f"   Objetivo: {self.goal_state}")

    def reset(self) -> int:
        """
        Reinicia el entorno al estado inicial.
        Returns:
            int: Estado inicial (0)
        """
        # TODO: Implementar reset
        self.current_state = 0
        return self.current_state

    def step(self, action: int) -> Tuple[int, float, bool, dict]:
        """
        Ejecuta una acci√≥n en el entorno.

        Args:
            action (int): Acci√≥n a ejecutar (0=‚Üë, 1=‚Üí, 2=‚Üì, 3=‚Üê)

        Returns:
            Tuple: (next_state, reward, done, info)
        """
        # TODO: Implementar l√≥gica de transici√≥n
        # Considerar: validaci√≥n de fronteras, detecci√≥n de obst√°culos, c√°lculo de recompensas

        # Obtener coordenadas actuales
        row, col = self.state_to_coordinates(self.current_state)

        # Calcular nueva posici√≥n
        delta_row, delta_col = self.actions[action]
        new_row = row + delta_row
        new_col = col + delta_col

        # Validar fronteras
        if new_row < 0 or new_row >= self.grid_size or new_col < 0 or new_col >= self.grid_size:
            # Fuera de fronteras, mantener posici√≥n actual
            next_state = self.current_state
        else:
            next_state = self.coordinates_to_state(new_row, new_col)

        # Calcular recompensa
        if next_state == self.goal_state:
            reward = 10.0
            done = True
        elif next_state in self.obstacles:
            reward = -1.0
            done = True
        else:
            reward = -0.1
            done = False

        # Actualizar estado actual
        self.current_state = next_state

        info = {
            'coordinates': self.state_to_coordinates(next_state),
            'action_name': ['‚Üë', '‚Üí', '‚Üì', '‚Üê'][action]
        }

        return next_state, reward, done, info

    def get_valid_actions(self, state: int) -> List[int]:
        """
        Obtiene acciones v√°lidas desde un estado dado.

        Args:
            state (int): Estado actual

        Returns:
            List[int]: Lista de acciones v√°lidas
        """
        # TODO: Implementar validaci√≥n de acciones
        valid_actions = []
        row, col = self.state_to_coordinates(state)

        for action in range(self.n_actions):
            delta_row, delta_col = self.actions[action]
            new_row = row + delta_row
            new_col = col + delta_col

            # Verificar si la nueva posici√≥n est√° dentro de fronteras
            if 0 <= new_row < self.grid_size and 0 <= new_col < self.grid_size:
                valid_actions.append(action)

        return valid_actions

    def state_to_coordinates(self, state: int) -> Tuple[int, int]:
        """Convierte estado a coordenadas (fila, columna)."""
        # TODO: Implementar conversi√≥n estado -> coordenadas
        row = state // self.grid_size
        col = state % self.grid_size
        return row, col

    def coordinates_to_state(self, row: int, col: int) -> int:
        """Convierte coordenadas a estado."""
        # TODO: Implementar conversi√≥n coordenadas -> estado
        return row * self.grid_size + col

    def render(self):
        """Visualiza el estado actual del entorno."""
        # TODO: Implementar visualizaci√≥n opcional
        print("\nGrid actual:")
        for row in range(self.grid_size):
            line = ""
            for col in range(self.grid_size):
                state = self.coordinates_to_state(row, col)
                if state == self.current_state:
                    line += "[A] "  # Agente
                elif state == self.goal_state:
                    line += "[G] "  # Goal
                elif state in self.obstacles:
                    line += "[X] "  # Obst√°culo
                else:
                    line += "[ ] "
            print(line)
        print()

# Prueba inicial del entorno
env = GridWorldEnvironment()
print("Estado inicial:", env.reset())
print("Acciones v√°lidas desde estado 0:", env.get_valid_actions(0))
env.render()

"""### ü§ñ **Paso 2: Implementar SimpleQLearningAgent**

**Especificaciones del agente:**
- Algoritmo: Q-Learning con tabla tabular
- Exploraci√≥n: Estrategia epsilon-greedy con decaimiento
- Actualizaci√≥n: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
- Par√°metros configurables: Œ±, Œ≥, Œµ, decaimiento de Œµ
"""

# Implementaci√≥n del agente Q-Learning
class SimpleQLearningAgent:
    """
    Agente Q-Learning con exploraci√≥n epsilon-greedy.

    Implementa la ecuaci√≥n de actualizaci√≥n:
    Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
    """

    def __init__(self, n_states: int, n_actions: int,
                 learning_rate: float = 0.1, discount_factor: float = 0.95,
                 epsilon: float = 0.1, epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01):
        """
        Inicializa el agente Q-Learning.

        Args:
            n_states: N√∫mero de estados en el entorno
            n_actions: N√∫mero de acciones disponibles
            learning_rate: Tasa de aprendizaje (Œ±)
            discount_factor: Factor de descuento (Œ≥)
            epsilon: Probabilidad de exploraci√≥n inicial
            epsilon_decay: Factor de decaimiento de epsilon
            epsilon_min: Valor m√≠nimo de epsilon
        """
        # TODO: Implementar inicializaci√≥n
        # Inicializar: tabla Q, par√°metros de aprendizaje, m√©tricas de seguimiento
        self.n_states = n_states
        self.n_actions = n_actions
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

        # Inicializar tabla Q con ceros
        self.q_table = np.zeros((n_states, n_actions))

        # M√©tricas de seguimiento
        self.td_errors = []
        self.training_step = 0

        print(f"ü§ñ Agente Q-Learning inicializado")
        print(f"   Estados: {n_states}, Acciones: {n_actions}")
        print(f"   Œ±: {learning_rate}, Œ≥: {discount_factor}")
        print(f"   Œµ inicial: {epsilon}, decaimiento: {epsilon_decay}")

    def choose_action(self, state: int, training: bool = True) -> int:
        """
        Selecciona acci√≥n usando estrategia epsilon-greedy.

        Args:
            state: Estado actual
            training: Si True, aplica exploraci√≥n; si False, pol√≠tica greedy pura

        Returns:
            int: Acci√≥n seleccionada
        """
        # TODO: Implementar selecci√≥n de acci√≥n epsilon-greedy
        if training and np.random.random() < self.epsilon:
            # Exploraci√≥n: acci√≥n aleatoria
            return np.random.randint(0, self.n_actions)
        else:
            # Explotaci√≥n: mejor acci√≥n seg√∫n tabla Q
            return np.argmax(self.q_table[state])

    def update_q_table(self, state: int, action: int, reward: float,
                      next_state: int, done: bool) -> float:
        """
        Actualiza la tabla Q usando la regla de Q-Learning.

        Args:
            state: Estado actual
            action: Acci√≥n ejecutada
            reward: Recompensa observada
            next_state: Estado siguiente
            done: Si el episodio termin√≥

        Returns:
            float: Error de diferencia temporal (TD error)
        """
        # TODO: Implementar actualizaci√≥n Q-Learning
        # Calcular: Q_target, TD_error, actualizar Q(s,a)

        # Valor Q actual
        current_q = self.q_table[state, action]

        # Calcular target
        if done:
            target_q = reward
        else:
            target_q = reward + self.discount_factor * np.max(self.q_table[next_state])

        # Error de diferencia temporal
        td_error = target_q - current_q

        # Actualizar tabla Q
        self.q_table[state, action] += self.learning_rate * td_error

        # Registrar m√©tricas
        self.td_errors.append(abs(td_error))
        self.training_step += 1

        return abs(td_error)

    def decay_epsilon(self):
        """Reduce epsilon gradualmente."""
        # TODO: Implementar decaimiento de epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def get_policy(self) -> np.ndarray:
        """
        Extrae pol√≠tica determin√≠stica de la tabla Q.

        Returns:
            np.ndarray: Matriz de pol√≠tica [n_states x n_actions]
        """
        # TODO: Implementar extracci√≥n de pol√≠tica
        policy = np.zeros((self.n_states, self.n_actions))
        for state in range(self.n_states):
            best_action = np.argmax(self.q_table[state])
            policy[state, best_action] = 1.0
        return policy

    def get_value_function(self) -> np.ndarray:
        """
        Calcula funci√≥n de valor V(s) = max_a Q(s,a).

        Returns:
            np.ndarray: Vector de funci√≥n de valor
        """
        # TODO: Implementar c√°lculo de funci√≥n de valor
        return np.max(self.q_table, axis=1)

# Prueba inicial del agente
agent = SimpleQLearningAgent(n_states=16, n_actions=4, epsilon=0.3)
print("Acci√≥n inicial desde estado 0:", agent.choose_action(0))
print("Forma de la tabla Q:", agent.q_table.shape)
print("Funci√≥n de valor inicial:", agent.get_value_function()[:5])

"""---

## üß™ **PARTE B: Experimentaci√≥n y An√°lisis de Datos (35 puntos)**

### üìä **Paso 3: Script de Entrenamiento y Recolecci√≥n de Datos**

En esta secci√≥n implementaremos el loop de entrenamiento que:
- Ejecuta 1000 episodios de entrenamiento
- Registra m√©tricas de rendimiento por episodio
- Aplica decaimiento gradual de epsilon
- Genera reportes de progreso peri√≥dicos
"""

# Funci√≥n de entrenamiento
def train_agent(env, agent, n_episodes: int = 1000, max_steps: int = 100) -> pd.DataFrame:
    """
    Entrena el agente y recolecta m√©tricas de rendimiento.

    Args:
        env: Entorno de entrenamiento
        agent: Agente a entrenar
        n_episodes: N√∫mero de episodios
        max_steps: M√°ximo de pasos por episodio

    Returns:
        pd.DataFrame: M√©tricas de entrenamiento
    """

    # Contenedores para m√©tricas
    metrics = {
        'episode': [],
        'reward': [],
        'steps': [],
        'epsilon': [],
        'states_visited': [],
        'td_errors': []
    }

    print(f"üöÄ Iniciando entrenamiento: {n_episodes} episodios")
    print("-" * 50)

    for episode in range(n_episodes):
        # TODO: Implementar loop de entrenamiento
        # 1. Reset del entorno
        state = env.reset()

        # 2. Loop del episodio hasta terminar o max_steps
        episode_reward = 0
        episode_steps = 0
        episode_states = set()
        episode_td_errors = []

        for step in range(max_steps):
            # Seleccionar acci√≥n
            action = agent.choose_action(state, training=True)

            # Ejecutar acci√≥n
            next_state, reward, done, info = env.step(action)

            # Actualizar agente
            td_error = agent.update_q_table(state, action, reward, next_state, done)

            # Registrar m√©tricas del paso
            episode_reward += reward
            episode_steps += 1
            episode_states.add(state)
            episode_td_errors.append(td_error)

            # Preparar siguiente iteraci√≥n
            state = next_state

            if done:
                break

        # 3. Registrar m√©tricas del episodio
        metrics['episode'].append(episode)
        metrics['reward'].append(episode_reward)
        metrics['steps'].append(episode_steps)
        metrics['epsilon'].append(agent.epsilon)
        metrics['states_visited'].append(len(episode_states))
        metrics['td_errors'].append(np.mean(episode_td_errors) if episode_td_errors else 0)

        # 4. Aplicar decaimiento de epsilon
        agent.decay_epsilon()

        # 5. Reportar progreso cada 100 episodios
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(metrics['reward'][-100:])
            avg_steps = np.mean(metrics['steps'][-100:])
            print(f"Episodio {episode + 1:4d} | "
                  f"Recompensa promedio: {avg_reward:6.2f} | "
                  f"Pasos promedio: {avg_steps:5.1f} | "
                  f"Epsilon: {agent.epsilon:.3f}")

    # TODO: Crear DataFrame con m√©tricas
    df = pd.DataFrame(metrics)
    df['moving_average'] = df['reward'].rolling(window=50, min_periods=1).mean()

    print(f"\n‚úÖ Entrenamiento completado")
    print(f"üìä Datos recolectados: {len(df)} episodios")

    return df

# Ejecuci√≥n del entrenamiento
print("üéÆ Creando nuevo entorno y agente para entrenamiento...")
env = GridWorldEnvironment()
agent = SimpleQLearningAgent(n_states=16, n_actions=4,
                           learning_rate=0.1,
                           epsilon=0.3,
                           epsilon_decay=0.995)

# Ejecutar entrenamiento
training_data = train_agent(env, agent, n_episodes=1000)

"""### üìà **Paso 4: Procesamiento y An√°lisis de Datos**

Ahora analizaremos estad√≠sticamente los datos recolectados durante el entrenamiento:
- Estad√≠sticas descriptivas b√°sicas
- Detecci√≥n de convergencia
- An√°lisis de distribuciones
- Correlaciones entre variables
"""

# An√°lisis estad√≠stico de los datos
def analyze_training_data(df: pd.DataFrame) -> Dict:
    """
    Realiza an√°lisis estad√≠stico comprehensivo de los datos de entrenamiento.

    Args:
        df: DataFrame con m√©tricas de entrenamiento

    Returns:
        Dict: Estad√≠sticas de an√°lisis
    """

    analysis = {}

    # TODO: Implementar an√°lisis estad√≠stico
    # 1. Estad√≠sticas descriptivas b√°sicas
    analysis['total_episodes'] = len(df)
    analysis['final_average_reward'] = df['reward'].tail(100).mean()
    analysis['best_episode_reward'] = df['reward'].max()
    analysis['worst_episode_reward'] = df['reward'].min()

    # 2. Tasa de convergencia (episodio donde se estabiliza)
    # Considerar convergencia cuando la media m√≥vil se estabiliza
    moving_avg = df['moving_average'].values
    differences = np.abs(np.diff(moving_avg))
    convergence_threshold = 0.1

    convergence_episode = None
    for i in range(100, len(differences)):
        if np.all(differences[i-20:i] < convergence_threshold):
            convergence_episode = i
            break

    analysis['convergence_episode'] = convergence_episode if convergence_episode else "No detectada"

    # 3. An√°lisis de cuartiles para longitud de episodios
    steps_quartiles = np.percentile(df['steps'], [25, 50, 75])
    analysis['steps_q1'] = steps_quartiles[0]
    analysis['steps_median'] = steps_quartiles[1]
    analysis['steps_q3'] = steps_quartiles[2]

    # 4. Correlaciones entre variables
    correlation_matrix = df[['reward', 'steps', 'epsilon', 'states_visited']].corr()
    analysis['reward_steps_correlation'] = correlation_matrix.loc['reward', 'steps']
    analysis['reward_epsilon_correlation'] = correlation_matrix.loc['reward', 'epsilon']

    # 5. Estad√≠sticas de exploraci√≥n
    analysis['initial_epsilon'] = df['epsilon'].iloc[0]
    analysis['final_epsilon'] = df['epsilon'].iloc[-1]
    analysis['average_states_visited'] = df['states_visited'].mean()

    # 6. Tendencias de mejora
    first_half_reward = df['reward'].iloc[:len(df)//2].mean()
    second_half_reward = df['reward'].iloc[len(df)//2:].mean()
    analysis['improvement_percentage'] = ((second_half_reward - first_half_reward) / abs(first_half_reward)) * 100

    return analysis

# Ejecutar an√°lisis
analysis_results = analyze_training_data(training_data)

print("üìä AN√ÅLISIS ESTAD√çSTICO DE ENTRENAMIENTO")
print("=" * 50)
for key, value in analysis_results.items():
    if isinstance(value, float):
        print(f"{key}: {value:.3f}")
    else:
        print(f"{key}: {value}")

# Mostrar estad√≠sticas descriptivas del DataFrame
print("\nüìà ESTAD√çSTICAS DESCRIPTIVAS")
print("=" * 30)
print(training_data[['reward', 'steps', 'epsilon', 'states_visited']].describe())

"""---

## üìä **PARTE C: Visualizaci√≥n y Interpretaci√≥n (25 puntos)**

### üé® **Paso 5: Generaci√≥n de Visualizaciones**

Crearemos un conjunto comprehensivo de visualizaciones que incluye:
1. **Curva de aprendizaje**: Recompensa por episodio + media m√≥vil
2. **Eficiencia del agente**: Evoluci√≥n de pasos por episodio
3. **Exploraci√≥n**: Decaimiento de epsilon durante entrenamiento
4. **Mapa de calor**: Pol√≠tica final aprendida en la cuadr√≠cula 4x4
"""

# Funci√≥n de visualizaci√≥n comprehensiva
def create_comprehensive_plots(df: pd.DataFrame, agent, env):
    """
    Genera visualizaciones comprehensivas del proceso de entrenamiento.

    Args:
        df: DataFrame con m√©tricas de entrenamiento
        agent: Agente entrenado
        env: Entorno utilizado
    """

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # TODO: Implementar cada subgr√°fico

    # 1. Curva de aprendizaje (superior izquierdo)
    axes[0,0].plot(df['episode'], df['reward'], alpha=0.3, color='lightblue', label='Recompensa por episodio')
    axes[0,0].plot(df['episode'], df['moving_average'], color='darkblue', linewidth=2, label='Media m√≥vil (50 ep.)')
    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='L√≠nea base')
    axes[0,0].set_title('Curva de Aprendizaje', fontweight='bold', fontsize=12)
    axes[0,0].set_xlabel('Episodio')
    axes[0,0].set_ylabel('Recompensa Acumulada')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)

    # 2. Eficiencia del agente (superior derecho)
    axes[0,1].plot(df['episode'], df['steps'], color='green', alpha=0.6, label='Pasos por episodio')
    # Agregar l√≠nea de tendencia
    z = np.polyfit(df['episode'], df['steps'], 1)
    p = np.poly1d(z)
    axes[0,1].plot(df['episode'], p(df['episode']), color='darkgreen', linewidth=2, linestyle='--', label='Tendencia')
    axes[0,1].set_title('Eficiencia del Agente', fontweight='bold', fontsize=12)
    axes[0,1].set_xlabel('Episodio')
    axes[0,1].set_ylabel('Pasos hasta Terminaci√≥n')
    axes[0,1].legend()
    axes[0,1].grid(True, alpha=0.3)

    # 3. Exploraci√≥n (inferior izquierdo)
    axes[1,0].plot(df['episode'], df['epsilon'], color='orange', linewidth=2)
    axes[1,0].axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='Exploraci√≥n baja')
    axes[1,0].axhline(y=0.3, color='yellow', linestyle='--', alpha=0.7, label='Exploraci√≥n alta')
    axes[1,0].set_title('Evoluci√≥n de la Exploraci√≥n (Œµ)', fontweight='bold', fontsize=12)
    axes[1,0].set_xlabel('Episodio')
    axes[1,0].set_ylabel('Epsilon')
    axes[1,0].legend()
    axes[1,0].grid(True, alpha=0.3)

    # 4. Mapa de calor de pol√≠tica (inferior derecho)
    policy = agent.get_policy()
    policy_grid = np.zeros((4, 4))

    # Convertir pol√≠tica a grid 4x4 con direcciones
    action_arrows = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']
    policy_arrows = np.empty((4, 4), dtype=object)

    for state in range(16):
        row, col = env.state_to_coordinates(state)
        best_action = np.argmax(policy[state])
        policy_grid[row, col] = best_action
        policy_arrows[row, col] = action_arrows[best_action]

    # Crear mapa de calor
    im = axes[1,1].imshow(policy_grid, cmap='viridis', alpha=0.7)

    # Agregar texto con direcciones y estados especiales
    for row in range(4):
        for col in range(4):
            state = env.coordinates_to_state(row, col)
            if state == 0:  # Estado inicial
                text = f"START\n{policy_arrows[row, col]}"
                color = 'white'
            elif state == 15:  # Objetivo
                text = "GOAL"
                color = 'yellow'
            elif state in env.obstacles:  # Obst√°culos
                text = "XXX"
                color = 'red'
            else:
                text = policy_arrows[row, col]
                color = 'white'

            axes[1,1].text(col, row, text, ha='center', va='center',
                         color=color, fontweight='bold', fontsize=10)

    axes[1,1].set_title('Pol√≠tica Final Aprendida', fontweight='bold', fontsize=12)
    axes[1,1].set_xlabel('Columna')
    axes[1,1].set_ylabel('Fila')

    # Configurar ticks
    axes[1,1].set_xticks(range(4))
    axes[1,1].set_yticks(range(4))

    plt.tight_layout()
    plt.suptitle('An√°lisis Comprehensivo: Entrenamiento Q-Learning GridWorld 4x4',
                 fontsize=16, fontweight='bold', y=0.98)
    plt.show()

# Ejecutar visualizaci√≥n
create_comprehensive_plots(training_data, agent, env)

"""### üîç **Paso 6: An√°lisis Interpretativo**

**INSTRUCCIONES**: Complete el siguiente an√°lisis con un m√≠nimo de 200 palabras, abordando cada punto solicitado. Sustente sus observaciones con los resultados obtenidos en su entrenamiento.

---

## üìù **AN√ÅLISIS E INTERPRETACI√ìN DE RESULTADOS**

### **1. Patrones de Convergencia Observados:**
[TODO: Describa c√≥mo evolucion√≥ la recompensa a lo largo del entrenamiento. ¬øEn qu√© episodio aproximado observa estabilizaci√≥n? ¬øHubo fluctuaciones significativas? Mencione valores espec√≠ficos de su entrenamiento.]

### **2. Efectividad de la Estrategia Epsilon-Greedy:**
[TODO: Analice c√≥mo el balance entre exploraci√≥n y explotaci√≥n afect√≥ el aprendizaje. ¬øFue apropiado el decaimiento de epsilon? ¬øQu√© mejoras propondr√≠a? Considere los valores inicial y final de epsilon.]

### **3. Calidad de la Pol√≠tica Aprendida:**
[TODO: Eval√∫e la pol√≠tica final mostrada en el mapa de calor. ¬øEl agente aprendi√≥ una estrategia √≥ptima para llegar al objetivo? ¬øEvita obst√°culos eficientemente? ¬øHay direcciones que parecen sub√≥ptimas?]

### **4. Limitaciones del Enfoque Tabular:**
[TODO: Reflexione sobre las limitaciones de Q-Learning tabular. ¬øC√≥mo escalar√≠a este enfoque a problemas con espacios de estados m√°s grandes (ej: 100x100)? ¬øQu√© alternativas conoce (aproximaci√≥n funcional, redes neuronales)?]

### **5. Observaciones Adicionales:**
[TODO: Incluya cualquier insight adicional sobre el comportamiento del agente, patrones inesperados, relaci√≥n entre m√©tricas, o mejoras potenciales al algoritmo implementado.]

---

**Extensi√≥n m√≠nima requerida**: 200 palabras  
**Criterios de evaluaci√≥n**: Profundidad del an√°lisis, conexi√≥n con resultados obtenidos, pensamiento cr√≠tico, propuestas de mejora
"""

# Celda para que los estudiantes escriban su an√°lisis interpretativo
analysis_text = """
# üìã MI AN√ÅLISIS INTERPRETATIVO

## 1. Patrones de Convergencia Observados
[Escriba aqu√≠ su an√°lisis basado en los gr√°ficos generados...]

## 2. Efectividad de la Estrategia Epsilon-Greedy
[Escriba aqu√≠ su an√°lisis sobre la exploraci√≥n vs explotaci√≥n...]

## 3. Calidad de la Pol√≠tica Aprendida
[Escriba aqu√≠ su evaluaci√≥n de la pol√≠tica final...]

## 4. Limitaciones del Enfoque Tabular
[Escriba aqu√≠ sus reflexiones sobre escalabilidad...]

## 5. Observaciones Adicionales
[Escriba aqu√≠ insights adicionales...]
"""

print("üìù Secci√≥n de an√°lisis preparada")
print("‚úèÔ∏è Complete su an√°lisis interpretativo arriba")
print("üí° Recuerde: m√≠nimo 200 palabras, conectar con sus resultados espec√≠ficos")

"""---

## üìä **Evaluaci√≥n y M√©tricas Finales**

### **Paso 7: Evaluaci√≥n del Rendimiento Final**

Evaluaremos el agente entrenado ejecutando episodios sin exploraci√≥n (Œµ=0) para medir su rendimiento real.
"""

# M√©tricas de evaluaci√≥n final
def evaluate_final_performance(agent, env, n_test_episodes: int = 100) -> Dict:
    """
    Eval√∫a el rendimiento final del agente entrenado.

    Args:
        agent: Agente entrenado
        env: Entorno de evaluaci√≥n
        n_test_episodes: N√∫mero de episodios de prueba

    Returns:
        Dict: M√©tricas de rendimiento final
    """

    # TODO: Implementar evaluaci√≥n final
    success_count = 0
    total_rewards = []
    total_steps = []
    outcomes = []

    print(f"üéØ Evaluando rendimiento final: {n_test_episodes} episodios sin exploraci√≥n")

    for episode in range(n_test_episodes):
        # Reiniciar entorno
        state = env.reset()
        episode_reward = 0
        episode_steps = 0

        # Ejecutar episodio sin exploraci√≥n
        for step in range(100):  # M√°ximo 100 pasos
            action = agent.choose_action(state, training=False)  # Sin exploraci√≥n
            next_state, reward, done, info = env.step(action)

            episode_reward += reward
            episode_steps += 1
            state = next_state

            if done:
                if reward > 0:  # Lleg√≥ al objetivo
                    success_count += 1
                    outcomes.append("√âxito")
                else:  # Cay√≥ en obst√°culo
                    outcomes.append("Obst√°culo")
                break
        else:
            outcomes.append("Timeout")  # No termin√≥ en tiempo l√≠mite

        total_rewards.append(episode_reward)
        total_steps.append(episode_steps)

    # Calcular m√©tricas
    metrics = {
        'success_rate': success_count / n_test_episodes,
        'average_reward': np.mean(total_rewards),
        'std_reward': np.std(total_rewards),
        'average_steps': np.mean(total_steps),
        'std_steps': np.std(total_steps),
        'best_reward': np.max(total_rewards),
        'worst_reward': np.min(total_rewards),
        'success_episodes': success_count,
        'total_episodes': n_test_episodes
    }

    # Mostrar distribuci√≥n de resultados
    outcome_counts = {outcome: outcomes.count(outcome) for outcome in set(outcomes)}
    metrics['outcome_distribution'] = outcome_counts

    return metrics

# Ejecutar evaluaci√≥n final
final_metrics = evaluate_final_performance(agent, env, n_test_episodes=100)

print("\nüéØ M√âTRICAS DE RENDIMIENTO FINAL")
print("=" * 40)
for metric, value in final_metrics.items():
    if isinstance(value, float):
        if metric.endswith('_rate'):
            print(f"{metric}: {value:.1%}")
        else:
            print(f"{metric}: {value:.3f}")
    else:
        print(f"{metric}: {value}")

# Visualizaci√≥n adicional: distribuci√≥n de recompensas finales
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
rewards_eval = []
steps_eval = []
for _ in range(100):
    state = env.reset()
    ep_reward = 0
    ep_steps = 0
    for step in range(100):
        action = agent.choose_action(state, training=False)
        state, reward, done, _ = env.step(action)
        ep_reward += reward
        ep_steps += 1
        if done:
            break
    rewards_eval.append(ep_reward)
    steps_eval.append(ep_steps)

plt.hist(rewards_eval, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
plt.title('Distribuci√≥n de Recompensas (Evaluaci√≥n Final)')
plt.xlabel('Recompensa por Episodio')
plt.ylabel('Frecuencia')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.hist(steps_eval, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
plt.title('Distribuci√≥n de Pasos (Evaluaci√≥n Final)')
plt.xlabel('Pasos por Episodio')
plt.ylabel('Frecuencia')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\nüìà Recompensa promedio en evaluaci√≥n: {np.mean(rewards_eval):.2f}")
print(f"üìä Pasos promedio en evaluaci√≥n: {np.mean(steps_eval):.1f}")

"""---

## üèÜ **Criterios de Evaluaci√≥n y Lista de Verificaci√≥n**

### **Criterios de Evaluaci√≥n**

| **Componente** | **Puntuaci√≥n** | **Criterios de Evaluaci√≥n** |
|----------------|----------------|------------------------------|
| **Implementaci√≥n POO** | 20 pts | Clases bien estructuradas, m√©todos apropiados, documentaci√≥n clara |
| **Algoritmo Q-Learning** | 20 pts | Implementaci√≥n matem√°ticamente correcta, manejo apropiado de par√°metros |
| **Procesamiento de Datos** | 15 pts | Uso efectivo de Pandas, an√°lisis estad√≠stico completo y apropiado |
| **Visualizaci√≥n** | 20 pts | Gr√°ficos informativos, etiquetado apropiado, est√©tica profesional |
| **An√°lisis Interpretativo** | 15 pts | Insights profundos, conexiones te√≥ricas, pensamiento cr√≠tico |
| **C√≥digo y Documentaci√≥n** | 10 pts | Estilo PEP8, comentarios explicativos, c√≥digo ejecutable |

### **Escala de Calificaci√≥n por Componente**

- **Excelente (90-100%)**: Implementaci√≥n completa y correcta, an√°lisis profundo, c√≥digo profesional
- **Bueno (80-89%)**: Implementaci√≥n correcta con peque√±as mejoras posibles, an√°lisis s√≥lido
- **Satisfactorio (70-79%)**: Funcionalidad b√°sica correcta, an√°lisis superficial pero presente
- **Insuficiente (<70%)**: Errores significativos, an√°lisis incompleto o incorrecto

---

## ‚úÖ **Lista de Verificaci√≥n Pre-Entrega**

Marque cada item antes de entregar su trabajo:

- [ ] **C√≥digo ejecutable**: Todas las celdas se ejecutan sin errores
- [ ] **GridWorldEnvironment**: Implementaci√≥n completa con todos los m√©todos requeridos
- [ ] **SimpleQLearningAgent**: Implementaci√≥n correcta del algoritmo Q-Learning
- [ ] **Entrenamiento**: 1000 episodios ejecutados exitosamente
- [ ] **An√°lisis de datos**: Procesamiento completo con Pandas, estad√≠sticas calculadas
- [ ] **Visualizaciones**: Las 4 gr√°ficas requeridas generadas correctamente
- [ ] **An√°lisis interpretativo**: M√≠nimo 200 palabras, todos los puntos abordados
- [ ] **Evaluaci√≥n final**: M√©tricas de rendimiento calculadas sin exploraci√≥n
- [ ] **Comentarios**: C√≥digo apropiadamente comentado y documentado
- [ ] **Nomenclatura**: Archivo nombrado como `RL_Python_[Apellido]_[Nombre].ipynb`

---

## üìö **Recursos de Referencia Adicionales**

- **Sutton & Barto (2018)**: Cap√≠tulos 1-6 para fundamentos te√≥ricos de RL
- **Documentaci√≥n NumPy**: [numpy.org/doc](https://numpy.org/doc/) - Para operaciones con arrays
- **Documentaci√≥n Pandas**: [pandas.pydata.org/docs](https://pandas.pydata.org/docs/) - Para manipulaci√≥n de datos
- **Documentaci√≥n Matplotlib**: [matplotlib.org](https://matplotlib.org/) - Para visualizaci√≥n
- **PEP 8**: [pep8.org](https://pep8.org/) - Gu√≠a de estilo para Python

---

## üéì **Mensaje Final**

**¬°Felicitaciones por completar esta tarea integradora!**

Esta actividad representa la culminaci√≥n de sus aprendizajes en el curso, integrando programaci√≥n orientada a objetos, manipulaci√≥n de datos, visualizaci√≥n y fundamentos de aprendizaje por refuerzo.

El agente Q-Learning que han implementado constituye uno de los algoritmos fundamentales del campo, y las habilidades desarrolladas les proporcionan una base s√≥lida para abordar problemas m√°s complejos en inteligencia artificial y ciencia de datos.

**Recuerden**: La excelencia no radica √∫nicamente en obtener resultados correctos, sino en comprender profundamente los procesos subyacentes y ser capaces de articular insights significativos sobre el comportamiento observado.

¬°√âxito en su implementaci√≥n! üöÄ
"""

# Celda final de verificaci√≥n y resumen
print("üéâ TAREA INTEGRADORA COMPLETADA")
print("=" * 50)
print("‚úÖ Configuraci√≥n del entorno")
print("‚úÖ Implementaci√≥n de clases (GridWorld + Q-Learning)")
print("‚úÖ Entrenamiento y recolecci√≥n de datos")
print("‚úÖ An√°lisis estad√≠stico")
print("‚úÖ Visualizaciones comprehensivas")
print("‚úÖ Evaluaci√≥n final del rendimiento")
print("\nüìù PENDIENTE POR COMPLETAR:")
print("‚è≥ An√°lisis interpretativo (m√≠nimo 200 palabras)")
print("‚è≥ Verificaci√≥n de lista de control")
print("‚è≥ Nomenclatura correcta del archivo")
print("\nüèÜ ¬°Buena suerte con su entrega!")

# Informaci√≥n del estudiante (completar)
print("\n" + "="*30)
print("INFORMACI√ìN DEL ESTUDIANTE")
print("="*30)
print("Nombre: [COMPLETAR]")
print("Apellido: [COMPLETAR]")
print("Fecha de entrega: [COMPLETAR]")
print("Nombre del archivo: RL_Python_[Apellido]_[Nombre].ipynb")