# -*- coding: utf-8 -*-
"""Aprendizaje_por_Refuerzo_SAE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jLYoCcPS--ggQhF-31y25aBpD6WFAD0y
"""

# -*- coding: utf-8 -*-
"""
# APRENDIZAJE POR REFUERZO: CUADERNO MODULAR PEDAG√ìGICO
## De los Fundamentos Matem√°ticos a las Implementaciones Avanzadas

Este cuaderno est√° dise√±ado para ser ejecutado en bloques modulares, permitiendo
una comprensi√≥n gradual y reflexiva de cada concepto antes de avanzar al siguiente.

**INSTRUCCIONES PEDAG√ìGICAS:**
1. Ejecute cada m√≥dulo completamente antes de pasar al siguiente
2. Observe y analice las visualizaciones generadas
3. Reflexione sobre las m√©tricas mostradas
4. Experimente modificando los hiperpar√°metros sugeridos
"""

# ============================================================================
# M√ìDULO 1: CONFIGURACI√ìN FUNDAMENTAL DEL ENTORNO
# ============================================================================

"""
## M√ìDULO 1: ESTABLECIMIENTO DEL ECOSISTEMA COMPUTACIONAL

**OBJETIVO PEDAG√ìGICO:**
Configurar el entorno computacional y verificar que todas las dependencias
funcionen correctamente. Este m√≥dulo establece las bases tecnol√≥gicas para
nuestros experimentos posteriores.

**CONCEPTOS CLAVE:**
- Gesti√≥n de dependencias en Python
- Compatibilidad entre versiones de bibliotecas
- Configuraci√≥n de reproducibilidad experimental
"""

import subprocess
import sys
import warnings
warnings.filterwarnings('ignore')  # Suprime warnings menores para claridad pedag√≥gica

def setup_reinforcement_learning_environment():
    """
    Configura el entorno computacional con manejo robusto de versiones.

    Esta funci√≥n implementa un sistema de fallbacks que garantiza compatibilidad
    con diferentes configuraciones de Google Colab y versiones de bibliotecas.
    """
    print("üîß M√ìDULO 1: Configurando Entorno Computacional")
    print("=" * 50)

    # Paquetes fundamentales para aprendizaje por refuerzo
    core_packages = [
        'torch', 'torchvision', 'numpy', 'matplotlib',
        'collections-extended', 'pygame'
    ]

    print("üì¶ Instalando dependencias core...")
    for package in core_packages:
        try:
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package],
                                 stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            print(f"   ‚úÖ {package}")
        except subprocess.CalledProcessError:
            print(f"   ‚ö†Ô∏è  {package} (ya instalado o error menor)")

    # Detecci√≥n inteligente de Gym vs Gymnasium
    print("\nüéÆ Configurando entornos de simulaci√≥n...")
    try:
        subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'gymnasium[classic_control]'],
                             stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        print("   ‚úÖ Gymnasium (versi√≥n moderna)")
        gym_type = 'gymnasium'
    except subprocess.CalledProcessError:
        try:
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'gym[classic_control]==0.21.0'],
                                 stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            print("   ‚úÖ OpenAI Gym (versi√≥n cl√°sica)")
            gym_type = 'gym'
        except subprocess.CalledProcessError:
            print("   ‚ùå Error instalando entornos")
            gym_type = None

    print(f"\nüéØ Configuraci√≥n completada exitosamente")
    print(f"   Tipo de entorno: {gym_type}")
    return gym_type

# Ejecutar configuraci√≥n
gym_version = setup_reinforcement_learning_environment()

# ============================================================================
# M√ìDULO 2: IMPORTACIONES Y CONFIGURACI√ìN MATEM√ÅTICA
# ============================================================================

"""
## M√ìDULO 2: FUNDAMENTOS MATEM√ÅTICOS Y CONFIGURACI√ìN

**OBJETIVO PEDAG√ìGICO:**
Establecer el marco matem√°tico fundamental y configurar la reproducibilidad
experimental. Comprender la importancia de la aleatoriedad controlada en
experimentos de aprendizaje autom√°tico.

**CONCEPTOS CLAVE:**
- Reproducibilidad en experimentos de ML
- Configuraci√≥n de bibliotecas cient√≠ficas
- Detecci√≥n de recursos computacionales
"""

print("\nüßÆ M√ìDULO 2: Configuraci√≥n Matem√°tica y Cient√≠fica")
print("=" * 50)

import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import defaultdict, namedtuple, deque
from typing import Tuple, List, Dict, Optional, Union
import time

# Configuraci√≥n de reproducibilidad experimental
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

# Configuraci√≥n de visualizaci√≥n optimizada
plt.style.use('default')  # Usar estilo por defecto para m√°xima compatibilidad
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 11

# Detecci√≥n de capacidades computacionales
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"üé≤ Semilla aleatoria establecida: {RANDOM_SEED}")
print(f"üî¢ Versi√≥n de PyTorch: {torch.__version__}")
print(f"üíª Dispositivo computacional: {device}")
print(f"üé® Backend de visualizaci√≥n: {plt.get_backend()}")

print("\n‚úÖ M√≥dulo 2 completado - Fundamentos matem√°ticos establecidos")

# ============================================================================
# INSTRUCCIONES PARA EL INSTRUCTOR
# ============================================================================

print("\n" + "="*60)
print("üìö PAUSA PEDAG√ìGICA - INSTRUCCIONES PARA EL INSTRUCTOR")
print("="*60)
print("""
En este punto, debe explicar a los estudiantes:

1. La importancia de la REPRODUCIBILIDAD en experimentos de ML
2. Por qu√© utilizamos semillas aleatorias fijas
3. C√≥mo la configuraci√≥n del entorno afecta los resultados
4. La diferencia entre Gym y Gymnasium

PREGUNTAS PARA REFLEXI√ìN:
- ¬øPor qu√© es crucial la reproducibilidad en experimentos cient√≠ficos?
- ¬øQu√© ventajas ofrece CUDA para el aprendizaje profundo?
- ¬øC√≥mo afecta la versi√≥n de PyTorch a nuestros experimentos?

ACTIVIDAD SUGERIDA:
Hacer que los estudiantes cambien la semilla aleatoria y observen c√≥mo
cambian los resultados en m√≥dulos posteriores.
""")
print("="*60)

# ============================================================================
# M√ìDULO 3: PROCESOS DE DECISI√ìN DE MARKOV - TEOR√çA IMPLEMENTADA
# ============================================================================

"""
## M√ìDULO 3: PROCESOS DE DECISI√ìN DE MARKOV (MDPs)

**OBJETIVO PEDAG√ìGICO:**
Implementar y visualizar los conceptos fundamentales de MDPs. Comprender
c√≥mo la ecuaci√≥n de Bellman establece la base matem√°tica para todos los
algoritmos de aprendizaje por refuerzo.

**CONCEPTOS CLAVE:**
- Definici√≥n formal de MDP: ‚ü®S, A, P, R, Œ≥‚ü©
- Ecuaci√≥n de Bellman para evaluaci√≥n de pol√≠ticas
- Convergencia iterativa de funciones de valor
"""

print("\nüèóÔ∏è  M√ìDULO 3: Procesos de Decisi√≥n de Markov")
print("=" * 50)

class MDPEducativo:
    """
    Implementaci√≥n pedag√≥gica de un MDP que revela la estructura matem√°tica
    subyacente de manera expl√≠cita y comprensible.
    """

    def __init__(self, n_estados: int, n_acciones: int, gamma: float = 0.9):
        """
        Inicializa un MDP con los componentes fundamentales.

        Args:
            n_estados: |S| - Cardinalidad del espacio de estados
            n_acciones: |A| - Cardinalidad del espacio de acciones
            gamma: Factor de descuento Œ≥ ‚àà [0,1]
        """
        self.S = n_estados
        self.A = n_acciones
        self.gamma = gamma

        # Inicializaci√≥n de las funciones de transici√≥n y recompensa
        # P(s'|s,a) - Probabilidades de transici√≥n
        self.P = np.zeros((n_estados, n_acciones, n_estados))
        # R(s,a,s') - Funci√≥n de recompensa
        self.R = np.zeros((n_estados, n_acciones, n_estados))

        print(f"üéØ MDP creado exitosamente:")
        print(f"   Estados (|S|): {n_estados}")
        print(f"   Acciones (|A|): {n_acciones}")
        print(f"   Factor de descuento (Œ≥): {gamma}")

    def ecuacion_bellman(self, politica: np.ndarray, funcion_valor: np.ndarray) -> np.ndarray:
        """
        Implementa la ecuaci√≥n de Bellman para evaluaci√≥n de pol√≠ticas.

        Ecuaci√≥n: v^œÄ(s) = Œ£_a œÄ(a|s) Œ£_{s'} P(s'|s,a)[R(s,a,s') + Œ≥v^œÄ(s')]

        Args:
            politica: œÄ(a|s) - Matriz de pol√≠tica [S√óA]
            funcion_valor: v^œÄ(s) - Vector de funci√≥n de valor [S]

        Returns:
            Nueva funci√≥n de valor actualizada seg√∫n Bellman
        """
        nueva_v = np.zeros(self.S)

        for estado in range(self.S):
            valor_estado = 0.0

            # Sumar sobre todas las acciones ponderadas por la pol√≠tica
            for accion in range(self.A):
                prob_accion = politica[estado, accion]
                valor_accion = 0.0

                # Sumar sobre todos los estados siguientes
                for siguiente_estado in range(self.S):
                    prob_transicion = self.P[estado, accion, siguiente_estado]
                    recompensa = self.R[estado, accion, siguiente_estado]
                    valor_futuro = self.gamma * funcion_valor[siguiente_estado]

                    valor_accion += prob_transicion * (recompensa + valor_futuro)

                valor_estado += prob_accion * valor_accion

            nueva_v[estado] = valor_estado

        return nueva_v

# Crear un MDP de demostraci√≥n pedag√≥gica
print("\nüìä DEMOSTRACI√ìN: Evaluaci√≥n de Pol√≠ticas con Bellman")
mdp_demo = MDPEducativo(n_estados=3, n_acciones=2, gamma=0.9)

# Configurar din√°micas de transici√≥n determin√≠sticas para claridad conceptual
mdp_demo.P[0, 0, 1] = 1.0  # Desde estado 0, acci√≥n 0 ‚Üí estado 1 (probabilidad 1)
mdp_demo.P[0, 1, 2] = 1.0  # Desde estado 0, acci√≥n 1 ‚Üí estado 2 (probabilidad 1)
mdp_demo.P[1, 0, 0] = 1.0  # Desde estado 1, acci√≥n 0 ‚Üí estado 0 (ciclo)
mdp_demo.P[1, 1, 2] = 1.0  # Desde estado 1, acci√≥n 1 ‚Üí estado 2 (terminal)
mdp_demo.P[2, :, 2] = 1.0  # Estado 2 es absorbente (terminal)

# Configurar recompensas estrat√©gicamente
mdp_demo.R[1, 1, 2] = 1.0  # Recompensa por transici√≥n exitosa al estado terminal

# Definir pol√≠tica equiprobable para an√°lisis
politica_uniforme = np.ones((3, 2)) * 0.5
print(f"Pol√≠tica bajo an√°lisis: Equiprobable œÄ(a|s) = 0.5 ‚àÄs,a")

# Evaluaci√≥n iterativa de la pol√≠tica
funcion_v = np.zeros(3)
print(f"\nIteraci√≥n | V(s‚ÇÄ)   | V(s‚ÇÅ)   | V(s‚ÇÇ)   | Œî_max")
print("-" * 50)

for iteracion in range(15):
    v_anterior = funcion_v.copy()
    funcion_v = mdp_demo.ecuacion_bellman(politica_uniforme, funcion_v)
    delta_max = np.max(np.abs(funcion_v - v_anterior))

    if iteracion % 3 == 0:  # Mostrar cada 3 iteraciones para claridad
        print(f"   {iteracion:2d}     | {funcion_v[0]:.4f} | {funcion_v[1]:.4f} | {funcion_v[2]:.4f} | {delta_max:.6f}")

print(f"\nüéØ Convergencia alcanzada - Funci√≥n de valor estabilizada")
print(f"   Interpretaci√≥n: V(s‚ÇÄ)={funcion_v[0]:.4f} representa el valor esperado")
print(f"   de comenzar en el estado 0 y seguir la pol√≠tica equiprobable")

print("\n‚úÖ M√≥dulo 3 completado - Fundamentos de MDP establecidos")

# ============================================================================
# PAUSA PEDAG√ìGICA 2
# ============================================================================

print("\n" + "="*60)
print("üìö PAUSA PEDAG√ìGICA - AN√ÅLISIS DE LA ECUACI√ìN DE BELLMAN")
print("="*60)
print("""
PUNTOS CLAVE PARA DISCUSI√ìN:

1. INTERPRETACI√ìN MATEM√ÅTICA:
   - V(s‚ÇÄ) ‚âà 0.281 significa que comenzar en estado 0 tiene valor esperado 0.281
   - La convergencia demuestra que la ecuaci√≥n de Bellman tiene punto fijo √∫nico
   - Œ≥ = 0.9 descuenta las recompensas futuras apropiadamente

2. OBSERVACIONES ALGOR√çTMICAS:
   - La iteraci√≥n converge geom√©tricamente (propiedad de contracci√≥n)
   - El estado terminal V(s‚ÇÇ) = 0 como es esperado
   - V(s‚ÇÅ) > V(s‚ÇÄ) porque s‚ÇÅ est√° m√°s cerca de la recompensa

EXPERIMENTO SUGERIDO:
Cambie el factor de descuento Œ≥ y observe c√≥mo afecta los valores finales.
¬øQu√© sucede cuando Œ≥ ‚Üí 0? ¬øY cuando Œ≥ ‚Üí 1?
""")
print("="*60)

# ============================================================================
# M√ìDULO 4: ENTORNO GRIDWORLD EDUCATIVO
# ============================================================================

"""
## M√ìDULO 4: LABORATORIO VIRTUAL - ENTORNO GRIDWORLD

**OBJETIVO PEDAG√ìGICO:**
Crear un entorno visual e intuitivo donde los conceptos abstractos de
aprendizaje por refuerzo se vuelven tangibles y observables.

**CONCEPTOS CLAVE:**
- Mapeo entre estados abstractos y representaciones espaciales
- Dise√±o de funciones de recompensa
- Estados terminales y din√°micas de transici√≥n
"""

print("\nüåê M√ìDULO 4: Entorno GridWorld Educativo")
print("=" * 50)

import matplotlib.patches as patches

class GridWorldEducativo:
    """
    Entorno GridWorld dise√±ado espec√≠ficamente para ense√±anza, con
    visualizaciones claras y m√©tricas pedag√≥gicas integradas.
    """

    def __init__(self, altura: int = 5, ancho: int = 6):
        """
        Inicializa el entorno GridWorld con configuraci√≥n pedag√≥gica optimizada.

        Args:
            altura: N√∫mero de filas en la grilla
            ancho: N√∫mero de columnas en la grilla
        """
        self.altura = altura
        self.ancho = ancho
        self.n_estados = altura * ancho
        self.n_acciones = 4  # {Arriba, Derecha, Abajo, Izquierda}

        # Mapeo bidireccional entre coordenadas y estados
        self.estado_a_coord = {s: (s // ancho, s % ancho) for s in range(self.n_estados)}
        self.coord_a_estado = {coord: estado for estado, coord in self.estado_a_coord.items()}

        # Configuraci√≥n de estados especiales con justificaci√≥n pedag√≥gica
        fila_objetivo, col_objetivo = altura // 2, ancho - 2
        self.estado_objetivo = self.coord_a_estado[(fila_objetivo, col_objetivo)]

        # Posicionamiento estrat√©gico de trampas para crear dilemas interesantes
        posiciones_trampas = [
            (fila_objetivo + 1, col_objetivo),    # Sur del objetivo
            (fila_objetivo, col_objetivo - 1),    # Oeste del objetivo
            (fila_objetivo - 1, col_objetivo)     # Norte del objetivo
        ]

        self.estados_trampa = [self.coord_a_estado[pos] for pos in posiciones_trampas
                              if self._posicion_valida(*pos)]

        self.estados_terminales = {self.estado_objetivo} | set(self.estados_trampa)

        # Estado inicial y actual
        self.estado_inicial = 0
        self.estado_actual = self.estado_inicial

        # Definici√≥n de acciones con sem√°ntica clara
        self.acciones = {
            0: (-1, 0),  # Arriba
            1: (0, 1),   # Derecha
            2: (1, 0),   # Abajo
            3: (0, -1)   # Izquierda
        }

        self.nombres_acciones = ['‚Üë', '‚Üí', '‚Üì', '‚Üê']

        print(f"üèóÔ∏è  GridWorld {altura}√ó{ancho} creado exitosamente")
        print(f"   üìç Objetivo: Estado {self.estado_objetivo} (coordenadas {self.estado_a_coord[self.estado_objetivo]})")
        print(f"   ‚ö†Ô∏è  Trampas: Estados {self.estados_trampa}")
        print(f"   üèÅ Estados terminales: {len(self.estados_terminales)}")
        print(f"   üéØ Espacio de estados: {self.n_estados}")
        print(f"   üéÆ Espacio de acciones: {self.n_acciones}")

    def _posicion_valida(self, fila: int, col: int) -> bool:
        """Verifica si una posici√≥n est√° dentro de los l√≠mites."""
        return 0 <= fila < self.altura and 0 <= col < self.ancho

    def reset(self) -> int:
        """Reinicia el entorno al estado inicial."""
        self.estado_actual = self.estado_inicial
        return self.estado_actual

    def step(self, accion: int) -> Tuple[int, float, bool, dict]:
        """
        Ejecuta una acci√≥n y retorna la transici√≥n resultante.

        Args:
            accion: √çndice de la acci√≥n {0: ‚Üë, 1: ‚Üí, 2: ‚Üì, 3: ‚Üê}

        Returns:
            (pr√≥ximo_estado, recompensa, terminado, info)
        """
        if self.estado_actual in self.estados_terminales:
            return self.estado_actual, 0.0, True, {}

        # Calcular posici√≥n resultante
        fila_actual, col_actual = self.estado_a_coord[self.estado_actual]
        delta_fila, delta_col = self.acciones[accion]
        nueva_fila, nueva_col = fila_actual + delta_fila, col_actual + delta_col

        # Aplicar restricciones de fronteras
        if not self._posicion_valida(nueva_fila, nueva_col):
            nueva_fila, nueva_col = fila_actual, col_actual

        # Actualizar estado
        self.estado_actual = self.coord_a_estado[(nueva_fila, nueva_col)]

        # Calcular recompensa con dise√±o pedag√≥gico claro
        if self.estado_actual == self.estado_objetivo:
            recompensa = 1.0   # Recompensa positiva por √©xito
        elif self.estado_actual in self.estados_trampa:
            recompensa = -1.0  # Penalizaci√≥n por fracaso
        else:
            recompensa = 0.0   # Movimiento neutral

        terminado = self.estado_actual in self.estados_terminales

        info = {
            'accion_nombre': self.nombres_acciones[accion],
            'coordenadas': self.estado_a_coord[self.estado_actual]
        }

        return self.estado_actual, recompensa, terminado, info

    def visualizar(self, politica: Optional[np.ndarray] = None,
                  funcion_valor: Optional[np.ndarray] = None,
                  titulo: str = "Estado Actual del GridWorld"):
        """
        Renderiza el estado actual con informaci√≥n opcional de pol√≠tica y valores.

        Args:
            politica: Pol√≠tica a visualizar (opcional)
            funcion_valor: Funci√≥n de valor a mostrar (opcional)
            titulo: T√≠tulo del gr√°fico
        """
        fig, ax = plt.subplots(1, 1, figsize=(10, 8))

        # Dibujar grilla base
        for i in range(self.altura + 1):
            ax.axhline(y=i, color='black', linewidth=0.5)
        for j in range(self.ancho + 1):
            ax.axvline(x=j, color='black', linewidth=0.5)

        # Colorear objetivo
        fila_obj, col_obj = self.estado_a_coord[self.estado_objetivo]
        rect_objetivo = patches.Rectangle((col_obj, self.altura - fila_obj - 1), 1, 1,
                                        linewidth=2, edgecolor='gold', facecolor='yellow', alpha=0.7)
        ax.add_patch(rect_objetivo)
        ax.text(col_obj + 0.5, self.altura - fila_obj - 0.5, '‚òÖ',
                fontsize=20, ha='center', va='center', color='orange')

        # Dibujar trampas
        for estado_trampa in self.estados_trampa:
            fila_t, col_t = self.estado_a_coord[estado_trampa]
            rect_trampa = patches.Rectangle((col_t, self.altura - fila_t - 1), 1, 1,
                                          linewidth=2, edgecolor='red', facecolor='salmon', alpha=0.7)
            ax.add_patch(rect_trampa)
            ax.text(col_t + 0.5, self.altura - fila_t - 0.5, '‚úó',
                    fontsize=16, ha='center', va='center', color='darkred')

        # Dibujar agente actual
        fila_agente, col_agente = self.estado_a_coord[self.estado_actual]
        circulo_agente = patches.Circle((col_agente + 0.5, self.altura - fila_agente - 0.5),
                                      0.3, facecolor='blue', edgecolor='darkblue', linewidth=2)
        ax.add_patch(circulo_agente)

        # Mostrar funci√≥n de valor si se proporciona
        if funcion_valor is not None:
            for estado in range(self.n_estados):
                fila, col = self.estado_a_coord[estado]
                if estado not in self.estados_terminales:
                    ax.text(col + 0.2, self.altura - fila - 0.8, f'{funcion_valor[estado]:.2f}',
                           fontsize=8, ha='center', va='center', color='purple', weight='bold')

        # Mostrar pol√≠tica si se proporciona
        if politica is not None:
            for estado in range(self.n_estados):
                if estado not in self.estados_terminales:
                    mejor_accion = np.argmax(politica[estado])
                    fila, col = self.estado_a_coord[estado]
                    ax.text(col + 0.8, self.altura - fila - 0.2, self.nombres_acciones[mejor_accion],
                           fontsize=12, ha='center', va='center', color='green', weight='bold')

        ax.set_xlim(0, self.ancho)
        ax.set_ylim(0, self.altura)
        ax.set_aspect('equal')
        ax.set_title(titulo, fontsize=14, weight='bold')
        ax.set_xlabel('Coordenada X')
        ax.set_ylabel('Coordenada Y')

        plt.tight_layout()
        plt.show()

# Crear y demostrar el entorno
print("\nüéØ CREANDO ENTORNO DE DEMOSTRACI√ìN")
env_educativo = GridWorldEducativo(altura=5, ancho=6)
env_educativo.visualizar(titulo="GridWorld Inicial - Estados y Configuraci√≥n")

# Simulaci√≥n educativa de episodio aleatorio
print("\nüé≤ SIMULACI√ìN: Episodio con Pol√≠tica Aleatoria")
env_educativo.reset()
trayectoria_estados = [env_educativo.estado_actual]
trayectoria_recompensas = []

print(f"Estado inicial: {env_educativo.estado_actual} {env_educativo.estado_a_coord[env_educativo.estado_actual]}")

for paso in range(15):  # L√≠mite para evitar bucles infinitos
    accion = np.random.randint(0, 4)
    siguiente_estado, recompensa, terminado, info = env_educativo.step(accion)

    trayectoria_estados.append(siguiente_estado)
    trayectoria_recompensas.append(recompensa)

    print(f"Paso {paso+1}: {info['accion_nombre']} ‚Üí Estado {siguiente_estado} {info['coordenadas']}, R={recompensa}")

    if terminado:
        print(f"üèÅ Episodio terminado en {paso+1} pasos")
        print(f"üìä Recompensa total: {sum(trayectoria_recompensas):.1f}")
        resultado = "√âXITO" if recompensa > 0 else "FRACASO"
        print(f"üéØ Resultado: {resultado}")
        break

print("\n‚úÖ M√≥dulo 4 completado - Entorno GridWorld establecido")

# ============================================================================
# PAUSA PEDAG√ìGICA 3
# ============================================================================

print("\n" + "="*60)
print("üìö PAUSA PEDAG√ìGICA - AN√ÅLISIS DEL ENTORNO GRIDWORLD")
print("="*60)
print("""
CONCEPTOS FUNDAMENTALES OBSERVADOS:

1. DISE√ëO DE RECOMPENSAS:
   - Objetivo (+1): Incentiva el comportamiento deseado
   - Trampas (-1): Penaliza errores y crea riesgo
   - Movimientos neutros (0): No sesgan la exploraci√≥n

2. DIN√ÅMICAS ESPACIALES:
   - Estados terminales crean episodios finitos
   - Restricciones de fronteras mantienen validez
   - Proximidad espacial afecta la dificultad del aprendizaje

3. INTERPRETACI√ìN DE TRAYECTORIAS:
   - Pol√≠tica aleatoria muestra comportamiento de l√≠nea base
   - Longitud del episodio indica eficiencia
   - Patr√≥n de movimientos revela estructura del problema

PREGUNTAS PARA REFLEXI√ìN:
- ¬øC√≥mo afecta la posici√≥n de las trampas al aprendizaje?
- ¬øQu√© estrategias podr√≠a desarrollar un agente inteligente?
- ¬øC√≥mo cambiar√≠a el problema con diferentes recompensas?
""")
print("="*60)

print("\nüéì ¬°Preparado para el siguiente m√≥dulo!")
print("En el pr√≥ximo bloque implementaremos Q-Learning tabular...")

# ============================================================================
# M√ìDULO 5: IMPLEMENTACI√ìN DE Q-LEARNING TABULAR
# ============================================================================

"""
## M√ìDULO 5: Q-LEARNING TABULAR - ALGORITMO FUNDAMENTAL

**OBJETIVO PEDAG√ìGICO:**
Implementar el algoritmo Q-Learning paso a paso, comprendiendo cada componente
y observando c√≥mo emerge el comportamiento inteligente desde la exploraci√≥n aleatoria.

**CONCEPTOS CLAVE:**
- Regla de actualizaci√≥n Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
- Balance exploraci√≥n-explotaci√≥n mediante Œµ-greedy
- Convergencia hacia la funci√≥n Q √≥ptima
- Construcci√≥n incremental de la pol√≠tica √≥ptima
"""

print("\nü§ñ M√ìDULO 5: Q-Learning Tabular - Implementaci√≥n Detallada")
print("=" * 55)

class AgenteQLearning:
    """
    Implementaci√≥n educativa de Q-Learning con instrumentaci√≥n pedag√≥gica completa.

    Esta clase est√° dise√±ada para revelar cada aspecto del proceso de aprendizaje,
    permitiendo a los estudiantes observar c√≥mo emerge la inteligencia artificial
    a trav√©s de la experiencia acumulada.
    """

    def __init__(self, n_estados: int, n_acciones: int,
                 tasa_aprendizaje: float = 0.1, factor_descuento: float = 0.9,
                 epsilon_inicial: float = 1.0, decaimiento_epsilon: float = 0.995,
                 epsilon_minimo: float = 0.01):
        """
        Inicializa el agente Q-Learning con par√°metros educativamente optimizados.

        Args:
            n_estados: Cardinalidad del espacio de estados |S|
            n_acciones: Cardinalidad del espacio de acciones |A|
            tasa_aprendizaje: Œ± - Qu√© tan r√°pido aprende de nuevas experiencias
            factor_descuento: Œ≥ - Importancia de recompensas futuras vs inmediatas
            epsilon_inicial: Probabilidad inicial de exploraci√≥n
            decaimiento_epsilon: Factor de reducci√≥n de exploraci√≥n por episodio
            epsilon_minimo: Nivel m√≠nimo de exploraci√≥n para mantener adaptabilidad
        """
        self.n_estados = n_estados
        self.n_acciones = n_acciones
        self.alpha = tasa_aprendizaje
        self.gamma = factor_descuento
        self.epsilon = epsilon_inicial
        self.decaimiento_epsilon = decaimiento_epsilon
        self.epsilon_minimo = epsilon_minimo

        # Inicializaci√≥n optimista de la tabla Q para fomentar exploraci√≥n
        self.tabla_q = defaultdict(lambda: np.zeros(n_acciones))

        # M√©tricas pedag√≥gicas para an√°lisis del proceso de aprendizaje
        self.historial_recompensas = []
        self.historial_longitudes = []
        self.historial_epsilon = []
        self.historial_errores_td = []
        self.contador_actualizaciones = 0

        print(f"üß† Agente Q-Learning inicializado:")
        print(f"   üìä Espacio de estados: {n_estados}")
        print(f"   üéÆ Espacio de acciones: {n_acciones}")
        print(f"   üìà Tasa de aprendizaje (Œ±): {tasa_aprendizaje}")
        print(f"   ‚è∞ Factor de descuento (Œ≥): {factor_descuento}")
        print(f"   üîç Exploraci√≥n inicial (Œµ): {epsilon_inicial}")

    def seleccionar_accion(self, estado: int, modo_entrenamiento: bool = True) -> int:
        """
        Implementa la estrategia Œµ-greedy para balance exploraci√≥n-explotaci√≥n.

        La pol√≠tica Œµ-greedy constituye una soluci√≥n elegante al dilema fundamental
        del aprendizaje por refuerzo: c√≥mo balancear la exploraci√≥n de nuevas
        posibilidades contra la explotaci√≥n del conocimiento actual.

        Args:
            estado: Estado actual del agente
            modo_entrenamiento: Si aplica exploraci√≥n (True) o usa pol√≠tica greedy (False)

        Returns:
            Acci√≥n seleccionada seg√∫n la estrategia Œµ-greedy
        """
        if modo_entrenamiento and np.random.random() < self.epsilon:
            # EXPLORACI√ìN: Selecci√≥n aleatoria para descubrir nuevas estrategias
            accion = np.random.randint(0, self.n_acciones)
            tipo_seleccion = "exploraci√≥n"
        else:
            # EXPLOTACI√ìN: Selecci√≥n greedy basada en conocimiento actual
            valores_q = self.tabla_q[estado]
            # Manejo de empates mediante selecci√≥n aleatoria para robustez
            max_q = np.max(valores_q)
            mejores_acciones = np.where(valores_q == max_q)[0]
            accion = np.random.choice(mejores_acciones)
            tipo_seleccion = "explotaci√≥n"

        # Registro para an√°lisis pedag√≥gico
        if hasattr(self, 'debug_mode') and self.debug_mode:
            print(f"   Estado {estado}: {tipo_seleccion} ‚Üí acci√≥n {accion}")

        return accion

    def actualizar_q(self, estado: int, accion: int, recompensa: float,
                    siguiente_estado: int, terminado: bool) -> float:
        """
        Implementa la regla de actualizaci√≥n fundamental de Q-Learning.

        Esta funci√≥n encapsula la esencia matem√°tica del aprendizaje por refuerzo:
        Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max_{a'} Q(s',a') - Q(s,a)]

        El t√©rmino entre corchetes representa el "error de diferencia temporal" (TD error),
        que mide qu√© tan equivocada estaba nuestra estimaci√≥n previa.

        Args:
            estado: Estado actual s
            accion: Acci√≥n ejecutada a
            recompensa: Recompensa observada r
            siguiente_estado: Estado resultante s'
            terminado: Si el episodio termin√≥

        Returns:
            Magnitud del error TD para an√°lisis de convergencia
        """
        # Valor Q actual para la tupla (estado, acci√≥n)
        q_actual = self.tabla_q[estado][accion]

        # C√°lculo del objetivo (target) seg√∫n la ecuaci√≥n de Q-Learning
        if terminado:
            # En estados terminales, no hay valor futuro
            q_objetivo = recompensa
        else:
            # En estados no terminales, incluimos el valor futuro √≥ptimo descontado
            q_objetivo = recompensa + self.gamma * np.max(self.tabla_q[siguiente_estado])

        # Error de diferencia temporal: diferencia entre target y predicci√≥n actual
        error_td = q_objetivo - q_actual

        # Actualizaci√≥n incremental de la funci√≥n Q
        self.tabla_q[estado][accion] += self.alpha * error_td

        # Registro de m√©tricas para an√°lisis pedag√≥gico
        self.contador_actualizaciones += 1
        self.historial_errores_td.append(abs(error_td))

        return abs(error_td)

    def reducir_epsilon(self):
        """
        Implementa el decaimiento gradual de la exploraci√≥n.

        A medida que el agente acumula experiencia, reduce gradualmente la exploraci√≥n
        para converger hacia una pol√≠tica determin√≠stica √≥ptima.
        """
        if self.epsilon > self.epsilon_minimo:
            self.epsilon *= self.decaimiento_epsilon

    def extraer_politica(self) -> np.ndarray:
        """
        Extrae la pol√≠tica determin√≠stica greedy de la funci√≥n Q actual.

        Returns:
            Matriz de pol√≠tica œÄ(a|s) donde cada fila suma 1
        """
        politica = np.zeros((self.n_estados, self.n_acciones))

        for estado in range(self.n_estados):
            valores_q = self.tabla_q[estado]
            mejor_accion = np.argmax(valores_q)
            politica[estado, mejor_accion] = 1.0

        return politica

    def extraer_funcion_valor(self) -> np.ndarray:
        """
        Calcula la funci√≥n de valor de estado V(s) = max_a Q(s,a).

        Returns:
            Vector de funci√≥n de valor para visualizaci√≥n
        """
        return np.array([np.max(self.tabla_q[estado]) for estado in range(self.n_estados)])

    def obtener_estadisticas_aprendizaje(self) -> Dict:
        """
        Proporciona m√©tricas comprehensivas del proceso de aprendizaje.

        Returns:
            Diccionario con estad√≠sticas detalladas para an√°lisis pedag√≥gico
        """
        return {
            'total_actualizaciones': self.contador_actualizaciones,
            'epsilon_actual': self.epsilon,
            'error_td_promedio': np.mean(self.historial_errores_td[-100:]) if self.historial_errores_td else 0,
            'convergencia_reciente': np.std(self.historial_errores_td[-50:]) if len(self.historial_errores_td) >= 50 else float('inf')
        }

# Demostraci√≥n interactiva de los componentes del agente
print("\nüîç DEMOSTRACI√ìN: Componentes del Agente Q-Learning")

# Crear agente con par√°metros educativos
agente_demo = AgenteQLearning(n_estados=env_educativo.n_estados,
                             n_acciones=env_educativo.n_acciones,
                             tasa_aprendizaje=0.1, factor_descuento=0.9)

# Activar modo debug para mostrar decisiones
agente_demo.debug_mode = True

print(f"\nüìã Simulaci√≥n de decisiones iniciales (tabla Q vac√≠a):")
for i in range(3):
    estado_ejemplo = np.random.randint(0, 5)  # Algunos estados de ejemplo
    accion = agente_demo.seleccionar_accion(estado_ejemplo)
    print(f"   Estado {estado_ejemplo}: Q = {agente_demo.tabla_q[estado_ejemplo]} ‚Üí Acci√≥n {accion}")

# Desactivar modo debug
agente_demo.debug_mode = False

print(f"\n‚úÖ M√≥dulo 5 completado - Agente Q-Learning implementado")

# ============================================================================
# PAUSA PEDAG√ìGICA 4
# ============================================================================

print("\n" + "="*60)
print("üìö PAUSA PEDAG√ìGICA - ANATOM√çA DEL Q-LEARNING")
print("="*60)
print("""
CONCEPTOS FUNDAMENTALES IMPLEMENTADOS:

1. REGLA DE ACTUALIZACI√ìN Q-LEARNING:
   - Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
   - Œ± controla velocidad de aprendizaje (0 = no aprende, 1 = olvida todo lo anterior)
   - Œ≥ determina importancia del futuro (0 = miope, 1 = visionario)
   - Error TD mide qu√© tan equivocada estaba nuestra estimaci√≥n

2. EXPLORACI√ìN vs EXPLOTACI√ìN:
   - Œµ-greedy balancea descubrimiento de nuevas estrategias vs uso del conocimiento actual
   - Decaimiento de Œµ: alta exploraci√≥n inicial ‚Üí explotaci√≥n gradual
   - Sin exploraci√≥n: convergencia prematura a pol√≠ticas sub√≥ptimas

3. PROCESO DE APRENDIZAJE:
   - Tabla Q inicialmente vac√≠a (o con valores optimistas)
   - Cada experiencia (s,a,r,s') actualiza una entrada espec√≠fica
   - Convergencia gradual hacia Q*(s,a) - funci√≥n Q √≥ptima

EXPERIMENTO MENTAL:
¬øQu√© pasar√≠a si Œ± = 0? ¬øY si Œµ = 0 desde el inicio?
¬øC√≥mo afecta Œ≥ a la estrategia del agente (miope vs planificador)?
""")
print("="*60)

# ============================================================================
# M√ìDULO 6: ENTRENAMIENTO Y AN√ÅLISIS DE Q-LEARNING
# ============================================================================

"""
## M√ìDULO 6: ENTRENAMIENTO SUPERVISADO DE Q-LEARNING

**OBJETIVO PEDAG√ìGICO:**
Observar el proceso completo de aprendizaje, desde la exploraci√≥n aleatoria inicial
hasta la emergencia de comportamiento inteligente y la convergencia a pol√≠ticas √≥ptimas.

**CONCEPTOS CLAVE:**
- Curvas de aprendizaje y m√©tricas de convergencia
- Evoluci√≥n de la exploraci√≥n a lo largo del tiempo
- An√°lisis de la pol√≠tica emergente
- Evaluaci√≥n sistem√°tica del rendimiento
"""

print("\nüéì M√ìDULO 6: Entrenamiento Supervisado de Q-Learning")
print("=" * 50)

def entrenar_agente_qlearning(env, agente, n_episodios: int = 300,
                             mostrar_progreso: bool = True,
                             intervalo_reporte: int = 50) -> Dict:
    """
    Funci√≥n educativa para entrenar un agente Q-Learning con an√°lisis pedag√≥gico completo.

    Esta funci√≥n no solo entrena al agente, sino que proporciona insights detallados
    sobre cada fase del proceso de aprendizaje, desde la exploraci√≥n inicial hasta
    la convergencia hacia comportamiento √≥ptimo.

    Args:
        env: Entorno de aprendizaje por refuerzo
        agente: Agente Q-Learning a entrenar
        n_episodios: N√∫mero total de episodios de entrenamiento
        mostrar_progreso: Mostrar reportes peri√≥dicos del progreso
        intervalo_reporte: Frecuencia de reportes (cada N episodios)

    Returns:
        Diccionario comprehensivo con m√©tricas de entrenamiento
    """
    print(f"üöÄ INICIANDO PROCESO DE ENTRENAMIENTO Q-LEARNING")
    print(f"üìä Configuraci√≥n: {n_episodios} episodios, reportes cada {intervalo_reporte}")
    print("-" * 60)

    # Contenedores para m√©tricas pedag√≥gicas
    metricas_entrenamiento = {
        'recompensas_episodio': [],
        'longitudes_episodio': [],
        'valores_epsilon': [],
        'errores_td_promedio': [],
        'estados_visitados': [],
        'tiempo_convergencia': None
    }

    # Variables para detecci√≥n de convergencia
    ventana_recompensas = deque(maxlen=100)
    mejor_promedio = -np.inf
    episodios_estables = 0
    umbral_convergencia = 0.8  # Recompensa promedio para considerar convergencia

    print(f"üéØ Iniciando entrenamiento...")
    tiempo_inicio = time.time()

    for episodio in range(n_episodios):
        # Reiniciar entorno para nuevo episodio
        estado = env.reset()
        recompensa_episodio = 0
        longitud_episodio = 0
        errores_td_episodio = []
        estados_visitados_episodio = set()

        # Ejecutar episodio completo
        while True:
            # Seleccionar acci√≥n seg√∫n pol√≠tica Œµ-greedy
            accion = agente.seleccionar_accion(estado, modo_entrenamiento=True)

            # Ejecutar acci√≥n en el entorno
            siguiente_estado, recompensa, terminado, info = env.step(accion)

            # Actualizar funci√≥n Q y registrar error TD
            error_td = agente.actualizar_q(estado, accion, recompensa,
                                         siguiente_estado, terminado)
            errores_td_episodio.append(error_td)

            # Registrar m√©tricas del episodio
            recompensa_episodio += recompensa
            longitud_episodio += 1
            estados_visitados_episodio.add(estado)

            # Preparar para siguiente iteraci√≥n
            estado = siguiente_estado

            # Verificar terminaci√≥n
            if terminado or longitud_episodio > 100:  # Evitar episodios infinitos
                break

        # Actualizar exploraci√≥n
        agente.reducir_epsilon()

        # Registrar m√©tricas del episodio
        metricas_entrenamiento['recompensas_episodio'].append(recompensa_episodio)
        metricas_entrenamiento['longitudes_episodio'].append(longitud_episodio)
        metricas_entrenamiento['valores_epsilon'].append(agente.epsilon)
        metricas_entrenamiento['errores_td_promedio'].append(np.mean(errores_td_episodio))
        metricas_entrenamiento['estados_visitados'].append(len(estados_visitados_episodio))

        # An√°lisis de convergencia
        ventana_recompensas.append(recompensa_episodio)
        promedio_reciente = np.mean(ventana_recompensas) if ventana_recompensas else 0

        # Detecci√≥n de convergencia
        if promedio_reciente > mejor_promedio and len(ventana_recompensas) == 100:
            mejor_promedio = promedio_reciente
            if promedio_reciente >= umbral_convergencia:
                episodios_estables += 1
                if episodios_estables >= 20 and metricas_entrenamiento['tiempo_convergencia'] is None:
                    metricas_entrenamiento['tiempo_convergencia'] = episodio
                    if mostrar_progreso:
                        print(f"üéØ ¬°CONVERGENCIA DETECTADA en episodio {episodio}!")

        # Reportes peri√≥dicos con an√°lisis pedag√≥gico
        if mostrar_progreso and (episodio + 1) % intervalo_reporte == 0:
            stats = agente.obtener_estadisticas_aprendizaje()
            exploracion_actual = f"{agente.epsilon:.3f}"
            cobertura_estados = len(set(range(env.n_estados)) &
                                   set().union(*[{s} for s in metricas_entrenamiento['estados_visitados'][-intervalo_reporte:]]))

            print(f"üìà Episodio {episodio + 1:3d} | "
                  f"R: {recompensa_episodio:5.1f} | "
                  f"L: {longitud_episodio:3d} | "
                  f"Œµ: {exploracion_actual} | "
                  f"Prom(100): {promedio_reciente:5.2f} | "
                  f"Estados: {cobertura_estados:2d}/{env.n_estados}")

    tiempo_total = time.time() - tiempo_inicio

    print(f"\n‚úÖ ENTRENAMIENTO COMPLETADO")
    print(f"   ‚è±Ô∏è  Tiempo total: {tiempo_total:.1f}s")
    print(f"   üîÑ Actualizaciones Q: {agente.contador_actualizaciones:,}")
    print(f"   üéØ Exploraci√≥n final: {agente.epsilon:.4f}")
    print(f"   üìä Rendimiento final (100 ep.): {np.mean(ventana_recompensas):.3f}")

    return metricas_entrenamiento

# Ejecutar entrenamiento con nuestro agente y entorno
print(f"\nüéÆ Iniciando entrenamiento en GridWorld {env_educativo.altura}√ó{env_educativo.ancho}")
agente_qlearning = AgenteQLearning(n_estados=env_educativo.n_estados,
                                  n_acciones=env_educativo.n_acciones)

resultados_entrenamiento = entrenar_agente_qlearning(env_educativo, agente_qlearning,
                                                   n_episodios=300, mostrar_progreso=True)

print("\n‚úÖ M√≥dulo 6 completado - Entrenamiento Q-Learning finalizado")

# ============================================================================
# M√ìDULO 6B: AN√ÅLISIS Y VISUALIZACI√ìN DE RESULTADOS
# ============================================================================

"""
## M√ìDULO 6B: AN√ÅLISIS PROFUNDO DE LOS RESULTADOS DE Q-LEARNING

**OBJETIVO PEDAG√ìGICO:**
Desarrollar habilidades de an√°lisis de datos de aprendizaje autom√°tico y
comprensi√≥n profunda de las m√©tricas de convergencia y rendimiento.
"""

print("\nüìä M√ìDULO 6B: An√°lisis de Resultados Q-Learning")
print("=" * 50)

def visualizar_analisis_qlearning(metricas: Dict, agente: AgenteQLearning, env):
    """
    Genera visualizaciones educativas comprehensivas del proceso de aprendizaje.

    Args:
        metricas: Diccionario con m√©tricas de entrenamiento
        agente: Agente entrenado para extraer pol√≠tica
        env: Entorno para visualizaci√≥n de pol√≠tica
    """

    # Configurar subplots para an√°lisis m√∫ltiple
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    episodios = range(1, len(metricas['recompensas_episodio']) + 1)

    # 1. Evoluci√≥n de recompensas con media m√≥vil
    axes[0,0].plot(episodios, metricas['recompensas_episodio'], alpha=0.3,
                  color='blue', label='Recompensa por episodio')

    # Calcular y mostrar media m√≥vil
    ventana = 20
    if len(metricas['recompensas_episodio']) >= ventana:
        media_movil = np.convolve(metricas['recompensas_episodio'],
                                np.ones(ventana)/ventana, mode='valid')
        axes[0,0].plot(episodios[ventana-1:], media_movil, color='red',
                      linewidth=2, label=f'Media m√≥vil ({ventana} ep.)')

    axes[0,0].set_title('Evoluci√≥n del Rendimiento', fontweight='bold')
    axes[0,0].set_xlabel('Episodio')
    axes[0,0].set_ylabel('Recompensa Acumulada')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)

    # 2. Eficiencia del agente (longitud de episodios)
    axes[0,1].plot(episodios, metricas['longitudes_episodio'], color='green', alpha=0.7)
    axes[0,1].set_title('Eficiencia del Agente', fontweight='bold')
    axes[0,1].set_xlabel('Episodio')
    axes[0,1].set_ylabel('Pasos hasta Terminaci√≥n')
    axes[0,1].grid(True, alpha=0.3)

    # 3. Decaimiento de exploraci√≥n
    axes[1,0].plot(episodios, metricas['valores_epsilon'], color='orange', linewidth=2)
    axes[1,0].set_title('Evoluci√≥n de la Exploraci√≥n', fontweight='bold')
    axes[1,0].set_xlabel('Episodio')
    axes[1,0].set_ylabel('Epsilon (Œµ)')
    axes[1,0].grid(True, alpha=0.3)

    # 4. Convergencia del algoritmo (errores TD)
    axes[1,1].plot(episodios, metricas['errores_td_promedio'], color='purple', alpha=0.7)
    axes[1,1].set_title('Convergencia del Algoritmo', fontweight='bold')
    axes[1,1].set_xlabel('Episodio')
    axes[1,1].set_ylabel('Error TD Promedio')
    axes[1,1].grid(True, alpha=0.3)

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.suptitle('An√°lisis Comprehensivo: Entrenamiento Q-Learning',
                fontsize=16, fontweight='bold', y=0.98)
    plt.show()

# Generar an√°lisis visual
visualizar_analisis_qlearning(resultados_entrenamiento, agente_qlearning, env_educativo)

# Extraer y mostrar la pol√≠tica aprendida
print(f"\nüß† AN√ÅLISIS DE LA POL√çTICA APRENDIDA:")
politica_aprendida = agente_qlearning.extraer_politica()
funcion_valor = agente_qlearning.extraer_funcion_valor()

print(f"   üìä Estados explorados: {len([s for s in range(env_educativo.n_estados) if np.any(politica_aprendida[s])])}")
print(f"   üéØ Valor m√°ximo alcanzado: {np.max(funcion_valor):.3f}")
print(f"   üìà Rango de valores: [{np.min(funcion_valor):.3f}, {np.max(funcion_valor):.3f}]")

# Visualizar pol√≠tica en el entorno
env_educativo.reset()
env_educativo.visualizar(politica=politica_aprendida, funcion_valor=funcion_valor,
                        titulo="Pol√≠tica √ìptima Aprendida - Q-Learning Tabular")

print("\n‚úÖ M√≥dulo 6B completado - An√°lisis de resultados Q-Learning")

# ============================================================================
# PAUSA PEDAG√ìGICA 5
# ============================================================================

print("\n" + "="*60)
print("üìö PAUSA PEDAG√ìGICA - INTERPRETACI√ìN DE RESULTADOS Q-LEARNING")
print("="*60)
print("""
AN√ÅLISIS DE LAS VISUALIZACIONES:

1. CURVA DE APRENDIZAJE (Recompensas):
   - Tendencia ascendente indica aprendizaje exitoso
   - Variabilidad inicial refleja exploraci√≥n
   - Estabilizaci√≥n sugiere convergencia a pol√≠tica √≥ptima

2. EFICIENCIA (Longitud de episodios):
   - Disminuci√≥n indica que el agente encuentra rutas m√°s directas
   - Estabilizaci√≥n en valores bajos = pol√≠tica eficiente
   - Picos ocasionales = exploraci√≥n residual (Œµ > 0)

3. EXPLORACI√ìN (Decaimiento de Œµ):
   - Reducci√≥n exponencial balanceada
   - Nunca llega a cero = mantiene adaptabilidad
   - Tasa de decaimiento afecta velocidad de convergencia

4. CONVERGENCIA (Errores TD):
   - Tendencia descendente = funci√≥n Q se estabiliza
   - Fluctuaciones = variabilidad natural del aprendizaje
   - Valores cercanos a cero = pol√≠tica cerca del √≥ptimo

PREGUNTAS DE REFLEXI√ìN:
- ¬øQu√© indica si la recompensa promedio se estanca?
- ¬øPor qu√© mantener Œµ > 0 incluso despu√©s de convergencia?
- ¬øC√≥mo afectar√≠a un Œ± muy alto o muy bajo?
""")
print("="*60)

# ============================================================================
# M√ìDULO 7: DEEP Q-NETWORKS - TEOR√çA E IMPLEMENTACI√ìN
# ============================================================================

"""
## M√ìDULO 7: DEEP Q-NETWORKS (DQN) - APROXIMACI√ìN FUNCIONAL

**OBJETIVO PEDAG√ìGICO:**
Comprender la transici√≥n de m√©todos tabulares a aproximaci√≥n funcional,
implementando las innovaciones clave que hacen posible el Deep Q-Learning.

**CONCEPTOS CLAVE:**
- Aproximaci√≥n funcional Q_Œ∏(s,a) mediante redes neuronales
- Experience Replay para romper correlaciones temporales
- Target Network para estabilizaci√≥n del entrenamiento
- Optimizaci√≥n de redes neuronales para RL
"""

print("\nüß† M√ìDULO 7: Deep Q-Networks - Aproximaci√≥n Funcional")
print("=" * 55)

# Detectar y configurar entorno CartPole
def crear_entorno_cartpole():
    """
    Crea entorno CartPole con compatibilidad multi-versi√≥n.

    Returns:
        Tupla (entorno, tipo_gym, dimensiones)
    """
    try:
        import gymnasium as gym
        env = gym.make('CartPole-v1', render_mode=None)
        tipo = 'gymnasium'
        print("‚úÖ Utilizando Gymnasium para CartPole")
    except ImportError:
        import gym
        env = gym.make('CartPole-v1')
        tipo = 'gym'
        print("‚úÖ Utilizando OpenAI Gym para CartPole")

    # Obtener dimensiones del problema
    if tipo == 'gymnasium':
        dim_estado = env.observation_space.shape[0]
        dim_accion = env.action_space.n
    else:
        dim_estado = env.observation_space.shape[0]
        dim_accion = env.action_space.n

    print(f"   üéØ Estados: {dim_estado} dimensiones continuas")
    print(f"   üéÆ Acciones: {dim_accion} (izquierda/derecha)")

    return env, tipo, dim_estado, dim_accion

# Crear entorno CartPole
env_cartpole, tipo_gym, dim_estados, dim_acciones = crear_entorno_cartpole()

class RedQProfunda(nn.Module):
    """
    Arquitectura de red neuronal optimizada para aproximaci√≥n de funci√≥n Q.

    Esta implementaci√≥n utiliza principios de dise√±o probados en literatura:
    - Capas densas con activaciones ReLU para capacidad expresiva
    - Dimensiones decrecientes para compresi√≥n jer√°rquica
    - Dropout para regularizaci√≥n y prevenci√≥n de sobreajuste
    """

    def __init__(self, dim_entrada: int, dim_salida: int,
                 capas_ocultas: List[int] = [256, 128, 64]):
        """
        Inicializa la arquitectura de la red Q.

        Args:
            dim_entrada: Dimensionalidad del espacio de estados
            dim_salida: N√∫mero de acciones disponibles
            capas_ocultas: Lista con dimensiones de capas ocultas
        """
        super(RedQProfunda, self).__init__()

        # Construcci√≥n din√°mica de la arquitectura
        capas = []
        dim_previa = dim_entrada

        for dim_oculta in capas_ocultas:
            capas.extend([
                nn.Linear(dim_previa, dim_oculta),
                nn.ReLU(),
                nn.Dropout(0.1)  # Regularizaci√≥n ligera
            ])
            dim_previa = dim_oculta

        # Capa de salida sin activaci√≥n (valores Q pueden ser negativos)
        capas.append(nn.Linear(dim_previa, dim_salida))

        self.red = nn.Sequential(*capas)

        # Inicializaci√≥n Xavier para convergencia estable
        self._inicializar_pesos()

        # Calcular par√°metros totales para reporte
        total_params = sum(p.numel() for p in self.parameters())

        print(f"üèóÔ∏è  Red DQN construida:")
        print(f"   üìê Arquitectura: {[dim_entrada] + capas_ocultas + [dim_salida]}")
        print(f"   üìä Par√°metros totales: {total_params:,}")
        print(f"   üéØ Entrada: {dim_entrada}D ‚Üí Salida: {dim_salida} valores Q")

    def _inicializar_pesos(self):
        """Inicializaci√≥n Xavier/Glorot para estabilidad de entrenamiento."""
        for modulo in self.modules():
            if isinstance(modulo, nn.Linear):
                nn.init.xavier_uniform_(modulo.weight)
                nn.init.constant_(modulo.bias, 0.0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Propagaci√≥n hacia adelante para calcular valores Q.

        Args:
            x: Tensor de estados [batch_size, dim_entrada]

        Returns:
            Valores Q para todas las acciones [batch_size, dim_salida]
        """
        return self.red(x)

class BufferExperienceReplay:
    """
    Buffer circular para Experience Replay con muestreo eficiente.

    Experience Replay constituye una innovaci√≥n fundamental en DQN que rompe
    la correlaci√≥n temporal entre experiencias consecutivas, permitiendo
    un aprendizaje m√°s estable y eficiente de la funci√≥n Q.
    """

    def __init__(self, capacidad: int = 10000):
        """
        Inicializa buffer circular de experiencias.

        Args:
            capacidad: N√∫mero m√°ximo de transiciones a almacenar
        """
        self.buffer = deque(maxlen=capacidad)
        self.Transicion = namedtuple('Transicion',
                                   ['estado', 'accion', 'recompensa', 'siguiente_estado', 'terminado'])

        print(f"üíæ Buffer Experience Replay creado:")
        print(f"   üìä Capacidad: {capacidad:,} transiciones")
        print(f"   üîÑ Tipo: Buffer circular (FIFO cuando lleno)")

    def almacenar(self, estado: np.ndarray, accion: int, recompensa: float,
                 siguiente_estado: np.ndarray, terminado: bool):
        """
        Almacena nueva transici√≥n en el buffer.

        Args:
            estado: Estado actual
            accion: Acci√≥n ejecutada
            recompensa: Recompensa observada
            siguiente_estado: Estado resultante
            terminado: Flag de terminaci√≥n del episodio
        """
        transicion = self.Transicion(estado, accion, recompensa, siguiente_estado, terminado)
        self.buffer.append(transicion)

    def muestrear(self, tama√±o_lote: int) -> List:
        """
        Muestrea aleatoriamente un lote de transiciones.

        Args:
            tama√±o_lote: N√∫mero de transiciones a muestrear

        Returns:
            Lista de transiciones muestreadas aleatoriamente
        """
        return random.sample(self.buffer, min(tama√±o_lote, len(self.buffer)))

    def __len__(self) -> int:
        """Retorna n√∫mero actual de transiciones almacenadas."""
        return len(self.buffer)

# Demostrar componentes DQN
print(f"\nüî¨ DEMOSTRACI√ìN: Componentes de Deep Q-Network")

# Crear red neuronal de ejemplo
red_demo = RedQProfunda(dim_entrada=dim_estados, dim_salida=dim_acciones)

# Crear buffer de experience replay
buffer_demo = BufferExperienceReplay(capacidad=5000)

# Simular almacenamiento de experiencias
print(f"\nüìù Simulando almacenamiento de experiencias:")
for i in range(10):
    estado_falso = np.random.random(dim_estados).astype(np.float32)
    accion_falsa = np.random.randint(0, dim_acciones)
    recompensa_falsa = np.random.random()
    siguiente_estado_falso = np.random.random(dim_estados).astype(np.float32)
    terminado_falso = np.random.random() < 0.1

    buffer_demo.almacenar(estado_falso, accion_falsa, recompensa_falsa,
                         siguiente_estado_falso, terminado_falso)

print(f"   üìä Experiencias almacenadas: {len(buffer_demo)}")

# Demostrar muestreo
lote_ejemplo = buffer_demo.muestrear(3)
print(f"   üé≤ Lote muestreado: {len(lote_ejemplo)} transiciones")

# Demostrar propagaci√≥n hacia adelante
with torch.no_grad():
    estado_tensor = torch.FloatTensor(estado_falso).unsqueeze(0)
    valores_q = red_demo(estado_tensor)
    print(f"   üß† Salida de la red: {valores_q.shape} ‚Üí Q-values: {valores_q.squeeze().numpy()}")

print("\n‚úÖ M√≥dulo 7 completado - Componentes DQN implementados")

# ============================================================================
# PAUSA PEDAG√ìGICA 6
# ============================================================================

print("\n" + "="*60)
print("üìö PAUSA PEDAG√ìGICA - INNOVACIONES EN DEEP Q-NETWORKS")
print("="*60)
print("""
TRANSICI√ìN: TABULAR ‚Üí APROXIMACI√ìN FUNCIONAL

1. LIMITACIONES DEL Q-LEARNING TABULAR:
   - Escalabilidad: |S| √ó |A| crece exponencialmente
   - Generalizaci√≥n: No aprovecha similitudes entre estados
   - Memoria: Almacenamiento prohibitivo para espacios grandes

2. SOLUCI√ìN: APROXIMACI√ìN FUNCIONAL
   - Q_Œ∏(s,a): Red neuronal parametrizada por Œ∏
   - Generalizaci√≥n autom√°tica a estados no visitados
   - Capacidad expresiva para funciones complejas

3. DESAF√çOS Y SOLUCIONES DQN:

   PROBLEMA: Correlaci√≥n temporal de experiencias
   SOLUCI√ìN: Experience Replay ‚Üí muestreo aleatorio rompe correlaciones

   PROBLEMA: Inestabilidad en targets m√≥viles
   SOLUCI√ìN: Target Network ‚Üí targets estables por per√≠odos fijos

   PROBLEMA: Distribuci√≥n de datos no-estacionaria
   SOLUCI√ìN: T√©cnicas de regularizaci√≥n y optimizaci√≥n cuidadosa

4. ARQUITECTURA DE RED:
   - Entrada: Vector de estado (ej: 4D para CartPole)
   - Capas ocultas: Transformaciones no lineales
   - Salida: Vector de Q-values (uno por acci√≥n)

INSIGHT CLAVE: DQN mantiene la elegancia matem√°tica de Q-Learning
pero escala a problemas complejos mediante aproximaci√≥n funcional.
""")
print("="*60)

print("\nüéì ¬°Preparado para el m√≥dulo final!")
print("En el siguiente bloque entrenaremos el agente DQN completo...")

# ============================================================================
# M√ìDULO 8: AGENTE DQN COMPLETO Y AN√ÅLISIS COMPARATIVO
# ============================================================================

"""
## M√ìDULO 8: IMPLEMENTACI√ìN COMPLETA DE DEEP Q-NETWORKS

**OBJETIVO PEDAG√ìGICO:**
Integrar todos los componentes en un agente DQN funcional, observar su
entrenamiento en un problema de estados continuos, y realizar un an√°lisis
comparativo comprehensivo con Q-Learning tabular.

**CONCEPTOS CLAVE:**
- Integraci√≥n de todos los componentes DQN
- Entrenamiento en espacios de estados continuos
- An√°lisis comparativo: tabular vs aproximaci√≥n funcional
- Escalabilidad y limitaciones de cada enfoque
"""

print("\nüöÄ M√ìDULO 8: Agente DQN Completo - Integraci√≥n Final")
print("=" * 55)

class AgenteDQN:
    """
    Implementaci√≥n completa de Deep Q-Network con todas las mejoras algor√≠tmicas.

    Esta clase integra todas las innovaciones de DQN en una implementaci√≥n
    pedag√≥gicamente clara que permite comprender cada componente y su funci√≥n
    en el algoritmo completo.
    """

    def __init__(self, dim_estado: int, dim_accion: int,
                 tasa_aprendizaje: float = 1e-3, factor_descuento: float = 0.99,
                 epsilon_inicial: float = 1.0, decaimiento_epsilon: float = 0.995,
                 epsilon_minimo: float = 0.01, tama√±o_memoria: int = 10000,
                 tama√±o_lote: int = 32, frecuencia_actualizacion_target: int = 100):
        """
        Inicializa el agente DQN con todos los hiperpar√°metros pedag√≥gicamente explicados.

        Args:
            dim_estado: Dimensionalidad del espacio de estados
            dim_accion: N√∫mero de acciones disponibles
            tasa_aprendizaje: Learning rate para el optimizador Adam
            factor_descuento: Œ≥ - Factor de descuento temporal
            epsilon_inicial: Œµ inicial para exploraci√≥n
            decaimiento_epsilon: Factor de decaimiento de Œµ por episodio
            epsilon_minimo: Œµ m√≠nimo para mantener exploraci√≥n residual
            tama√±o_memoria: Capacidad del buffer de experience replay
            tama√±o_lote: Tama√±o de lote para entrenamiento de la red
            frecuencia_actualizacion_target: Cada cu√°ntos pasos actualizar target network
        """
        self.dim_estado = dim_estado
        self.dim_accion = dim_accion
        self.epsilon = epsilon_inicial
        self.decaimiento_epsilon = decaimiento_epsilon
        self.epsilon_minimo = epsilon_minimo
        self.gamma = factor_descuento
        self.tama√±o_lote = tama√±o_lote
        self.frecuencia_target = frecuencia_actualizacion_target

        # Detectar dispositivo computacional √≥ptimo
        self.dispositivo = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Inicializar redes neuronales gemelas
        self.red_q = RedQProfunda(dim_estado, dim_accion).to(self.dispositivo)
        self.red_target = RedQProfunda(dim_estado, dim_accion).to(self.dispositivo)

        # Sincronizar red target inicialmente
        self.actualizar_red_target()

        # Configurar optimizador con regularizaci√≥n L2
        self.optimizador = optim.Adam(self.red_q.parameters(),
                                    lr=tasa_aprendizaje, weight_decay=1e-4)

        # Funci√≥n de p√©rdida
        self.criterio = nn.MSELoss()

        # Experience Replay
        self.memoria = BufferExperienceReplay(tama√±o_memoria)

        # M√©tricas de entrenamiento para an√°lisis pedag√≥gico
        self.paso_entrenamiento = 0
        self.historial_perdidas = []
        self.historial_q_values = []
        self.historial_epsilon = []

        print(f"ü§ñ Agente DQN inicializado:")
        print(f"   üñ•Ô∏è  Dispositivo: {self.dispositivo}")
        print(f"   üß† Arquitectura: Red principal + Red target")
        print(f"   üìä Par√°metros: LR={tasa_aprendizaje}, Œ≥={factor_descuento}")
        print(f"   üéØ Memoria: {tama√±o_memoria:,} experiencias")
        print(f"   üîÑ Actualizaci√≥n target: cada {frecuencia_actualizacion_target} pasos")

    def actuar(self, estado: np.ndarray, modo_entrenamiento: bool = True) -> int:
        """
        Selecciona acci√≥n usando pol√≠tica Œµ-greedy con red neuronal.

        Args:
            estado: Estado actual del entorno
            modo_entrenamiento: Si aplicar exploraci√≥n (True) o pol√≠tica greedy (False)

        Returns:
            Acci√≥n seleccionada
        """
        if modo_entrenamiento and np.random.random() < self.epsilon:
            # Exploraci√≥n: acci√≥n aleatoria
            return np.random.randint(0, self.dim_accion)

        # Explotaci√≥n: usar red neuronal para seleccionar mejor acci√≥n
        estado_tensor = torch.FloatTensor(estado).unsqueeze(0).to(self.dispositivo)

        with torch.no_grad():
            q_values = self.red_q(estado_tensor)
            # Registrar Q-values promedio para an√°lisis
            self.historial_q_values.append(q_values.mean().item())

        return q_values.argmax().item()

    def recordar(self, estado: np.ndarray, accion: int, recompensa: float,
                siguiente_estado: np.ndarray, terminado: bool):
        """Almacena experiencia en el buffer de replay."""
        self.memoria.almacenar(estado, accion, recompensa, siguiente_estado, terminado)

    def aprender(self) -> float:
        """
        Realiza una actualizaci√≥n de los par√°metros de la red Q usando experience replay.

        Returns:
            Valor de la p√©rdida para monitoreo de convergencia
        """
        if len(self.memoria) < self.tama√±o_lote:
            return 0.0

        # Muestrear lote de experiencias
        lote = self.memoria.muestrear(self.tama√±o_lote)

        # Convertir lote a tensores optimizados
        estados = torch.FloatTensor([t.estado for t in lote]).to(self.dispositivo)
        acciones = torch.LongTensor([t.accion for t in lote]).to(self.dispositivo)
        recompensas = torch.FloatTensor([t.recompensa for t in lote]).to(self.dispositivo)
        siguientes_estados = torch.FloatTensor([t.siguiente_estado for t in lote]).to(self.dispositivo)
        terminados = torch.BoolTensor([t.terminado for t in lote]).to(self.dispositivo)

        # Calcular Q-values actuales para las acciones tomadas
        q_values_actuales = self.red_q(estados).gather(1, acciones.unsqueeze(1)).squeeze(1)

        # Calcular Q-values objetivo usando red target (Double DQN)
        with torch.no_grad():
            q_values_siguiente = self.red_target(siguientes_estados).max(1)[0]
            q_targets = recompensas + (self.gamma * q_values_siguiente * ~terminados)

        # Calcular p√©rdida y realizar backpropagation
        perdida = self.criterio(q_values_actuales, q_targets)

        self.optimizador.zero_grad()
        perdida.backward()

        # Gradient clipping para estabilidad num√©rica
        torch.nn.utils.clip_grad_norm_(self.red_q.parameters(), max_norm=1.0)

        self.optimizador.step()

        # Actualizar red target peri√≥dicamente
        self.paso_entrenamiento += 1
        if self.paso_entrenamiento % self.frecuencia_target == 0:
            self.actualizar_red_target()

        # Decaimiento de exploraci√≥n
        if self.epsilon > self.epsilon_minimo:
            self.epsilon *= self.decaimiento_epsilon

        # Registrar m√©tricas
        valor_perdida = perdida.item()
        self.historial_perdidas.append(valor_perdida)
        self.historial_epsilon.append(self.epsilon)

        return valor_perdida

    def actualizar_red_target(self):
        """Copia par√°metros de la red principal a la red target."""
        self.red_target.load_state_dict(self.red_q.state_dict())

    def obtener_estadisticas(self) -> Dict:
        """
        Proporciona m√©tricas comprehensivas del entrenamiento DQN.

        Returns:
            Diccionario con estad√≠sticas para an√°lisis pedag√≥gico
        """
        return {
            'pasos_entrenamiento': self.paso_entrenamiento,
            'epsilon_actual': self.epsilon,
            'experiencias_memoria': len(self.memoria),
            'perdida_promedio': np.mean(self.historial_perdidas[-100:]) if self.historial_perdidas else 0,
            'q_value_promedio': np.mean(self.historial_q_values[-100:]) if self.historial_q_values else 0
        }

def entrenar_agente_dqn(env, agente_dqn, n_episodios: int = 500,
                       pasos_maximos: int = 500, mostrar_progreso: bool = True) -> Dict:
    """
    Funci√≥n educativa para entrenar agente DQN con an√°lisis pedag√≥gico completo.

    Args:
        env: Entorno de entrenamiento (CartPole)
        agente_dqn: Agente DQN a entrenar
        n_episodios: N√∫mero de episodios de entrenamiento
        pasos_maximos: M√°ximo de pasos por episodio
        mostrar_progreso: Mostrar reportes peri√≥dicos

    Returns:
        Diccionario con m√©tricas completas del entrenamiento
    """
    print(f"\nüéØ INICIANDO ENTRENAMIENTO DEEP Q-NETWORK")
    print(f"üéÆ Entorno: CartPole-v1 (estados continuos)")
    print(f"üìä Configuraci√≥n: {n_episodios} episodios, m√°x {pasos_maximos} pasos/episodio")
    print("-" * 65)

    # Contenedores para m√©tricas
    metricas_dqn = {
        'recompensas_episodio': [],
        'longitudes_episodio': [],
        'perdidas_entrenamiento': [],
        'valores_epsilon': [],
        'q_values_promedio': []
    }

    # Variables para an√°lisis de convergencia
    ventana_rendimiento = deque(maxlen=100)
    mejor_promedio = -np.inf
    tiempo_inicio = time.time()

    for episodio in range(n_episodios):
        # Reiniciar entorno
        if tipo_gym == 'gymnasium':
            estado, _ = env.reset()
        else:
            estado = env.reset()

        estado = np.array(estado, dtype=np.float32)
        recompensa_episodio = 0
        perdida_episodio = 0
        pasos = 0

        # Ejecutar episodio completo
        for paso in range(pasos_maximos):
            # Seleccionar y ejecutar acci√≥n
            accion = agente_dqn.actuar(estado, modo_entrenamiento=True)

            if tipo_gym == 'gymnasium':
                siguiente_estado, recompensa, terminado, truncado, _ = env.step(accion)
                terminado = terminado or truncado
            else:
                siguiente_estado, recompensa, terminado, _ = env.step(accion)

            siguiente_estado = np.array(siguiente_estado, dtype=np.float32)

            # Almacenar experiencia y aprender
            agente_dqn.recordar(estado, accion, recompensa, siguiente_estado, terminado)
            perdida = agente_dqn.aprender()

            # Actualizar m√©tricas
            recompensa_episodio += recompensa
            perdida_episodio += perdida
            pasos += 1
            estado = siguiente_estado

            if terminado:
                break

        # Registrar m√©tricas del episodio
        metricas_dqn['recompensas_episodio'].append(recompensa_episodio)
        metricas_dqn['longitudes_episodio'].append(pasos)
        metricas_dqn['valores_epsilon'].append(agente_dqn.epsilon)

        if perdida_episodio > 0:
            metricas_dqn['perdidas_entrenamiento'].append(perdida_episodio / pasos)

        # An√°lisis de rendimiento
        ventana_rendimiento.append(recompensa_episodio)
        promedio_reciente = np.mean(ventana_rendimiento)

        if promedio_reciente > mejor_promedio and len(ventana_rendimiento) == 100:
            mejor_promedio = promedio_reciente

        # Reportes peri√≥dicos con m√©tricas educativas
        if mostrar_progreso and (episodio + 1) % 100 == 0:
            stats = agente_dqn.obtener_estadisticas()
            tiempo_transcurrido = time.time() - tiempo_inicio

            print(f"üìà Ep. {episodio + 1:3d} | "
                  f"R: {recompensa_episodio:6.1f} | "
                  f"Prom(100): {promedio_reciente:6.1f} | "
                  f"Œµ: {agente_dqn.epsilon:.3f} | "
                  f"Memoria: {len(agente_dqn.memoria):5,} | "
                  f"Tiempo: {tiempo_transcurrido:.0f}s")

    tiempo_total = time.time() - tiempo_inicio

    print(f"\n‚úÖ ENTRENAMIENTO DQN COMPLETADO")
    print(f"   ‚è±Ô∏è  Tiempo total: {tiempo_total:.1f}s")
    print(f"   üîÑ Pasos de entrenamiento: {agente_dqn.paso_entrenamiento:,}")
    print(f"   üß† Experiencias acumuladas: {len(agente_dqn.memoria):,}")
    print(f"   üéØ Mejor promedio (100 ep.): {mejor_promedio:.1f}")
    print(f"   üîç Exploraci√≥n final: {agente_dqn.epsilon:.4f}")

    return metricas_dqn

# Crear y entrenar agente DQN
print(f"\nüéÆ Creando agente DQN para CartPole...")
agente_dqn = AgenteDQN(dim_estado=dim_estados, dim_accion=dim_acciones,
                      tasa_aprendizaje=0.001, factor_descuento=0.99)

# Entrenamiento
resultados_dqn = entrenar_agente_dqn(env_cartpole, agente_dqn, n_episodios=500)

print("\n‚úÖ M√≥dulo 8 completado - Agente DQN entrenado exitosamente")

# ============================================================================
# M√ìDULO 8B: AN√ÅLISIS COMPARATIVO COMPREHENSIVO
# ============================================================================

"""
## M√ìDULO 8B: AN√ÅLISIS COMPARATIVO - TABULAR vs APROXIMACI√ìN FUNCIONAL

**OBJETIVO PEDAG√ìGICO:**
Desarrollar comprensi√≥n profunda de las fortalezas y limitaciones de cada
enfoque, identificando cu√°ndo usar cada m√©todo seg√∫n las caracter√≠sticas
del problema.
"""

print("\nüìä M√ìDULO 8B: An√°lisis Comparativo Q-Learning vs DQN")
print("=" * 55)

def visualizar_comparacion_algoritmos(resultados_qlearning: Dict, resultados_dqn: Dict):
    """
    Genera visualizaciones comparativas entre Q-Learning tabular y DQN.

    Args:
        resultados_qlearning: M√©tricas del entrenamiento Q-Learning
        resultados_dqn: M√©tricas del entrenamiento DQN
    """

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))

    # 1. Comparaci√≥n de curvas de aprendizaje
    episodios_q = range(1, len(resultados_qlearning['recompensas_episodio']) + 1)
    episodios_dqn = range(1, len(resultados_dqn['recompensas_episodio']) + 1)

    axes[0,0].plot(episodios_q, resultados_qlearning['recompensas_episodio'],
                  alpha=0.3, color='blue', label='Q-Learning (GridWorld)')
    axes[0,0].plot(episodios_dqn, resultados_dqn['recompensas_episodio'],
                  alpha=0.3, color='red', label='DQN (CartPole)')

    # Medias m√≥viles para claridad
    ventana = 20
    if len(resultados_qlearning['recompensas_episodio']) >= ventana:
        media_q = np.convolve(resultados_qlearning['recompensas_episodio'],
                             np.ones(ventana)/ventana, mode='valid')
        axes[0,0].plot(episodios_q[ventana-1:], media_q, color='darkblue', linewidth=2)

    if len(resultados_dqn['recompensas_episodio']) >= ventana:
        media_dqn = np.convolve(resultados_dqn['recompensas_episodio'],
                               np.ones(ventana)/ventana, mode='valid')
        axes[0,0].plot(episodios_dqn[ventana-1:], media_dqn, color='darkred', linewidth=2)

    axes[0,0].set_title('Curvas de Aprendizaje Comparativas', fontweight='bold')
    axes[0,0].set_xlabel('Episodio')
    axes[0,0].set_ylabel('Recompensa')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)

    # 2. Eficiencia (longitud de episodios)
    axes[0,1].plot(episodios_q, resultados_qlearning['longitudes_episodio'],
                  color='blue', alpha=0.6, label='Q-Learning')
    axes[0,1].plot(episodios_dqn, resultados_dqn['longitudes_episodio'],
                  color='red', alpha=0.6, label='DQN')
    axes[0,1].set_title('Eficiencia de Pol√≠ticas', fontweight='bold')
    axes[0,1].set_xlabel('Episodio')
    axes[0,1].set_ylabel('Duraci√≥n del Episodio')
    axes[0,1].legend()
    axes[0,1].grid(True, alpha=0.3)

    # 3. Evoluci√≥n de exploraci√≥n
    axes[0,2].plot(episodios_q, resultados_qlearning['valores_epsilon'],
                  color='blue', linewidth=2, label='Q-Learning')
    axes[0,2].plot(episodios_dqn, resultados_dqn['valores_epsilon'],
                  color='red', linewidth=2, label='DQN')
    axes[0,2].set_title('Decaimiento de Exploraci√≥n', fontweight='bold')
    axes[0,2].set_xlabel('Episodio')
    axes[0,2].set_ylabel('Epsilon')
    axes[0,2].legend()
    axes[0,2].grid(True, alpha=0.3)

    # 4. Convergencia Q-Learning
    if resultados_qlearning['errores_td_promedio']:
        axes[1,0].plot(episodios_q, resultados_qlearning['errores_td_promedio'],
                      color='blue', alpha=0.7)
        axes[1,0].set_title('Convergencia Q-Learning (Errores TD)', fontweight='bold')
        axes[1,0].set_xlabel('Episodio')
        axes[1,0].set_ylabel('Error TD Promedio')
        axes[1,0].grid(True, alpha=0.3)

    # 5. Convergencia DQN
    if resultados_dqn['perdidas_entrenamiento']:
        axes[1,1].plot(resultados_dqn['perdidas_entrenamiento'], color='red', alpha=0.7)
        axes[1,1].set_title('Convergencia DQN (P√©rdida MSE)', fontweight='bold')
        axes[1,1].set_xlabel('Episodio')
        axes[1,1].set_ylabel('P√©rdida de Entrenamiento')
        axes[1,1].grid(True, alpha=0.3)

    # 6. Distribuciones de rendimiento - IMPLEMENTACI√ìN MEJORADA
    axes[1,2].remove()  # Eliminar el subplot original


    # Crear subplots personalizados para distribuciones separadas
    gs_dist = fig.add_gridspec(2, 2, left=0.68, right=0.98, top=0.42, bottom=0.05,
                          hspace=0.4, wspace=0.1)

    # Subplot superior: Q-Learning
    ax_q_dist = fig.add_subplot(gs_dist[0, :])
    ax_q_dist.hist(resultados_qlearning['recompensas_episodio'], bins=7,
                   alpha=0.8, color='blue', density=True, edgecolor='navy')
    ax_q_dist.set_title('Q-Learning (GridWorld)', fontweight='bold', fontsize=11)
    ax_q_dist.set_ylabel('Densidad')
    ax_q_dist.grid(True, alpha=0.3)

    # Estad√≠sticas para Q-Learning
    media_q = np.mean(resultados_qlearning['recompensas_episodio'])
    ax_q_dist.axvline(media_q, color='darkblue', linestyle='--', linewidth=2)
    ax_q_dist.text(0.7, 0.8, f'Media: {media_q:.2f}',
                   transform=ax_q_dist.transAxes, fontsize=10,
                   bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.8))

    # Subplot inferior: DQN
    ax_dqn_dist = fig.add_subplot(gs_dist[1, :])
    ax_dqn_dist.hist(resultados_dqn['recompensas_episodio'], bins=25,
                     alpha=0.8, color='red', density=True, edgecolor='darkred')
    ax_dqn_dist.set_title('DQN (CartPole)', fontweight='bold', fontsize=11)
    ax_dqn_dist.set_xlabel('Recompensa por Episodio')
    ax_dqn_dist.set_ylabel('Densidad')
    ax_dqn_dist.grid(True, alpha=0.3)

    # Estad√≠sticas para DQN
    media_dqn = np.mean(resultados_dqn['recompensas_episodio'])
    ax_dqn_dist.axvline(media_dqn, color='darkred', linestyle='--', linewidth=2)
    ax_dqn_dist.text(0.7, 0.8, f'Media: {media_dqn:.1f}',
                     transform=ax_dqn_dist.transAxes, fontsize=10,
                     bbox=dict(boxstyle="round,pad=0.3", facecolor="lightcoral", alpha=0.8))

    # Layout final optimizado
    plt.tight_layout(rect=[0, 0, 1, 0.94])
    plt.suptitle('An√°lisis Comparativo: Q-Learning Tabular vs Deep Q-Networks',
                fontsize=16, fontweight='bold', y=0.96)
    plt.show()

# Generar an√°lisis comparativo
visualizar_comparacion_algoritmos(resultados_entrenamiento, resultados_dqn)

print("\n‚úÖ M√≥dulo 8B completado - An√°lisis comparativo finalizado")

# ============================================================================
# EVALUACI√ìN FINAL DE AMBOS AGENTES
# ============================================================================

print(f"\nüéØ EVALUACI√ìN COMPARATIVA FINAL")

def evaluar_agente_qlearning(env, agente, n_evaluaciones: int = 100) -> Dict:
    """Eval√∫a rendimiento del agente Q-Learning entrenado."""

    recompensas = []
    longitudes = []
    tasa_exito = 0

    for evaluacion in range(n_evaluaciones):
        estado = env.reset()
        recompensa_total = 0
        pasos = 0

        while True:
            accion = agente.seleccionar_accion(estado, modo_entrenamiento=False)
            estado, recompensa, terminado, _ = env.step(accion)
            recompensa_total += recompensa
            pasos += 1

            if terminado or pasos > 50:
                if recompensa > 0:  # √âxito en GridWorld
                    tasa_exito += 1
                break

        recompensas.append(recompensa_total)
        longitudes.append(pasos)

    return {
        'recompensa_promedio': np.mean(recompensas),
        'std_recompensa': np.std(recompensas),
        'longitud_promedio': np.mean(longitudes),
        'std_longitud': np.std(longitudes),
        'tasa_exito': tasa_exito / n_evaluaciones
    }

def evaluar_agente_dqn(env, agente, n_evaluaciones: int = 100) -> Dict:
    """Eval√∫a rendimiento del agente DQN entrenado."""

    recompensas = []
    longitudes = []
    umbral_exito = 195  # Umbral est√°ndar para CartPole

    for evaluacion in range(n_evaluaciones):
        if tipo_gym == 'gymnasium':
            estado, _ = env.reset()
        else:
            estado = env.reset()

        estado = np.array(estado, dtype=np.float32)
        recompensa_total = 0
        pasos = 0

        while True:
            accion = agente.actuar(estado, modo_entrenamiento=False)

            if tipo_gym == 'gymnasium':
                estado, recompensa, terminado, truncado, _ = env.step(accion)
                terminado = terminado or truncado
            else:
                estado, recompensa, terminado, _ = env.step(accion)

            estado = np.array(estado, dtype=np.float32)
            recompensa_total += recompensa
            pasos += 1

            if terminado or pasos > 500:
                break

        recompensas.append(recompensa_total)
        longitudes.append(pasos)

    tasa_exito = np.mean([r >= umbral_exito for r in recompensas])

    return {
        'recompensa_promedio': np.mean(recompensas),
        'std_recompensa': np.std(recompensas),
        'longitud_promedio': np.mean(longitudes),
        'std_longitud': np.std(longitudes),
        'tasa_exito': tasa_exito
    }

# Evaluaciones finales
print(f"\nüìä Evaluando Q-Learning en GridWorld...")
eval_qlearning = evaluar_agente_qlearning(env_educativo, agente_qlearning)

print(f"\nüìä Evaluando DQN en CartPole...")
eval_dqn = evaluar_agente_dqn(env_cartpole, agente_dqn)

# Mostrar resultados comparativos
print(f"\n" + "="*70)
print(f"üìã RESULTADOS COMPARATIVOS FINALES")
print(f"="*70)

print(f"\nüéØ Q-LEARNING TABULAR (GridWorld 5√ó6):")
print(f"   ‚Ä¢ Recompensa promedio: {eval_qlearning['recompensa_promedio']:.3f} ¬± {eval_qlearning['std_recompensa']:.3f}")
print(f"   ‚Ä¢ Longitud promedio: {eval_qlearning['longitud_promedio']:.1f} ¬± {eval_qlearning['std_longitud']:.1f} pasos")
print(f"   ‚Ä¢ Tasa de √©xito: {eval_qlearning['tasa_exito']:.1%}")
print(f"   ‚Ä¢ Espacio de estados: Discreto, 30 estados")
print(f"   ‚Ä¢ Memoria requerida: ~30√ó4 = 120 valores Q")

print(f"\nüß† DEEP Q-NETWORK (CartPole):")
print(f"   ‚Ä¢ Recompensa promedio: {eval_dqn['recompensa_promedio']:.1f} ¬± {eval_dqn['std_recompensa']:.1f}")
print(f"   ‚Ä¢ Longitud promedio: {eval_dqn['longitud_promedio']:.1f} ¬± {eval_dqn['std_longitud']:.1f} pasos")
print(f"   ‚Ä¢ Tasa de √©xito (‚â•195): {eval_dqn['tasa_exito']:.1%}")
print(f"   ‚Ä¢ Espacio de estados: Continuo, 4 dimensiones")
print(f"   ‚Ä¢ Memoria requerida: {sum(p.numel() for p in agente_dqn.red_q.parameters()):,} par√°metros")

print("\n‚úÖ M√≥dulo 8B completado - An√°lisis comparativo finalizado")

# ============================================================================
# S√çNTESIS FINAL Y CONCLUSIONES PEDAG√ìGICAS
# ============================================================================

"""
## S√çNTESIS FINAL: REFLEXIONES SOBRE EL APRENDIZAJE POR REFUERZO

**OBJETIVO PEDAG√ìGICO:**
Consolidar todo el aprendizaje en una s√≠ntesis coherente que permita a los
estudiantes comprender el panorama completo del campo y las direcciones
futuras de investigaci√≥n.
"""

print("\n" + "="*70)
print("üéì S√çNTESIS FINAL: PANORAMA DEL APRENDIZAJE POR REFUERZO")
print("="*70)

print("""
üî¨ HALLAZGOS FUNDAMENTALES DE NUESTRO EXPERIMENTO:

1. ESCALABILIDAD Y APLICABILIDAD:

   Q-Learning Tabular ‚Üí √ìptimo para:
   ‚Ä¢ Espacios de estados discretos peque√±os (< 10‚Å¥ estados)
   ‚Ä¢ Problemas donde la interpretabilidad es crucial
   ‚Ä¢ Situaciones con garant√≠as te√≥ricas de convergencia
   ‚Ä¢ Recursos computacionales limitados

   Deep Q-Networks ‚Üí √ìptimo para:
   ‚Ä¢ Espacios de estados continuos o muy grandes
   ‚Ä¢ Problemas con representaciones de alta dimensionalidad
   ‚Ä¢ Necesidad de generalizaci√≥n a estados no visitados
   ‚Ä¢ Disponibilidad de recursos computacionales significativos

2. TRADE-OFFS OBSERVADOS:

   Interpretabilidad vs Escalabilidad:
   ‚Ä¢ Tabular: Cada Q(s,a) es interpretable pero limitado por |S|√ó|A|
   ‚Ä¢ DQN: Escalable pero funci√≥n Q impl√≠cita en par√°metros de red

   Garant√≠as Te√≥ricas vs Flexibilidad Pr√°ctica:
   ‚Ä¢ Tabular: Convergencia garantizada bajo condiciones apropiadas
   ‚Ä¢ DQN: Mayor flexibilidad pero convergencia no garantizada

   Eficiencia Muestral vs Capacidad Expresiva:
   ‚Ä¢ Tabular: Eficiente en problemas simples, cada experiencia actualiza solo una entrada
   ‚Ä¢ DQN: Menos eficiente muestralmente pero captura patrones complejos

3. INNOVACIONES ARQUITECT√ìNICAS CLAVE:

   Experience Replay:
   ‚Ä¢ Rompe correlaciones temporales ‚Üí estabilidad de entrenamiento
   ‚Ä¢ Reutilizaci√≥n de experiencias ‚Üí eficiencia muestral mejorada

   Target Networks:
   ‚Ä¢ Targets estables ‚Üí reduce varianza en actualizaci√≥n de par√°metros
   ‚Ä¢ Actualizaci√≥n peri√≥dica ‚Üí balance entre estabilidad y adaptabilidad

üåü INSIGHTS PEDAG√ìGICOS EMERGENTES:

‚Ä¢ El aprendizaje por refuerzo unifica conceptos de optimizaci√≥n, probabilidad,
  y aproximaci√≥n funcional en un marco conceptual elegante

‚Ä¢ La transici√≥n tabular‚Üífuncional ilustra perfectamente el trade-off
  interpretabilidad-escalabilidad central en machine learning

‚Ä¢ Experience replay demuestra c√≥mo las innovaciones algor√≠tmicas pueden
  resolver limitaciones fundamentales (correlaci√≥n temporal)

‚Ä¢ El balance exploraci√≥n-explotaci√≥n representa un meta-principio que
  trasciende el RL y se aplica a cualquier proceso de optimizaci√≥n secuencial

üîÆ DIRECCIONES FUTURAS PROMETEDORAS:

1. M√âTODOS DE GRADIENTE DE POL√çTICA:
   ‚Ä¢ REINFORCE, Actor-Critic, PPO, SAC
   ‚Ä¢ Optimizaci√≥n directa de pol√≠ticas vs aproximaci√≥n de funciones de valor

2. APRENDIZAJE BASADO EN MODELOS:
   ‚Ä¢ Model-Predictive Control + Deep Learning
   ‚Ä¢ Planificaci√≥n en espacios latentes aprendidos

3. APRENDIZAJE POR REFUERZO JER√ÅRQUICO:
   ‚Ä¢ Descomposici√≥n de problemas complejos en subproblemas
   ‚Ä¢ Transferencia de conocimiento entre tareas relacionadas

4. RL MULTI-AGENTE:
   ‚Ä¢ Coordinaci√≥n y competencia entre agentes inteligentes
   ‚Ä¢ Emergencia de comportamientos sociales complejos

üéØ COMPETENCIAS DESARROLLADAS:

‚úì Comprensi√≥n profunda de MDPs como formalizaci√≥n matem√°tica
‚úì Implementaci√≥n pr√°ctica de algoritmos fundamentales de RL
‚úì An√°lisis cr√≠tico de trade-offs algor√≠tmicos y arquitect√≥nicos
‚úì Capacidad de seleccionar m√©todos apropiados seg√∫n caracter√≠sticas del problema
‚úì Habilidades de experimentaci√≥n y an√°lisis de resultados en ML
‚úì Visi√≥n panor√°mica del campo con apertura hacia investigaci√≥n avanzada

üìö RECURSOS PARA PROFUNDIZACI√ìN CONTINUA:

‚Ä¢ Libros fundamentales:
  - Sutton & Barto: "Reinforcement Learning: An Introduction" (2018)
  - Bertsekas: "Dynamic Programming and Optimal Control" (2017)

‚Ä¢ Art√≠culos seminales:
  - Mnih et al.: "Human-level control through deep RL" (Nature, 2015)
  - Schulman et al.: "Proximal Policy Optimization Algorithms" (2017)
  - Haarnoja et al.: "Soft Actor-Critic" (2018)

‚Ä¢ Recursos pr√°cticos:
  - OpenAI Spinning Up: Implementaciones educativas de algoritmos modernos
  - Stable Baselines3: Biblioteca de algoritmos de RL bien documentada
  - Papers With Code: Tracking del estado del arte en RL

üèÜ MENSAJE FINAL:

El aprendizaje por refuerzo representa una de las fronteras m√°s emocionantes
de la inteligencia artificial, donde la elegancia matem√°tica se encuentra
con aplicaciones transformadoras del mundo real. Desde robots que aprenden
a caminar hasta sistemas que dominan juegos complejos, RL ofrece un paradigma
poderoso para crear agentes verdaderamente aut√≥nomos.

Su viaje en este campo apenas comienza. Los fundamentos s√≥lidos que han
construido hoy les proporcionan las herramientas conceptuales y pr√°cticas
necesarias para contribuir a esta disciplina en r√°pida evoluci√≥n.

¬°El futuro de la IA inteligente est√° en sus manos!
""")

print("\nüéâ ¬°FELICITACIONES!")
print("‚úÖ Han completado exitosamente el recorrido comprehensivo por el aprendizaje por refuerzo")
print("üî¨ Desde los fundamentos matem√°ticos hasta las implementaciones m√°s avanzadas")
print("üåü Est√°n preparados para abordar problemas reales de RL y contribuir al campo")

# Cerrar entornos apropiadamente
env_cartpole.close()

print("\nüéì FIN DEL CUADERNO EDUCATIVO")
print("üìß Para consultas adicionales o profundizaci√≥n, contacten al instructor")