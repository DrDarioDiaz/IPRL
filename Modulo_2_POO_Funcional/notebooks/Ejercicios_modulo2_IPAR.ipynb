{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  M贸dulo 2: Programaci贸n Funcional y Orientada a Objetos en Python\n",
        "## Gu铆a de Ejercicios para Aprendizaje por Refuerzo\n",
        "\n",
        "![Python & RL](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)\n",
        "![OpenAI Gym](https://img.shields.io/badge/OpenAI_Gym-0081A5?style=for-the-badge&logo=OpenAI&logoColor=white)\n",
        "\n",
        "###  Sociedad Argentina de Estad铆stica\n",
        "**Curso:** Introducci贸n a la Programaci贸n en Python para Aprendizaje por Refuerzo  \n",
        "**Docente:** Dr. Dar铆o Ezequiel D铆az  \n",
        "**Marzo-Abril 2025**\n",
        "\n",
        "---\n",
        "\n",
        "> Esta gu铆a de ejercicios est谩 dise帽ada para desarrollar habilidades en programaci贸n funcional y orientada a objetos en Python, con 茅nfasis en su aplicaci贸n al campo del Aprendizaje por Refuerzo. Los ejercicios avanzan progresivamente en complejidad, desde conceptos b谩sicos hasta implementaciones avanzadas.\n",
        "\n",
        "###  Instrucciones:\n",
        "- Complete cada ejercicio en las celdas correspondientes\n",
        "- Ejecute el c贸digo para verificar su funcionamiento\n",
        "- Los ejercicios est谩n dise帽ados para reforzar conceptos te贸ricos vistos en clase\n",
        "- Consulte la documentaci贸n oficial de Python cuando sea necesario\n",
        "\n",
        "###  Contenidos:\n",
        "- Funciones y par谩metros\n",
        "- Funciones Lambda\n",
        "- Programaci贸n orientada a objetos\n",
        "- Listas por comprensi贸n\n",
        "- Decoradores\n",
        "- Manejo de excepciones\n",
        "- Patrones de dise帽o\n",
        "\n",
        "---\n",
        "\n",
        "*\"En el aprendizaje por refuerzo, como en la programaci贸n, la exploraci贸n y la explotaci贸n deben estar en perfecto equilibrio.\"*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SKPOBBYaGFqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 1: Funciones con Par谩metros por Defecto y Nombrados\n",
        "**Dificultad: B谩sica**\n",
        "\n",
        "**Objetivo**: Practicar la creaci贸n de funciones con par谩metros opcionales y valores por defecto.\n",
        "\n",
        "**Descripci贸n**: Crea una funci贸n llamada `calcular_precio_final` que calcule el precio de un producto despu茅s de aplicar impuestos y descuentos.\n",
        "\n",
        "La funci贸n debe aceptar:\n",
        "- `precio_base` (obligatorio)\n",
        "- `impuesto` (por defecto 0.21 - equivale a 21%)\n",
        "- `descuento` (por defecto 0 - sin descuento)\n",
        "\n",
        "Primero se debe aplicar el descuento al precio base, y luego calcular el impuesto sobre ese valor con descuento."
      ],
      "metadata": {
        "id": "-UHO1aCg66k5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zQ_m9es6q6F"
      },
      "outputs": [],
      "source": [
        "# Completa la funci贸n calcular_precio_final\n",
        "def calcular_precio_final(precio_base, impuesto=0.21, descuento=0):\n",
        "    # Aplica primero el descuento y luego el impuesto\n",
        "    # Tu c贸digo aqu铆\n",
        "\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Pruebas:\n",
        "print(calcular_precio_final(100))  # Deber铆a ser 121.0\n",
        "print(calcular_precio_final(100, descuento=0.1))  # Deber铆a ser 108.9\n",
        "print(calcular_precio_final(100, 0.05, 0.2))  # Deber铆a ser 84.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2: Funciones Lambda para Ordenamiento\n",
        "**Dificultad: B谩sica**\n",
        "\n",
        "**Objetivo**: Practicar el uso de funciones lambda con la funci贸n `sorted()` para ordenar colecciones complejas.\n",
        "\n",
        "**Descripci贸n**: Utiliza funciones lambda con la funci贸n `sorted()` para ordenar una lista de tuplas que representan episodios de entrenamiento de un agente.\n",
        "\n",
        "Cada tupla contiene:\n",
        "- (n煤mero_episodio, pasos, recompensa_total)\n",
        "\n",
        "Debes ordenar la lista de 3 formas diferentes:\n",
        "1. Por recompensa (de mayor a menor)\n",
        "2. Por n煤mero de pasos (de menor a mayor)\n",
        "3. Por eficiencia (recompensa/pasos, de mayor a menor)"
      ],
      "metadata": {
        "id": "Izt9JenHEyyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de episodios: (n煤mero_episodio, pasos, recompensa_total)\n",
        "episodios = [\n",
        "    (1, 145, 24),\n",
        "    (2, 97, 31),\n",
        "    (3, 156, 18),\n",
        "    (4, 82, 42),\n",
        "    (5, 113, 37)\n",
        "]\n",
        "\n",
        "# Ordena los episodios por recompensa (mayor a menor)\n",
        "episodios_por_recompensa = None  # Tu c贸digo aqu铆\n",
        "\n",
        "# Ordena los episodios por n煤mero de pasos (menor a mayor)\n",
        "episodios_por_pasos = None  # Tu c贸digo aqu铆\n",
        "\n",
        "# Ordena los episodios por eficiencia (recompensa/pasos, mayor a menor)\n",
        "episodios_por_eficiencia = None  # Tu c贸digo aqu铆\n",
        "\n",
        "# Imprime los resultados\n",
        "print(\"Por recompensa:\", episodios_por_recompensa)\n",
        "print(\"Por pasos:\", episodios_por_pasos)\n",
        "print(\"Por eficiencia:\", episodios_por_eficiencia)"
      ],
      "metadata": {
        "id": "PvoHjToXEzG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 3: Clase B谩sica para Agente de Aprendizaje por Refuerzo\n",
        "**Dificultad: B谩sica**\n",
        "\n",
        "**Objetivo**: Practicar la creaci贸n de clases, atributos y m茅todos en POO.\n",
        "\n",
        "**Descripci贸n**: Crea una clase `AgenteRL` que represente un agente b谩sico de aprendizaje por refuerzo con:\n",
        "- Atributos para almacenar par谩metros y valores Q\n",
        "- Un m茅todo para seleccionar acciones con pol铆tica epsilon-greedy\n",
        "- Un m茅todo para aprender de experiencias\n",
        "- Un m茅todo `__str__` para visualizar informaci贸n del agente\n",
        "\n",
        "La pol铆tica epsilon-greedy consiste en:\n",
        "- Con probabilidad epsilon: seleccionar una acci贸n aleatoria (exploraci贸n)\n",
        "- Con probabilidad 1-epsilon: seleccionar la mejor acci贸n conocida (explotaci贸n)"
      ],
      "metadata": {
        "id": "mI6kvMDnE39U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Completa la clase AgenteRL\n",
        "class AgenteRL:\n",
        "    def __init__(self, num_acciones, epsilon=0.1):\n",
        "        # Inicializa atributos: tabla de valores Q, epsilon, etc.\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def seleccionar_accion(self, estado):\n",
        "        # Implementa la pol铆tica epsilon-greedy:\n",
        "        # - Con probabilidad epsilon: acci贸n aleatoria (exploraci贸n)\n",
        "        # - Con probabilidad 1-epsilon: mejor acci贸n conocida (explotaci贸n)\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado):\n",
        "        # Implementa actualizaci贸n simple de valor Q\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def __str__(self):\n",
        "        # Representaci贸n en string del agente\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Prueba la clase\n",
        "agente = AgenteRL(num_acciones=4)\n",
        "print(agente)\n",
        "\n",
        "# Prueba selecci贸n de acciones\n",
        "estado1 = \"s1\"\n",
        "accion1 = agente.seleccionar_accion(estado1)\n",
        "print(f\"Acci贸n seleccionada para {estado1}: {accion1}\")\n",
        "\n",
        "# Prueba aprendizaje\n",
        "agente.aprender(estado1, accion1, 5, \"s2\")\n",
        "print(f\"Despu茅s de aprender una vez: {agente}\")"
      ],
      "metadata": {
        "id": "B1UrfIGgE51c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 4: Listas por Comprensi贸n para Transformaci贸n de Datos\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Practicar el uso de listas por comprensi贸n para transformar y filtrar datos.\n",
        "\n",
        "**Descripci贸n**: Utiliza listas por comprensi贸n para procesar un conjunto de observaciones de un entorno de aprendizaje por refuerzo.\n",
        "\n",
        "Cada observaci贸n es una tupla con formato:\n",
        "- (estado, acci贸n, recompensa, siguiente_estado, terminado)\n",
        "\n",
        "Debes crear:\n",
        "1. Una lista de todas las recompensas\n",
        "2. Una lista de transiciones (estado, acci贸n) que obtuvieron recompensa positiva\n",
        "3. Un diccionario que mapee cada estado a la recompensa promedio obtenida desde 茅l\n",
        "4. Una lista de estados terminales (donde terminado=True)"
      ],
      "metadata": {
        "id": "5ro-XroqE7kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos de observaciones de un entorno - cada tupla contiene:\n",
        "# (estado, acci贸n, recompensa, siguiente_estado, terminado)\n",
        "observaciones = [\n",
        "    (0, 1, 5, 1, False),\n",
        "    (1, 0, -1, 2, False),\n",
        "    (2, 1, 0, 3, False),\n",
        "    (3, 2, 10, 4, True),\n",
        "    (0, 2, 3, 2, False),\n",
        "    (2, 0, 7, 3, False),\n",
        "    (3, 1, -2, 0, False),\n",
        "    (0, 0, 4, 4, True)\n",
        "]\n",
        "\n",
        "# 1. Crea una lista de todas las recompensas\n",
        "recompensas = None  # Tu c贸digo aqu铆\n",
        "\n",
        "# 2. Crea una lista de transiciones (estado, acci贸n) que obtuvieron recompensa positiva\n",
        "transiciones_positivas = None  # Tu c贸digo aqu铆\n",
        "\n",
        "# 3. Crea un diccionario que mapee cada estado a la recompensa promedio obtenida desde 茅l\n",
        "# Pista: Necesitar谩s contar recompensas y acciones por estado\n",
        "recompensa_por_estado = None  # Tu c贸digo aqu铆\n",
        "\n",
        "# 4. Crea una lista de estados terminales\n",
        "estados_terminales = None  # Tu c贸digo aqu铆\n",
        "\n",
        "# Imprime los resultados\n",
        "print(\"Recompensas:\", recompensas)\n",
        "print(\"Transiciones con recompensa positiva:\", transiciones_positivas)\n",
        "print(\"Recompensa promedio por estado:\", recompensa_por_estado)\n",
        "print(\"Estados terminales:\", estados_terminales)"
      ],
      "metadata": {
        "id": "EljpcKHPE9XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 5: Argumentos Variables con *args y **kwargs\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Practicar el uso de argumentos variables en funciones.\n",
        "\n",
        "**Descripci贸n**: Crea una funci贸n llamada `crear_informe_agente` que genere un informe para agentes de aprendizaje por refuerzo.\n",
        "\n",
        "La funci贸n debe aceptar:\n",
        "- `nombre_agente` (obligatorio)\n",
        "- Cualquier n煤mero de episodios con sus recompensas (`*args`) - cada episodio es una tupla (num_episodio, recompensa)\n",
        "- Cualquier n煤mero de par谩metros de configuraci贸n (`**kwargs`) - pueden ser alpha, gamma, epsilon, etc.\n",
        "\n",
        "La funci贸n debe calcular la recompensa total, promedio, m谩xima, m铆nima, y retornar un diccionario con toda esta informaci贸n junto con los par谩metros recibidos."
      ],
      "metadata": {
        "id": "G6H_yrCPE_D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Completa la funci贸n crear_informe_agente\n",
        "def crear_informe_agente(nombre_agente, *args, **kwargs):\n",
        "    # args contiene tuplas (episodio, recompensa)\n",
        "    # kwargs contiene par谩metros de configuraci贸n\n",
        "\n",
        "    # Calcula la recompensa total y promedio\n",
        "    # Tu c贸digo aqu铆\n",
        "\n",
        "    # Genera y retorna un diccionario con el informe\n",
        "    # Tu c贸digo aqu铆\n",
        "\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Pruebas:\n",
        "informe = crear_informe_agente(\"AgentQ\",\n",
        "                             (1, 10), (2, 15), (3, 5),\n",
        "                             alpha=0.1, gamma=0.99, epsilon=0.2)\n",
        "print(informe)\n",
        "# Deber铆a mostrar un diccionario con nombre_agente, episodios, recompensa_total,\n",
        "# recompensa_promedio, recompensa_m谩xima, recompensa_m铆nima y un diccionario con los par谩metros"
      ],
      "metadata": {
        "id": "9YOS2GB6FAxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 6: Decorador para Medir Tiempo de Ejecuci贸n\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Entender y crear decoradores para modificar el comportamiento de funciones.\n",
        "\n",
        "**Descripci贸n**: Crea un decorador `medir_tiempo` que mida y muestre el tiempo de ejecuci贸n de una funci贸n. El decorador debe:\n",
        "\n",
        "1. Registrar el tiempo antes de llamar a la funci贸n\n",
        "2. Ejecutar la funci贸n y almacenar su resultado\n",
        "3. Calcular el tiempo transcurrido\n",
        "4. Imprimir informaci贸n sobre el tiempo de ejecuci贸n\n",
        "5. Retornar el resultado original de la funci贸n\n",
        "\n",
        "Prueba el decorador con la funci贸n recursiva fibonacci, que suele tener tiempos de ejecuci贸n crecientes."
      ],
      "metadata": {
        "id": "0C3OHZlAFExU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Completa el decorador medir_tiempo\n",
        "def medir_tiempo(funcion):\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Aplica el decorador a la funci贸n fibonacci\n",
        "@medir_tiempo\n",
        "def fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci(n-1) + fibonacci(n-2)\n",
        "\n",
        "# Prueba la funci贸n decorada\n",
        "resultado = fibonacci(30)\n",
        "print(f\"Resultado: {resultado}\")"
      ],
      "metadata": {
        "id": "z0ri8pktFCoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 7: Procesamiento de Datos con Map/Filter/Reduce\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Practicar programaci贸n funcional aplicando funciones de orden superior.\n",
        "\n",
        "**Descripci贸n**: Procesa un conjunto de datos de recompensas obtenidas por un agente utilizando las funciones `map()`, `filter()` y `reduce()`.\n",
        "\n",
        "En este ejercicio debes:\n",
        "1. Usar `map()` para calcular la recompensa total de cada episodio\n",
        "2. Usar `filter()` para encontrar episodios con recompensa total positiva\n",
        "3. Usar `reduce()` para encontrar el episodio con la mayor recompensa\n",
        "\n",
        "Cada episodio contiene una lista de valores de recompensa recibidos."
      ],
      "metadata": {
        "id": "G7M_ft54FHJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "# Datos de recompensas por episodio\n",
        "recompensas = [\n",
        "    {\"episodio\": 1, \"valores\": [5, -1, 0, 10, -2]},\n",
        "    {\"episodio\": 2, \"valores\": [7, 8, -3, 4, 2]},\n",
        "    {\"episodio\": 3, \"valores\": [-4, 2, 0, 9, 1]},\n",
        "    {\"episodio\": 4, \"valores\": [6, -2, 3, -1, 8]},\n",
        "    {\"episodio\": 5, \"valores\": [1, 5, 3, 7, -6]}\n",
        "]\n",
        "\n",
        "# 1. Usa map para calcular la recompensa total de cada episodio\n",
        "# Tu c贸digo aqu铆\n",
        "\n",
        "# 2. Usa filter para encontrar episodios con recompensa total positiva\n",
        "# Tu c贸digo aqu铆\n",
        "\n",
        "# 3. Usa reduce para encontrar el episodio con la mayor recompensa\n",
        "# Tu c贸digo aqu铆\n",
        "\n",
        "# Imprime los resultados\n",
        "print(\"Recompensas totales:\", recompensas_totales)\n",
        "print(\"Episodios positivos:\", episodios_positivos)\n",
        "print(\"Mejor episodio:\", mejor_episodio)"
      ],
      "metadata": {
        "id": "oLYd6b47FI8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 8: Encapsulaci贸n con Propiedades y Validaci贸n\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Implementar encapsulaci贸n y validaci贸n de datos utilizando propiedades en Python.\n",
        "\n",
        "**Descripci贸n**: Mejora una clase `AgenteRL` a帽adiendo propiedades con validaci贸n para par谩metros cr铆ticos.\n",
        "\n",
        "Las propiedades deben incluir:\n",
        "- `epsilon`: Valor entre 0 y 1 (probabilidad de exploraci贸n)\n",
        "- `alpha`: Valor entre 0 y 1 (tasa de aprendizaje)\n",
        "- `gamma`: Valor entre 0 y 1 (factor de descuento)\n",
        "\n",
        "Cada propiedad debe incluir validaci贸n para asegurar que los valores est茅n en el rango correcto,\n",
        "lanzando `ValueError` con un mensaje descriptivo cuando se intente asignar un valor inv谩lido."
      ],
      "metadata": {
        "id": "9IlcAU1lFKhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Completa la clase AgenteRL mejorada con propiedades\n",
        "class AgenteRL:\n",
        "    def __init__(self, num_acciones, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
        "        self._num_acciones = num_acciones\n",
        "        self._q_values = {}  # Diccionario para almacenar valores Q\n",
        "\n",
        "        # Usa propiedades para estos par谩metros\n",
        "        self._epsilon = None\n",
        "        self._alpha = None\n",
        "        self._gamma = None\n",
        "\n",
        "        # Asigna valores a trav茅s de propiedades para validaci贸n\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    # Implementa la propiedad epsilon con validaci贸n\n",
        "    @property\n",
        "    def epsilon(self):\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass\n",
        "\n",
        "    @epsilon.setter\n",
        "    def epsilon(self, valor):\n",
        "        # Valida que epsilon est茅 entre 0 y 1\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass\n",
        "\n",
        "    # Implementa propiedades alpha y gamma con validaci贸n similar\n",
        "    # Tu c贸digo aqu铆\n",
        "\n",
        "    def seleccionar_accion(self, estado):\n",
        "        \"\"\"Selecciona una acci贸n usando la pol铆tica epsilon-greedy.\"\"\"\n",
        "        estado_str = str(estado)\n",
        "\n",
        "        # Inicializa valores Q si es necesario\n",
        "        if estado_str not in self._q_values:\n",
        "            self._q_values[estado_str] = [0.0] * self._num_acciones\n",
        "\n",
        "        # Pol铆tica epsilon-greedy\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self._num_acciones - 1)  # Exploraci贸n\n",
        "        else:\n",
        "            return self._q_values[estado_str].index(max(self._q_values[estado_str]))  # Explotaci贸n\n",
        "\n",
        "# Prueba la clase con propiedades\n",
        "import random\n",
        "\n",
        "agente = AgenteRL(num_acciones=4, epsilon=0.2)\n",
        "print(f\"Epsilon: {agente.epsilon}\")\n",
        "\n",
        "# Intenta asignar valores v谩lidos e inv谩lidos\n",
        "try:\n",
        "    agente.epsilon = 0.5\n",
        "    print(f\"Nuevo epsilon: {agente.epsilon}\")\n",
        "\n",
        "    agente.epsilon = 1.5  # Deber铆a lanzar ValueError\n",
        "except ValueError as e:\n",
        "    print(f\"Error capturado: {e}\")"
      ],
      "metadata": {
        "id": "UCMnoTaNFMHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 9: Herencia para Diferentes Tipos de Agentes\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Practicar la herencia y la sobreescritura de m茅todos en POO.\n",
        "\n",
        "**Descripci贸n**: Crea una jerarqu铆a de clases para diferentes tipos de agentes de aprendizaje por refuerzo,\n",
        "heredando de una clase base com煤n llamada `AgenteBase`.\n",
        "\n",
        "Implementa:\n",
        "1. `AgenteBase`: Clase abstracta con m茅todos `seleccionar_accion` y `aprender`\n",
        "2. `AgenteAleatorio`: Un agente que selecciona acciones aleatorias y no aprende\n",
        "3. `AgenteQLearning`: Un agente que implementa el algoritmo Q-Learning\n",
        "\n",
        "Cada agente debe tener su propia implementaci贸n espec铆fica de los m茅todos heredados."
      ],
      "metadata": {
        "id": "Ah8VzQ1bFNpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Completa la jerarqu铆a de clases de agentes\n",
        "\n",
        "# Clase base\n",
        "class AgenteBase:\n",
        "    def __init__(self, num_acciones, nombre=\"AgenteGen茅rico\"):\n",
        "        self.num_acciones = num_acciones\n",
        "        self.nombre = nombre\n",
        "        self.experiencia_total = 0\n",
        "\n",
        "    def seleccionar_accion(self, estado):\n",
        "        # M茅todo abstracto que las subclases deben implementar\n",
        "        raise NotImplementedError(\"Las subclases deben implementar seleccionar_accion\")\n",
        "\n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado):\n",
        "        # M茅todo abstracto que las subclases deben implementar\n",
        "        raise NotImplementedError(\"Las subclases deben implementar aprender\")\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.nombre} (acciones: {self.num_acciones}, exp: {self.experiencia_total})\"\n",
        "\n",
        "# Completa la clase AgenteAleatorio\n",
        "class AgenteAleatorio(AgenteBase):\n",
        "    # Un agente que selecciona acciones aleatorias\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Completa la clase AgenteQLearning\n",
        "class AgenteQLearning(AgenteBase):\n",
        "    # Un agente que implementa Q-Learning\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Prueba las clases\n",
        "agente_aleatorio = AgenteAleatorio(4, \"Explorador\")\n",
        "agente_q = AgenteQLearning(4, \"Aprendiz\", alpha=0.2, gamma=0.9, epsilon=0.1)\n",
        "\n",
        "print(agente_aleatorio)\n",
        "print(agente_q)\n",
        "\n",
        "# Prueba m茅todos\n",
        "estado = \"estado_prueba\"\n",
        "accion_aleatoria = agente_aleatorio.seleccionar_accion(estado)\n",
        "accion_q = agente_q.seleccionar_accion(estado)\n",
        "\n",
        "print(f\"Acci贸n aleatoria: {accion_aleatoria}\")\n",
        "print(f\"Acci贸n Q-Learning: {accion_q}\")\n",
        "\n",
        "# Prueba aprendizaje\n",
        "agente_aleatorio.aprender(estado, accion_aleatoria, 1, \"siguiente_estado\")\n",
        "agente_q.aprender(estado, accion_q, 1, \"siguiente_estado\")"
      ],
      "metadata": {
        "id": "SDKDxLdZFPoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 10: M茅todos de Instancia, de Clase y Est谩ticos\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Entender y practicar los diferentes tipos de m茅todos en clases Python.\n",
        "\n",
        "**Descripci贸n**: Crea una clase `Experimento` que utilice los tres tipos de m茅todos para\n",
        "gestionar experimentos de aprendizaje por refuerzo:\n",
        "\n",
        "1. **M茅todo de instancia**: `ejecutar()` - Ejecuta el experimento y almacena resultados\n",
        "2. **M茅todo de clase**: `crear_desde_configuracion()` - Crea un experimento a partir de un diccionario de configuraci贸n\n",
        "3. **M茅todo est谩tico**: `calcular_metricas()` - Calcula m茅tricas a partir de un hist贸rico de recompensas\n",
        "\n",
        "El m茅todo de clase debe usar el decorador `@classmethod` y el m茅todo est谩tico debe usar `@staticmethod`."
      ],
      "metadata": {
        "id": "OadYtdLIFRHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Completa la clase Experimento con los tres tipos de m茅todos\n",
        "class Experimento:\n",
        "    # Atributo de clase\n",
        "    contador_experimentos = 0\n",
        "\n",
        "    def __init__(self, nombre, agente, entorno):\n",
        "        # Inicializa atributos de instancia\n",
        "        self.nombre = nombre\n",
        "        self.agente = agente\n",
        "        self.entorno = entorno\n",
        "        self.fecha_inicio = None\n",
        "        self.fecha_fin = None\n",
        "        self.resultados = None\n",
        "\n",
        "        # Incrementa contador de clase\n",
        "        Experimento.contador_experimentos += 1\n",
        "        self.id = Experimento.contador_experimentos\n",
        "\n",
        "    # M茅todo de instancia\n",
        "    def ejecutar(self, episodios=100, max_pasos=1000):\n",
        "        \"\"\"Ejecuta el experimento y almacena resultados.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    # M茅todo de clase\n",
        "    @classmethod\n",
        "    def crear_desde_configuracion(cls, config_dict):\n",
        "        \"\"\"\n",
        "        M茅todo de clase que crea un experimento a partir de un diccionario de configuraci贸n.\n",
        "        \"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    # M茅todo est谩tico\n",
        "    @staticmethod\n",
        "    def calcular_metricas(historico_recompensas):\n",
        "        \"\"\"\n",
        "        M茅todo est谩tico que calcula m茅tricas a partir de un hist贸rico de recompensas.\n",
        "        No depende del estado del objeto.\n",
        "        \"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Prueba los diferentes tipos de m茅todos\n",
        "# Simula agente y entorno para pruebas\n",
        "class AgenteSimulado:\n",
        "    def seleccionar_accion(self, estado):\n",
        "        return 0\n",
        "\n",
        "class EntornoSimulado:\n",
        "    def reset(self):\n",
        "        return 0\n",
        "\n",
        "    def paso(self, accion):\n",
        "        return 0, random.randint(1, 10), random.random() < 0.1, {}\n",
        "\n",
        "# Prueba m茅todo de instancia\n",
        "agente = AgenteSimulado()\n",
        "entorno = EntornoSimulado()\n",
        "experimento = Experimento(\"Prueba1\", agente, entorno)\n",
        "experimento.ejecutar(episodios=5)\n",
        "print(f\"Resultados: {experimento.resultados}\")\n",
        "\n",
        "# Prueba m茅todo de clase\n",
        "config = {\n",
        "    \"nombre\": \"ExperimentoConfig\",\n",
        "    \"agente\": agente,\n",
        "    \"entorno\": entorno\n",
        "}\n",
        "experimento2 = Experimento.crear_desde_configuracion(config)\n",
        "print(f\"Experimento desde config: {experimento2.nombre} (ID: {experimento2.id})\")\n",
        "\n",
        "# Prueba m茅todo est谩tico\n",
        "historico = [[5, 8, 10], [6, 7, 9, 12], [3, 5, 8]]\n",
        "metricas = Experimento.calcular_metricas(historico)\n",
        "print(f\"M茅tricas calculadas: {metricas}\")"
      ],
      "metadata": {
        "id": "RSkWXkflFS4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 11: Polimorfismo para Entornos de Entrenamiento\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Implementar polimorfismo con clases de entornos que comparten una interfaz com煤n.\n",
        "\n",
        "**Descripci贸n**: Crea diferentes clases de entornos que comparten una interfaz com煤n, y una funci贸n\n",
        "de entrenamiento que funcione con cualquiera de ellos.\n",
        "\n",
        "Implementa:\n",
        "1. `Entorno`: Clase base abstracta con m茅todos `reset()`, `paso()` y `obtener_num_acciones()`\n",
        "2. `EntornoSimple`: Entorno con estados lineales (0-9) y dos acciones (izquierda/derecha)\n",
        "3. `EntornoRejilla`: Entorno tipo rejilla 2D con 4 acciones (arriba/derecha/abajo/izquierda)\n",
        "4. Funci贸n `entrenar()`: Entrena un agente en cualquier entorno que implemente la interfaz\n",
        "\n",
        "El polimorfismo permite que la funci贸n de entrenamiento funcione con diferentes tipos de entornos."
      ],
      "metadata": {
        "id": "1r8-aX8jFUZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Completa la jerarqu铆a de clases para entornos\n",
        "\n",
        "# Clase base abstracta\n",
        "class Entorno(ABC):\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        \"\"\"Reinicia el entorno y retorna el estado inicial.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def paso(self, accion):\n",
        "        \"\"\"\n",
        "        Ejecuta una acci贸n y retorna (siguiente_estado, recompensa, terminado, info).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def obtener_num_acciones(self):\n",
        "        \"\"\"Retorna el n煤mero de acciones posibles.\"\"\"\n",
        "        pass\n",
        "\n",
        "# Completa la clase EntornoSimple\n",
        "class EntornoSimple(Entorno):\n",
        "    # Un entorno con estados del 0 al 9 y 2 acciones (izquierda/derecha)\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Completa la clase EntornoRejilla\n",
        "class EntornoRejilla(Entorno):\n",
        "    # Un entorno tipo rejilla 2D con 4 acciones (arriba/derecha/abajo/izquierda)\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Funci贸n polim贸rfica para entrenar un agente en cualquier entorno\n",
        "def entrenar(agente, entorno, episodios=100, max_pasos=1000):\n",
        "    \"\"\"\n",
        "    Entrena un agente en un entorno.\n",
        "    Funciona con cualquier agente y entorno que implementen las interfaces requeridas.\n",
        "    \"\"\"\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Prueba de polimorfismo\n",
        "from random import randint\n",
        "\n",
        "# Creamos un agente aleatorio simple\n",
        "class AgenteAleatorio:\n",
        "    def __init__(self, num_acciones):\n",
        "        self.num_acciones = num_acciones\n",
        "\n",
        "    def seleccionar_accion(self, estado):\n",
        "        return randint(0, self.num_acciones - 1)\n",
        "\n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado):\n",
        "        pass  # Este agente no aprende\n",
        "\n",
        "# Entrenamos al agente en diferentes entornos\n",
        "entorno1 = EntornoSimple()\n",
        "entorno2 = EntornoRejilla(ancho=5, alto=5)\n",
        "\n",
        "agente1 = AgenteAleatorio(entorno1.obtener_num_acciones())\n",
        "agente2 = AgenteAleatorio(entorno2.obtener_num_acciones())\n",
        "\n",
        "resultado1 = entrenar(agente1, entorno1, episodios=10)\n",
        "resultado2 = entrenar(agente2, entorno2, episodios=10)\n",
        "\n",
        "print(\"Resultados en EntornoSimple:\", resultado1)\n",
        "print(\"Resultados en EntornoRejilla:\", resultado2)"
      ],
      "metadata": {
        "id": "Vp6GQ2ZjFXzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 12: Manejo de Excepciones en Entorno de RL\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Implementar manejo de excepciones robusto para un entorno de aprendizaje por refuerzo.\n",
        "\n",
        "**Descripci贸n**: Crea excepciones personalizadas y un sistema de manejo de errores para un entorno de RL.\n",
        "\n",
        "Debes implementar:\n",
        "1. Excepciones personalizadas:\n",
        "   - `EntornoError`: Clase base para excepciones del entorno\n",
        "   - `AccionInvalidaError`: Cuando se intenta una acci贸n no v谩lida\n",
        "   - `EstadoInvalidoError`: Cuando se intenta acceder a un estado no v谩lido\n",
        "\n",
        "2. Un entorno robusto `EntornoRobusto` con manejo de excepciones para:\n",
        "   - Validar acciones\n",
        "   - Validar estados\n",
        "   - Manejar errores durante la ejecuci贸n de acciones\n",
        "\n",
        "Usa bloques try/except en los m茅todos relevantes y proporciona mensajes de error descriptivos."
      ],
      "metadata": {
        "id": "1OF6DHDKFZjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Excepciones personalizadas para entorno de RL\n",
        "class EntornoError(Exception):\n",
        "    \"\"\"Clase base para excepciones relacionadas con el entorno.\"\"\"\n",
        "    pass\n",
        "\n",
        "class AccionInvalidaError(EntornoError):\n",
        "    \"\"\"Se lanza cuando se intenta realizar una acci贸n no v谩lida.\"\"\"\n",
        "    pass\n",
        "\n",
        "class EstadoInvalidoError(EntornoError):\n",
        "    \"\"\"Se lanza cuando se intenta acceder a un estado no v谩lido.\"\"\"\n",
        "    pass\n",
        "\n",
        "# Completa la clase EntornoRobusto con manejo de excepciones\n",
        "class EntornoRobusto:\n",
        "    def __init__(self, num_estados=10, num_acciones=4):\n",
        "        self.num_estados = num_estados\n",
        "        self.num_acciones = num_acciones\n",
        "        self.estado_actual = 0\n",
        "        self.terminado = False\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reinicia el entorno al estado inicial.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def validar_accion(self, accion):\n",
        "        \"\"\"Valida que una acci贸n sea v谩lida, lanzando excepci贸n si no lo es.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def validar_estado(self, estado):\n",
        "        \"\"\"Valida que un estado sea v谩lido, lanzando excepci贸n si no lo es.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def paso(self, accion):\n",
        "        \"\"\"\n",
        "        Ejecuta una acci贸n y retorna (siguiente_estado, recompensa, terminado, info).\n",
        "        Incluye manejo de excepciones.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Validar acci贸n\n",
        "            # Tu c贸digo aqu铆\n",
        "\n",
        "            # L贸gica del entorno\n",
        "            # Tu c贸digo aqu铆\n",
        "\n",
        "            # Retornar resultado\n",
        "            # Tu c贸digo aqu铆\n",
        "            pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "        except AccionInvalidaError as e:\n",
        "            # Manejo del error\n",
        "            # Tu c贸digo aqu铆\n",
        "            pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "        except Exception as e:\n",
        "            # Manejo de otros errores inesperados\n",
        "            # Tu c贸digo aqu铆\n",
        "            pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Prueba el manejo de excepciones\n",
        "entorno = EntornoRobusto(num_estados=5, num_acciones=2)\n",
        "estado = entorno.reset()\n",
        "print(f\"Estado inicial: {estado}\")\n",
        "\n",
        "# Prueba con acci贸n v谩lida\n",
        "try:\n",
        "    resultado = entorno.paso(1)\n",
        "    print(f\"Paso exitoso: {resultado}\")\n",
        "except EntornoError as e:\n",
        "    print(f\"Error controlado: {e}\")\n",
        "\n",
        "# Prueba con acci贸n inv谩lida\n",
        "try:\n",
        "    resultado = entorno.paso(5)  # Acci贸n fuera de rango\n",
        "    print(f\"Paso exitoso: {resultado}\")\n",
        "except EntornoError as e:\n",
        "    print(f\"Error controlado: {e}\")"
      ],
      "metadata": {
        "id": "IPaiOlUWFbks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 13: Decorador Parametrizado para Control de Intentos\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Crear decoradores parametrizados que a帽adan funcionalidad espec铆fica a funciones.\n",
        "\n",
        "**Descripci贸n**: Implementa un decorador `reintentar` que intentar谩 ejecutar una funci贸n varias veces si ocurre una excepci贸n.\n",
        "\n",
        "El decorador debe:\n",
        "1. Aceptar par谩metros para configurar el comportamiento:\n",
        "   - `max_intentos`: N煤mero m谩ximo de intentos (por defecto 3)\n",
        "   - `excepciones`: Tupla de tipos de excepciones a capturar (por defecto todas)\n",
        "   - `espera`: Tiempo de espera entre intentos (por defecto 0)\n",
        "2. Aplicar la l贸gica de reintentos solo si ocurre alguna de las excepciones especificadas\n",
        "3. Registrar cada intento fallido y el error\n",
        "4. Tras alcanzar el n煤mero m谩ximo de intentos, relanzar la 煤ltima excepci贸n\n",
        "\n",
        "Este tipo de decorador es 煤til en entornos donde pueden ocurrir errores transitorios."
      ],
      "metadata": {
        "id": "J8nO5ZdlFdac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Completa el decorador parametrizado reintentar\n",
        "def reintentar(max_intentos=3, excepciones=(Exception,), espera=0):\n",
        "    # Tu c贸digo aqu铆 - recuerda que es un decorador con argumentos\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Funci贸n que simula un servicio inestable\n",
        "@reintentar(max_intentos=5, excepciones=(ValueError, RuntimeError), espera=0.5)\n",
        "def servicio_inestable(probabilidad_error=0.7):\n",
        "    \"\"\"\n",
        "    Simula un servicio que falla con cierta probabilidad.\n",
        "    \"\"\"\n",
        "    # Simula un servicio que falla con cierta probabilidad\n",
        "    if random.random() < probabilidad_error:\n",
        "        if random.random() < 0.5:\n",
        "            raise ValueError(\"Error de valor simulado\")\n",
        "        else:\n",
        "            raise RuntimeError(\"Error de ejecuci贸n simulado\")\n",
        "    return \"隆Operaci贸n exitosa!\"\n",
        "\n",
        "# Prueba el servicio con reintento\n",
        "for i in range(3):  # Prueba 3 veces\n",
        "    try:\n",
        "        resultado = servicio_inestable()\n",
        "        print(f\"Intento {i+1}: {resultado}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Intento {i+1}: Fall贸 despu茅s de m煤ltiples intentos: {e}\")"
      ],
      "metadata": {
        "id": "4znG41vnFfks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 14: Jerarqu铆a de Excepciones Personalizadas\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Dise帽ar una jerarqu铆a completa de excepciones personalizadas para un framework de aprendizaje por refuerzo.\n",
        "\n",
        "**Descripci贸n**: Crea una estructura jer谩rquica de excepciones para diferentes componentes del framework:\n",
        "\n",
        "1. Estructura b谩sica:\n",
        "   - `RLError`: Clase base para todas las excepciones del framework\n",
        "   - `AgentError`: Errores relacionados con agentes\n",
        "   - `EntornoError`: Errores relacionados con entornos\n",
        "   - `PoliticaError`: Errores relacionados con pol铆ticas\n",
        "   - `EntrenamientoError`: Errores relacionados con el proceso de entrenamiento\n",
        "\n",
        "2. Excepciones espec铆ficas para cada categor铆a\n",
        "\n",
        "3. Funci贸n `entrenar_agente_robusto` que utiliza diferentes tipos de excepciones para mostrar\n",
        "   c贸mo pueden manejarse de manera diferenciada.\n",
        "\n",
        "Este enfoque permite un manejo de errores m谩s granular y espec铆fico."
      ],
      "metadata": {
        "id": "-zl_5Th-FhZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Completa la jerarqu铆a de excepciones para un framework RL\n",
        "\n",
        "# Excepci贸n base para todo el framework\n",
        "class RLError(Exception):\n",
        "    \"\"\"Clase base para todas las excepciones del framework RL.\"\"\"\n",
        "    pass\n",
        "\n",
        "# ---- Completa las excepciones relacionadas con agentes ----\n",
        "class AgentError(RLError):\n",
        "    \"\"\"Error relacionado con agentes.\"\"\"\n",
        "    pass\n",
        "\n",
        "class AgenteFuncionFaltanteError(AgentError):\n",
        "    \"\"\"Error cuando un agente no implementa una funci贸n requerida.\"\"\"\n",
        "    def __init__(self, funcion):\n",
        "        self.funcion = funcion\n",
        "        super().__init__(f\"El agente debe implementar la funci贸n '{funcion}'\")\n",
        "\n",
        "# ---- Completa las excepciones relacionadas con entornos ----\n",
        "class EntornoError(RLError):\n",
        "    \"\"\"Error relacionado con entornos.\"\"\"\n",
        "    pass\n",
        "\n",
        "class EntornoInterfazError(EntornoError):\n",
        "    \"\"\"Error cuando un entorno no implementa la interfaz requerida.\"\"\"\n",
        "    pass\n",
        "\n",
        "# ---- Completa las excepciones relacionadas con pol铆ticas ----\n",
        "class PoliticaError(RLError):\n",
        "    \"\"\"Error relacionado con pol铆ticas.\"\"\"\n",
        "    pass\n",
        "\n",
        "# ---- Completa las excepciones relacionadas con entrenamiento ----\n",
        "class EntrenamientoError(RLError):\n",
        "    \"\"\"Error relacionado con entrenamiento.\"\"\"\n",
        "    pass\n",
        "\n",
        "class EpisodioError(EntrenamientoError):\n",
        "    \"\"\"Error durante un episodio de entrenamiento.\"\"\"\n",
        "    pass\n",
        "\n",
        "# Funci贸n de ejemplo que utiliza las excepciones\n",
        "def entrenar_agente_robusto(agente, entorno, num_episodios):\n",
        "    \"\"\"\n",
        "    Funci贸n de ejemplo que usa el sistema de excepciones para un entrenamiento robusto.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not hasattr(agente, 'seleccionar_accion'):\n",
        "            raise AgenteFuncionFaltanteError(\"seleccionar_accion\")\n",
        "\n",
        "        if not hasattr(entorno, 'reset') or not hasattr(entorno, 'paso'):\n",
        "            raise EntornoInterfazError(\"El entorno debe implementar reset() y paso()\")\n",
        "\n",
        "        resultados = []\n",
        "        for episodio in range(num_episodios):\n",
        "            try:\n",
        "                recompensa_total = ejecutar_episodio(agente, entorno)\n",
        "                resultados.append(recompensa_total)\n",
        "            except EpisodioError as e:\n",
        "                print(f\"Error en episodio {episodio}: {e}\")\n",
        "                # Contin煤a con el siguiente episodio\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    except AgentError as e:\n",
        "        print(f\"Error cr铆tico en el agente: {e}\")\n",
        "        return None\n",
        "    except EntornoError as e:\n",
        "        print(f\"Error cr铆tico en el entorno: {e}\")\n",
        "        return None\n",
        "    except RLError as e:\n",
        "        print(f\"Error general en el framework: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error inesperado: {e}\")\n",
        "        raise  # Re-lanza excepciones no controladas\n",
        "\n",
        "# Funci贸n auxiliar para el ejemplo\n",
        "def ejecutar_episodio(agente, entorno):\n",
        "    \"\"\"Ejecuta un episodio de entrenamiento.\"\"\"\n",
        "    estado = entorno.reset()\n",
        "    recompensa_total = 0\n",
        "    terminado = False\n",
        "\n",
        "    while not terminado:\n",
        "        try:\n",
        "            accion = agente.seleccionar_accion(estado)\n",
        "            siguiente_estado, recompensa, terminado, _ = entorno.paso(accion)\n",
        "            agente.aprender(estado, accion, recompensa, siguiente_estado)\n",
        "            estado = siguiente_estado\n",
        "            recompensa_total += recompensa\n",
        "        except Exception as e:\n",
        "            # Convertimos la excepci贸n a un tipo espec铆fico de nuestro framework\n",
        "            raise EpisodioError(f\"Error durante el episodio: {e}\")\n",
        "\n",
        "    return recompensa_total\n",
        "\n",
        "# Prueba con algunas excepciones\n",
        "class AgenteIncompleto:\n",
        "    # Falta el m茅todo seleccionar_accion\n",
        "    pass\n",
        "\n",
        "class EntornoIncompleto:\n",
        "    def reset(self):\n",
        "        return 0\n",
        "    # Falta el m茅todo paso\n",
        "\n",
        "# Escenarios de prueba\n",
        "try:\n",
        "    entrenar_agente_robusto(AgenteIncompleto(), EntornoIncompleto(), 10)\n",
        "except Exception as e:\n",
        "    print(f\"Error de prueba: {e}\")"
      ],
      "metadata": {
        "id": "zNDWm5-YFjDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 15: Uso Avanzado de Try-Except-Finally\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Implementar un sistema robusto de manejo de errores con try-except-finally para garantizar limpieza de recursos.\n",
        "\n",
        "**Descripci贸n**: Crea un sistema de guardado de modelos para agentes de RL que use bloques `try-except-finally` para garantizar la persistencia de datos incluso en caso de errores.\n",
        "\n",
        "La clase `ModelManager` debe:\n",
        "1. Manejar el guardado seguro de modelos usando archivos temporales\n",
        "2. Garantizar limpieza de recursos incluso en caso de fallo\n",
        "3. Implementar manejo espec铆fico para diferentes tipos de errores\n",
        "4. Proporcionar funciones para cargar, listar y eliminar modelos\n",
        "\n",
        "Este enfoque garantiza que no se pierdan datos ni queden recursos sin liberar, incluso cuando ocurren errores."
      ],
      "metadata": {
        "id": "QEDyy0dLFk8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Completa la clase ModelManager para gesti贸n robusta de modelos\n",
        "class ModelManager:\n",
        "    def __init__(self, directorio=\"modelos\"):\n",
        "        self.directorio = directorio\n",
        "        self.archivo_temporal = None\n",
        "\n",
        "        # Crea el directorio si no existe\n",
        "        if not os.path.exists(directorio):\n",
        "            os.makedirs(directorio)\n",
        "\n",
        "    def guardar_modelo(self, agente, nombre_archivo=None):\n",
        "        \"\"\"\n",
        "        Guarda un modelo de agente de forma segura, con protecci贸n contra corrupci贸n.\n",
        "        Usa try-except-finally para garantizar limpieza de recursos.\n",
        "        \"\"\"\n",
        "        # Tu c贸digo aqu铆: implementa guardado seguro usando try-except-finally\n",
        "        # Recuerda utilizar un archivo temporal primero, y renombrarlo solo si todo va bien\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def cargar_modelo(self, nombre_archivo):\n",
        "        \"\"\"\n",
        "        Carga un modelo de agente con manejo de errores.\n",
        "        Incluye verificaci贸n de integridad b谩sica.\n",
        "        \"\"\"\n",
        "        # Tu c贸digo aqu铆: implementa carga segura usando try-except\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def listar_modelos(self):\n",
        "        \"\"\"Lista todos los modelos guardados en el directorio.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def eliminar_modelo(self, nombre_archivo):\n",
        "        \"\"\"Elimina un modelo con confirmaci贸n de 茅xito.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Agente simple para pruebas\n",
        "class AgenteDemo:\n",
        "    def __init__(self, nombre=\"AgentePrueba\", parametros=None):\n",
        "        self.nombre = nombre\n",
        "        self.parametros = parametros or {\"alpha\": 0.1, \"gamma\": 0.9}\n",
        "        self.q_table = {str(i): [random.random() for _ in range(4)] for i in range(5)}\n",
        "        self.timestamp = time.time()\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Agente '{self.nombre}' (timestamp: {self.timestamp})\"\n",
        "\n",
        "# Prueba el sistema de guardado y carga\n",
        "manager = ModelManager()\n",
        "\n",
        "# Crea y guarda un agente\n",
        "agente_original = AgenteDemo(\"Explorador\")\n",
        "print(f\"Agente original: {agente_original}\")\n",
        "\n",
        "# Guarda el modelo\n",
        "try:\n",
        "    ruta = manager.guardar_modelo(agente_original, \"explorador_v1\")\n",
        "    print(f\"Modelo guardado en: {ruta}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al guardar: {e}\")\n",
        "\n",
        "# Lista modelos disponibles\n",
        "modelos = manager.listar_modelos()\n",
        "print(f\"Modelos disponibles: {modelos}\")\n",
        "\n",
        "# Carga el modelo\n",
        "try:\n",
        "    agente_cargado = manager.cargar_modelo(\"explorador_v1\")\n",
        "    print(f\"Modelo cargado: {agente_cargado}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar: {e}\")\n",
        "\n",
        "# Simula una corrupci贸n o error durante guardado\n",
        "try:\n",
        "    # Forzamos un error durante guardado\n",
        "    class AgenteProblematico(AgenteDemo):\n",
        "        def __getstate__(self):\n",
        "            # Este m茅todo se llama durante la serializaci贸n con pickle\n",
        "            raise RuntimeError(\"隆Error simulado durante serializaci贸n!\")\n",
        "\n",
        "    agente_malo = AgenteProblematico(\"AgenteMalo\")\n",
        "    manager.guardar_modelo(agente_malo, \"no_deberia_existir\")\n",
        "except Exception as e:\n",
        "    print(f\"Error esperado capturado: {e}\")\n",
        "\n",
        "# Verificamos que los archivos temporales se hayan limpiado\n",
        "archivos = os.listdir(manager.directorio)\n",
        "print(f\"Archivos en directorio: {archivos}\")"
      ],
      "metadata": {
        "id": "C5tjmxfjFnHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 16: Patr贸n Singleton para Configuraci贸n de Experimentos\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Implementar el patr贸n de dise帽o Singleton para mantener una configuraci贸n global.\n",
        "\n",
        "**Descripci贸n**: Crea una clase `ConfigExperimentos` que use el patr贸n Singleton para mantener una configuraci贸n global de experimentos de aprendizaje por refuerzo.\n",
        "\n",
        "La clase debe:\n",
        "1. Implementar el patr贸n Singleton (solo puede existir una instancia)\n",
        "2. Almacenar par谩metros de configuraci贸n globales como seed, n煤mero de episodios, etc.\n",
        "3. Permitir registrar y recuperar configuraciones de entornos\n",
        "4. Proporcionar m茅todo para reiniciar la configuraci贸n a valores por defecto\n",
        "\n",
        "Este patr贸n evita la duplicaci贸n de configuraciones y garantiza que todos los componentes\n",
        "del sistema accedan a los mismos par谩metros."
      ],
      "metadata": {
        "id": "m_9Nlnz1Fo0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Completa el Singleton para configuraci贸n de experimentos\n",
        "\n",
        "class ConfigExperimentos:\n",
        "    \"\"\"\n",
        "    Singleton para gestionar la configuraci贸n global de experimentos.\n",
        "    \"\"\"\n",
        "    _instancia = None\n",
        "\n",
        "    def __new__(cls):\n",
        "        # Implementa el patr贸n Singleton\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def __init__(self):\n",
        "        # Solo se ejecuta una vez para la 煤nica instancia\n",
        "        # Valores por defecto\n",
        "        if not hasattr(self, 'inicializado'):\n",
        "            self.seed = 42\n",
        "            self.num_episodios = 1000\n",
        "            self.max_pasos = 500\n",
        "            self.directorio_resultados = \"./resultados\"\n",
        "            self.hiperparametros = {\n",
        "                \"alpha\": 0.1,\n",
        "                \"gamma\": 0.99,\n",
        "                \"epsilon\": 0.1\n",
        "            }\n",
        "            self.entornos_registrados = []\n",
        "            self.inicializado = True\n",
        "\n",
        "    def registrar_entorno(self, nombre_entorno, configuracion):\n",
        "        \"\"\"Registra un nuevo entorno con su configuraci贸n.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def obtener_entorno(self, nombre_entorno):\n",
        "        \"\"\"Obtiene la configuraci贸n de un entorno registrado.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def reiniciar_configuracion(self):\n",
        "        \"\"\"Reinicia la configuraci贸n a valores por defecto.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Representaci贸n en texto de la configuraci贸n actual.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Prueba el Singleton\n",
        "# 1. Obtenemos la instancia y modificamos valores\n",
        "config1 = ConfigExperimentos()\n",
        "config1.seed = 123\n",
        "config1.hiperparametros[\"alpha\"] = 0.05\n",
        "config1.registrar_entorno(\"CartPole\", {\"version\": \"v1\", \"render_mode\": \"rgb_array\"})\n",
        "\n",
        "# 2. Obtenemos otra \"instancia\" (que debe ser la misma)\n",
        "config2 = ConfigExperimentos()\n",
        "print(f\"驴Son el mismo objeto? {config1 is config2}\")  # Debe ser True\n",
        "print(f\"Seed en config2: {config2.seed}\")  # Debe mostrar 123\n",
        "print(f\"Alpha en config2: {config2.hiperparametros['alpha']}\")  # Debe mostrar 0.05\n",
        "\n",
        "# 3. Verificamos que los entornos registrados son accesibles\n",
        "entorno = config2.obtener_entorno(\"CartPole\")\n",
        "print(f\"Entorno CartPole: {entorno}\")"
      ],
      "metadata": {
        "id": "KHEqsAYyFq9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 17: Patr贸n Factory para Crear Diferentes Tipos de Entornos\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Implementar el patr贸n de dise帽o Factory para crear diferentes tipos de objetos.\n",
        "\n",
        "**Descripci贸n**: Crea una Factory para generar diferentes tipos de entornos de aprendizaje por refuerzo.\n",
        "\n",
        "El sistema debe incluir:\n",
        "1. Clase base abstracta `Entorno` definiendo la interfaz com煤n\n",
        "2. Implementaciones de diferentes tipos de entornos:\n",
        "   - `EntornoDiscreto`: Con espacio de estados discreto y finito\n",
        "   - `EntornoRejilla`: Entorno tipo rejilla 2D\n",
        "   - `EntornoDinamico`: Entorno con din谩mica cambiante\n",
        "3. `EntornoFactory`: Factory b谩sica para crear entornos a partir de su tipo\n",
        "4. `RegistroEntornos`: Factory avanzada con registro din谩mico de tipos de entornos\n",
        "\n",
        "Este patr贸n facilita la creaci贸n de diferentes tipos de objetos sin exponer la l贸gica\n",
        "de instanciaci贸n al cliente."
      ],
      "metadata": {
        "id": "t3T0xrDHFuQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import random\n",
        "\n",
        "# Completa la jerarqu铆a de clases de entornos y su factory\n",
        "\n",
        "# Clase base abstracta para entornos\n",
        "class Entorno(ABC):\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        \"\"\"Reinicia el entorno y retorna el estado inicial.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def paso(self, accion):\n",
        "        \"\"\"Ejecuta acci贸n y retorna (siguiente_estado, recompensa, terminado, info).\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def obtener_num_acciones(self):\n",
        "        \"\"\"Retorna el n煤mero de acciones posibles.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def obtener_nombre(self):\n",
        "        \"\"\"Retorna el nombre del entorno.\"\"\"\n",
        "        pass\n",
        "\n",
        "# Implementa diferentes tipos de entornos\n",
        "\n",
        "class EntornoDiscreto(Entorno):\n",
        "    \"\"\"Entorno con espacio de estados discreto y finito.\"\"\"\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "class EntornoRejilla(Entorno):\n",
        "    \"\"\"Entorno tipo rejilla 2D.\"\"\"\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "class EntornoDinamico(Entorno):\n",
        "    \"\"\"Entorno con din谩mica cambiante.\"\"\"\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Implementa la factory de entornos\n",
        "\n",
        "class EntornoFactory:\n",
        "    \"\"\"Factory para crear diferentes tipos de entornos.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def crear_entorno(tipo, **kwargs):\n",
        "        \"\"\"\n",
        "        Crea un entorno del tipo especificado con los par谩metros dados.\n",
        "\n",
        "        Args:\n",
        "            tipo: Tipo de entorno (\"discreto\", \"rejilla\", \"dinamico\")\n",
        "            **kwargs: Par谩metros espec铆ficos para cada tipo de entorno\n",
        "\n",
        "        Returns:\n",
        "            Instancia del entorno solicitado\n",
        "        \"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Factory m谩s avanzada con registro din谩mico de entornos\n",
        "\n",
        "class RegistroEntornos:\n",
        "    \"\"\"Factory con registro din谩mico de tipos de entornos.\"\"\"\n",
        "\n",
        "    _entornos_registrados = {}\n",
        "\n",
        "    @classmethod\n",
        "    def registrar(cls, nombre, clase_entorno):\n",
        "        \"\"\"Registra un nuevo tipo de entorno.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    @classmethod\n",
        "    def crear_entorno(cls, nombre, **kwargs):\n",
        "        \"\"\"Crea un entorno del tipo registrado.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    @classmethod\n",
        "    def listar_entornos(cls):\n",
        "        \"\"\"Lista todos los tipos de entornos registrados.\"\"\"\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Registramos los entornos disponibles\n",
        "RegistroEntornos.registrar(\"discreto\", EntornoDiscreto)\n",
        "RegistroEntornos.registrar(\"rejilla\", EntornoRejilla)\n",
        "RegistroEntornos.registrar(\"dinamico\", EntornoDinamico)\n",
        "\n",
        "# Prueba la factory simple\n",
        "entorno1 = EntornoFactory.crear_entorno(\"discreto\", num_estados=10, num_acciones=3)\n",
        "entorno2 = EntornoFactory.crear_entorno(\"rejilla\", ancho=5, alto=5)\n",
        "\n",
        "print(f\"Entorno 1: {entorno1.obtener_nombre()}, acciones: {entorno1.obtener_num_acciones()}\")\n",
        "print(f\"Entorno 2: {entorno2.obtener_nombre()}, acciones: {entorno2.obtener_num_acciones()}\")\n",
        "\n",
        "# Prueba el registro din谩mico\n",
        "entornos_disponibles = RegistroEntornos.listar_entornos()\n",
        "print(f\"Entornos registrados: {entornos_disponibles}\")\n",
        "\n",
        "entorno3 = RegistroEntornos.crear_entorno(\"dinamico\", dificultad=\"alta\")\n",
        "print(f\"Entorno 3: {entorno3.obtener_nombre()}\")\n",
        "\n",
        "# Intenta crear un tipo no registrado\n",
        "try:\n",
        "    entorno_invalido = RegistroEntornos.crear_entorno(\"inexistente\")\n",
        "except Exception as e:\n",
        "    print(f\"Error esperado: {e}\")"
      ],
      "metadata": {
        "id": "dLWgVSPqFwUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 18: Patr贸n Strategy para Pol铆ticas de Exploraci贸n\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Implementar el patr贸n de dise帽o Strategy para crear comportamientos intercambiables.\n",
        "\n",
        "**Descripci贸n**: Crea un sistema basado en el patr贸n Strategy para pol铆ticas de exploraci贸n intercambiables en un agente de aprendizaje por refuerzo.\n",
        "\n",
        "El sistema debe incluir:\n",
        "1. Interfaz `EstrategiaExploracion` con m茅todos comunes\n",
        "2. Implementaciones concretas de estrategias:\n",
        "   - `EpsilonGreedy`: Basada en probabilidad epsilon para exploraci贸n\n",
        "   - `Softmax`: Usa distribuci贸n de probabilidad basada en valores Q\n",
        "   - `UCB`: Balance exploraci贸n/explotaci贸n basado en Upper Confidence Bound\n",
        "3. Clase `AgenteConEstrategia` que utilice estas estrategias de forma intercambiable\n",
        "\n",
        "Este patr贸n permite cambiar algoritmos \"en tiempo de ejecuci贸n\", ideal para experimentar\n",
        "con diferentes pol铆ticas de exploraci贸n."
      ],
      "metadata": {
        "id": "3klrN-i5Fx8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Completa la jerarqu铆a de estrategias de exploraci贸n\n",
        "\n",
        "# Interfaz para estrategias de exploraci贸n\n",
        "class EstrategiaExploracion(ABC):\n",
        "    \"\"\"Interfaz para implementar diferentes estrategias de exploraci贸n.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def seleccionar_accion(self, estado, valores_q):\n",
        "        \"\"\"\n",
        "        Selecciona una acci贸n basada en los valores Q y la estrategia.\n",
        "\n",
        "        Args:\n",
        "            estado: El estado actual\n",
        "            valores_q: Lista/array de valores Q para cada acci贸n posible\n",
        "\n",
        "        Returns:\n",
        "            ndice de la acci贸n seleccionada\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def actualizar(self, **kwargs):\n",
        "        \"\"\"Actualiza par谩metros internos de la estrategia.\"\"\"\n",
        "        pass\n",
        "\n",
        "# Implementa estrategias concretas\n",
        "\n",
        "class EpsilonGreedy(EstrategiaExploracion):\n",
        "    \"\"\"Estrategia epsilon-greedy con decaimiento opcional.\"\"\"\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "class Softmax(EstrategiaExploracion):\n",
        "    \"\"\"Estrategia Softmax/Boltzmann usando distribuci贸n de probabilidad.\"\"\"\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "class UCB(EstrategiaExploracion):\n",
        "    \"\"\"Estrategia Upper Confidence Bound para balance exploraci贸n/explotaci贸n.\"\"\"\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# Agente flexible que utiliza una estrategia de exploraci贸n\n",
        "class AgenteConEstrategia:\n",
        "    \"\"\"Agente de RL que puede cambiar de estrategia de exploraci贸n.\"\"\"\n",
        "\n",
        "    def __init__(self, num_acciones, estrategia_exploracion):\n",
        "        self.num_acciones = num_acciones\n",
        "        self.q_table = {}  # Tabla de valores Q: {estado: [valores para cada acci贸n]}\n",
        "        self.estrategia = estrategia_exploracion\n",
        "\n",
        "    def seleccionar_accion(self, estado):\n",
        "        \"\"\"Selecciona una acci贸n usando la estrategia actual.\"\"\"\n",
        "        # Convertimos el estado a string para usarlo como clave\n",
        "        estado_str = str(estado)\n",
        "\n",
        "        # Si es la primera vez que vemos este estado, inicializamos valores\n",
        "        if estado_str not in self.q_table:\n",
        "            self.q_table[estado_str] = [0.0] * self.num_acciones\n",
        "\n",
        "        # Delegamos la selecci贸n a la estrategia de exploraci贸n\n",
        "        return self.estrategia.seleccionar_accion(estado, self.q_table[estado_str])\n",
        "\n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado, alpha=0.1, gamma=0.9):\n",
        "        \"\"\"Actualiza tabla Q con aprendizaje b谩sico.\"\"\"\n",
        "        estado_str = str(estado)\n",
        "        sig_estado_str = str(siguiente_estado)\n",
        "\n",
        "        # Inicializamos valores si es necesario\n",
        "        if estado_str not in self.q_table:\n",
        "            self.q_table[estado_str] = [0.0] * self.num_acciones\n",
        "        if sig_estado_str not in self.q_table:\n",
        "            self.q_table[sig_estado_str] = [0.0] * self.num_acciones\n",
        "\n",
        "        # Actualizaci贸n Q-Learning\n",
        "        q_actual = self.q_table[estado_str][accion]\n",
        "        max_q_siguiente = max(self.q_table[sig_estado_str])\n",
        "\n",
        "        # F贸rmula Q-Learning\n",
        "        self.q_table[estado_str][accion] = q_actual + alpha * (\n",
        "            recompensa + gamma * max_q_siguiente - q_actual)\n",
        "\n",
        "    def cambiar_estrategia(self, nueva_estrategia):\n",
        "        \"\"\"Cambia la estrategia de exploraci贸n en tiempo de ejecuci贸n.\"\"\"\n",
        "        self.estrategia = nueva_estrategia\n",
        "        return self\n",
        "\n",
        "    def actualizar_estrategia(self, **kwargs):\n",
        "        \"\"\"Actualiza par谩metros de la estrategia actual.\"\"\"\n",
        "        return self.estrategia.actualizar(**kwargs)\n",
        "\n",
        "# Prueba el patr贸n Strategy con las diferentes estrategias\n",
        "# Simulamos un estado y valores Q para pruebas\n",
        "estado_prueba = [1, 2, 3]\n",
        "valores_q_prueba = [0.1, 0.5, 0.3, 0.7]\n",
        "\n",
        "# Creamos las estrategias\n",
        "epsilon_greedy = EpsilonGreedy(epsilon=0.3, decaimiento=0.95)\n",
        "softmax = Softmax(temperatura=1.0)\n",
        "ucb = UCB(c=2.0)\n",
        "\n",
        "# Creamos agente con estrategia epsilon-greedy\n",
        "agente = AgenteConEstrategia(num_acciones=4, estrategia_exploracion=epsilon_greedy)\n",
        "\n",
        "# Simulamos algunas selecciones con epsilon-greedy\n",
        "print(\"Estrategia: Epsilon-Greedy\")\n",
        "for i in range(5):\n",
        "    accion = agente.seleccionar_accion(estado_prueba)\n",
        "    print(f\"Selecci贸n {i+1}: Acci贸n {accion}\")\n",
        "    agente.actualizar_estrategia()  # Aplica decaimiento\n",
        "\n",
        "# Cambiamos a estrategia Softmax\n",
        "agente.cambiar_estrategia(softmax)\n",
        "print(\"\\nEstrategia: Softmax\")\n",
        "for i in range(5):\n",
        "    accion = agente.seleccionar_accion(estado_prueba)\n",
        "    print(f\"Selecci贸n {i+1}: Acci贸n {accion}\")\n",
        "    agente.actualizar_estrategia(temperatura=max(0.1, softmax.temperatura * 0.9))\n",
        "\n",
        "# Cambiamos a estrategia UCB\n",
        "agente.cambiar_estrategia(ucb)\n",
        "print(\"\\nEstrategia: UCB\")\n",
        "for i in range(5):\n",
        "    accion = agente.seleccionar_accion(estado_prueba)\n",
        "    print(f\"Selecci贸n {i+1}: Acci贸n {accion}\")"
      ],
      "metadata": {
        "id": "mpYvGDmlFzzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 19: Desarrollo de una Funci贸n para Entrenar Agentes de RL\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Integrar diferentes conceptos para crear una funci贸n completa de entrenamiento de agentes RL.\n",
        "\n",
        "**Descripci贸n**: Desarrolla una funci贸n completa para entrenar agentes de RL integrando varios conceptos vistos en el m贸dulo:\n",
        "- Manejo de excepciones\n",
        "- Par谩metros con valores por defecto\n",
        "- Algoritmos de aprendizaje\n",
        "- Control de progreso\n",
        "\n",
        "La funci贸n `entrenar_agente` debe:\n",
        "1. Entrenar un agente en un entorno durante m煤ltiples episodios\n",
        "2. Gestionar par谩metros como tasas de aprendizaje, factores de descuento, etc.\n",
        "3. Implementar decaimiento de epsilon para exploraci贸n\n",
        "4. Registrar y retornar m茅tricas de progreso\n",
        "5. Incluir manejo de excepciones para errores durante el entrenamiento\n",
        "6. Proporcionar evaluaciones peri贸dicas para medir el progreso real\n",
        "\n",
        "Este tipo de funci贸n es el coraz贸n de cualquier sistema de aprendizaje por refuerzo."
      ],
      "metadata": {
        "id": "IHCFsqSyF1Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def entrenar_agente(agente, entorno, num_episodios=500, max_pasos=1000,\n",
        "                   gamma=0.99, alpha=0.1, epsilon_inicial=1.0, epsilon_final=0.01,\n",
        "                   decaimiento_epsilon=0.995, intervalo_eval=10, mostrar_progreso=True):\n",
        "    \"\"\"\n",
        "    Funci贸n completa para entrenar agentes de aprendizaje por refuerzo.\n",
        "\n",
        "    Args:\n",
        "        agente: Objeto agente con m茅todos seleccionar_accion y aprender\n",
        "        entorno: Objeto entorno con m茅todos reset y paso\n",
        "        num_episodios: N煤mero de episodios de entrenamiento\n",
        "        max_pasos: N煤mero m谩ximo de pasos por episodio\n",
        "        gamma: Factor de descuento para el aprendizaje\n",
        "        alpha: Tasa de aprendizaje\n",
        "        epsilon_inicial: Valor inicial de epsilon para exploraci贸n\n",
        "        epsilon_final: Valor m铆nimo de epsilon\n",
        "        decaimiento_epsilon: Factor de decaimiento de epsilon\n",
        "        intervalo_eval: Cada cu谩ntos episodios evaluar sin exploraci贸n\n",
        "        mostrar_progreso: Si mostrar informaci贸n durante el entrenamiento\n",
        "\n",
        "    Returns:\n",
        "        dict: Diccionario con resultados del entrenamiento\n",
        "    \"\"\"\n",
        "    # Estructuras para almacenar resultados\n",
        "    todas_recompensas = []\n",
        "    recompensas_evaluacion = []\n",
        "    epsilon = epsilon_inicial\n",
        "    tiempo_inicio = time.time()\n",
        "\n",
        "    # Ventana deslizante para promediar recompensas recientes\n",
        "    ventana_recompensas = deque(maxlen=50)\n",
        "\n",
        "    # Funci贸n para ejecutar un episodio de evaluaci贸n (sin exploraci贸n)\n",
        "    def ejecutar_evaluacion():\n",
        "        estado = entorno.reset()\n",
        "        recompensa_total = 0\n",
        "        terminado = False\n",
        "\n",
        "        for paso in range(max_pasos):\n",
        "            # Durante evaluaci贸n, siempre seleccionamos la mejor acci贸n\n",
        "            # Tu c贸digo aqu铆\n",
        "            pass\n",
        "\n",
        "            if terminado:\n",
        "                break\n",
        "\n",
        "        return recompensa_total\n",
        "\n",
        "    # Bucle principal de entrenamiento\n",
        "    try:\n",
        "        for episodio in range(1, num_episodios + 1):\n",
        "            # Reiniciamos el entorno\n",
        "            estado = entorno.reset()\n",
        "            recompensa_episodio = 0\n",
        "            pasos_episodio = 0\n",
        "            terminado = False\n",
        "\n",
        "            # Bucle del episodio\n",
        "            for paso in range(max_pasos):\n",
        "                # Seleccionamos acci贸n seg煤n la pol铆tica actual (con exploraci贸n)\n",
        "                # Tu c贸digo aqu铆\n",
        "\n",
        "                # Ejecutamos la acci贸n en el entorno\n",
        "                # Tu c贸digo aqu铆\n",
        "\n",
        "                # El agente aprende de la experiencia\n",
        "                # Tu c贸digo aqu铆\n",
        "\n",
        "                # Actualizamos estado y contadores\n",
        "                # Tu c贸digo aqu铆\n",
        "\n",
        "                if terminado:\n",
        "                    break\n",
        "\n",
        "            # Guardamos estad铆sticas del episodio\n",
        "            # Tu c贸digo aqu铆\n",
        "\n",
        "            # Actualizamos 茅psilon con decaimiento\n",
        "            # Tu c贸digo aqu铆\n",
        "\n",
        "            # Evaluaci贸n peri贸dica sin exploraci贸n\n",
        "            if episodio % intervalo_eval == 0:\n",
        "                # Tu c贸digo aqu铆\n",
        "                pass\n",
        "\n",
        "            # Mostramos progreso\n",
        "            if mostrar_progreso and (episodio % 10 == 0 or episodio == 1):\n",
        "                # Tu c贸digo aqu铆\n",
        "                pass\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nEntrenamiento interrumpido por el usuario.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError durante el entrenamiento: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Calculamos estad铆sticas finales\n",
        "    tiempo_total = time.time() - tiempo_inicio\n",
        "\n",
        "    # Resultados finales\n",
        "    resultados = {\n",
        "        \"tiempo_total\": tiempo_total,\n",
        "        \"num_episodios\": episodio,\n",
        "        \"recompensas\": todas_recompensas,\n",
        "        \"recompensas_eval\": recompensas_evaluacion,\n",
        "        \"epsilon_final\": epsilon,\n",
        "        \"recompensa_mejor\": max(ventana_recompensas) if ventana_recompensas else 0,\n",
        "        \"recompensa_promedio\": np.mean(ventana_recompensas) if ventana_recompensas else 0\n",
        "    }\n",
        "\n",
        "    if mostrar_progreso:\n",
        "        print(f\"\\nEntrenamiento completado en {tiempo_total:.2f} segundos\")\n",
        "        print(f\"Recompensa promedio final: {resultados['recompensa_promedio']:.2f}\")\n",
        "\n",
        "    return resultados\n",
        "\n",
        "# Ejemplo de uso\n",
        "# Estas clases son solo para la demostraci贸n, deber铆an implementarse completamente\n",
        "class AgenteDemo:\n",
        "    def seleccionar_accion(self, estado, epsilon=0):\n",
        "        # Implementaci贸n simplificada para demo\n",
        "        return random.randint(0, 1)\n",
        "\n",
        "    def aprender(self, estado, accion, recompensa, sig_estado, alpha, gamma):\n",
        "        # Implementaci贸n simplificada para demo\n",
        "        pass\n",
        "\n",
        "class EntornoDemo:\n",
        "    def reset(self):\n",
        "        return [0, 0]\n",
        "\n",
        "    def paso(self, accion):\n",
        "        # Simplificado para demostraci贸n\n",
        "        recompensa = random.random()\n",
        "        sig_estado = [random.random(), random.random()]\n",
        "        terminado = random.random() < 0.1\n",
        "        return sig_estado, recompensa, terminado, {}\n",
        "\n",
        "# Ejecutamos entrenamiento de demostraci贸n\n",
        "agente_demo = AgenteDemo()\n",
        "entorno_demo = EntornoDemo()\n",
        "\n",
        "# Par谩metros simplificados para la demo\n",
        "resultados = entrenar_agente(\n",
        "    agente_demo,\n",
        "    entorno_demo,\n",
        "    num_episodios=50,  # N煤mero reducido para demostraci贸n\n",
        "    max_pasos=50,      # Pasos reducidos para demostraci贸n\n",
        "    intervalo_eval=10,\n",
        "    mostrar_progreso=True\n",
        ")\n",
        "\n",
        "# Visualizamos resultados\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(resultados[\"recompensas\"])\n",
        "plt.xlabel(\"Episodios\")\n",
        "plt.ylabel(\"Recompensa total\")\n",
        "plt.title(\"Curva de aprendizaje\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qmMCKGYFF3dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 20: Implementaci贸n Completa de un Sistema de Agentes y Entornos\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Integrar todos los conceptos aprendidos en un sistema completo de aprendizaje por refuerzo.\n",
        "\n",
        "**Descripci贸n**: Crea un sistema completo que combine excepciones personalizadas, patrones de dise帽o,\n",
        "y programaci贸n orientada a objetos para implementar un framework de aprendizaje por refuerzo.\n",
        "\n",
        "El sistema debe incluir:\n",
        "1. Jerarqu铆a de excepciones para diferentes componentes\n",
        "2. Estrategias de exploraci贸n usando el patr贸n Strategy\n",
        "3. Clases de entorno y agente con interfaces bien definidas\n",
        "4. Funciones de entrenamiento y evaluaci贸n\n",
        "5. Visualizaci贸n de resultados\n",
        "\n",
        "Este ejercicio final integra todos los conceptos del m贸dulo en un solo sistema coherente."
      ],
      "metadata": {
        "id": "81NOxf_-F4_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# ------------- Excepciones Personalizadas -------------\n",
        "class RLError(Exception):\n",
        "    \"\"\"Error base para el framework de RL.\"\"\"\n",
        "    pass\n",
        "\n",
        "class AgentError(RLError):\n",
        "    \"\"\"Error relacionado con agentes.\"\"\"\n",
        "    pass\n",
        "\n",
        "class EnvironmentError(RLError):\n",
        "    \"\"\"Error relacionado con entornos.\"\"\"\n",
        "    pass\n",
        "\n",
        "# ------------- Estrategias de Exploraci贸n (Patr贸n Strategy) -------------\n",
        "class ExplorationStrategy(ABC):\n",
        "    @abstractmethod\n",
        "    def select_action(self, state, q_values):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def update(self, **kwargs):\n",
        "        pass\n",
        "\n",
        "class EpsilonGreedy(ExplorationStrategy):\n",
        "    def __init__(self, epsilon=0.1, decay=0.995, epsilon_min=0.01):\n",
        "        self.epsilon = epsilon\n",
        "        self.decay = decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "    def select_action(self, state, q_values):\n",
        "        # Implementa la estrategia epsilon-greedy\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        # Actualiza epsilon con decaimiento\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# ------------- Interfaces Base -------------\n",
        "class Environment(ABC):\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        \"\"\"Reinicia el entorno y retorna estado inicial.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step(self, action):\n",
        "        \"\"\"Ejecuta acci贸n y retorna (next_state, reward, done, info).\"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def action_space(self):\n",
        "        \"\"\"Retorna el espacio de acciones.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def observation_space(self):\n",
        "        \"\"\"Retorna el espacio de observaciones.\"\"\"\n",
        "        pass\n",
        "\n",
        "class Agent(ABC):\n",
        "    @abstractmethod\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Selecciona acci贸n basada en estado actual.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Actualiza conocimiento basado en experiencia.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def save(self, path):\n",
        "        \"\"\"Guarda el modelo del agente.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def load(self, path):\n",
        "        \"\"\"Carga el modelo del agente.\"\"\"\n",
        "        pass\n",
        "\n",
        "# ------------- Implementaciones Concretas -------------\n",
        "\n",
        "# GridWorld: un entorno de rejilla 2D simple\n",
        "class GridWorld(Environment):\n",
        "    def __init__(self, width=5, height=5, max_steps=100):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.max_steps = max_steps\n",
        "        self.target_position = (height - 1, width - 1)  # Esquina inferior derecha\n",
        "        self.agent_position = None\n",
        "        self.steps = 0\n",
        "        self._action_space = 4  # Arriba, Derecha, Abajo, Izquierda\n",
        "        self._observation_space = width * height\n",
        "\n",
        "    def reset(self):\n",
        "        # Reinicia entorno y retorna estado inicial\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def step(self, action):\n",
        "        # Ejecuta acci贸n y retorna resultado\n",
        "        # Tu c贸digo aqu铆: mueve al agente seg煤n la acci贸n\n",
        "        # y calcula recompensa y estado terminal\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return self._action_space\n",
        "\n",
        "    @property\n",
        "    def observation_space(self):\n",
        "        return self._observation_space\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"Convierte posici贸n 2D a un 铆ndice de estado.\"\"\"\n",
        "        x, y = self.agent_position\n",
        "        return x * self.width + y\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Visualiza el entorno en consola.\"\"\"\n",
        "        # Tu c贸digo aqu铆: muestra la rejilla con agente y objetivo\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# QLearningAgent: agente que implementa el algoritmo Q-Learning\n",
        "class QLearningAgent(Agent):\n",
        "    def __init__(self, action_space, observation_space, **kwargs):\n",
        "        self.action_space = action_space\n",
        "        self.observation_space = observation_space\n",
        "        self.q_table = {}\n",
        "\n",
        "        # Hiperpar谩metros\n",
        "        self.alpha = kwargs.get('alpha', 0.1)  # Tasa de aprendizaje\n",
        "        self.gamma = kwargs.get('gamma', 0.99)  # Factor de descuento\n",
        "\n",
        "        # Estrategia de exploraci贸n (patr贸n Strategy)\n",
        "        self.exploration = kwargs.get('exploration',\n",
        "                                     EpsilonGreedy(epsilon=kwargs.get('epsilon', 0.1)))\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # Selecciona acci贸n usando estrategia de exploraci贸n\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        # Actualiza tabla Q seg煤n algoritmo Q-Learning\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def save(self, path):\n",
        "        # Guarda la tabla Q en un archivo\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "    def load(self, path):\n",
        "        # Carga la tabla Q desde un archivo\n",
        "        # Tu c贸digo aqu铆\n",
        "        pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# ------------- Funciones de Entrenamiento -------------\n",
        "def train_agent(agent, env, episodes=1000, max_steps=100,\n",
        "               verbose=True, eval_interval=100):\n",
        "    \"\"\"\n",
        "    Entrena un agente en un entorno.\n",
        "\n",
        "    Args:\n",
        "        agent: Instancia de Agent\n",
        "        env: Instancia de Environment\n",
        "        episodes: N煤mero de episodios\n",
        "        max_steps: Pasos m谩ximos por episodio\n",
        "        verbose: Si mostrar progreso\n",
        "        eval_interval: Intervalo para evaluaci贸n sin exploraci贸n\n",
        "\n",
        "    Returns:\n",
        "        dict: Resultados del entrenamiento\n",
        "    \"\"\"\n",
        "    # Implementa la funci贸n de entrenamiento completa\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "def evaluate_agent(agent, env, episodes=10, max_steps=100):\n",
        "    \"\"\"\n",
        "    Eval煤a un agente sin exploraci贸n.\n",
        "\n",
        "    Args:\n",
        "        agent: Instancia de Agent\n",
        "        env: Instancia de Environment\n",
        "        episodes: N煤mero de episodios\n",
        "        max_steps: Pasos m谩ximos por episodio\n",
        "\n",
        "    Returns:\n",
        "        float: Recompensa promedio\n",
        "    \"\"\"\n",
        "    # Implementa la funci贸n de evaluaci贸n\n",
        "    # Tu c贸digo aqu铆\n",
        "    pass  # Elimina esta l铆nea al implementar tu soluci贸n\n",
        "\n",
        "# ------------- Funci贸n Principal -------------\n",
        "def main():\n",
        "    # Creaci贸n de entorno y agente\n",
        "    env = GridWorld(width=5, height=5)\n",
        "\n",
        "    # Estrategia de exploraci贸n\n",
        "    exploration = EpsilonGreedy(epsilon=1.0, decay=0.995, epsilon_min=0.01)\n",
        "\n",
        "    # Agente\n",
        "    agent = QLearningAgent(\n",
        "        action_space=env.action_space,\n",
        "        observation_space=env.observation_space,\n",
        "        alpha=0.1,\n",
        "        gamma=0.99,\n",
        "        exploration=exploration\n",
        "    )\n",
        "\n",
        "    # Entrenamiento\n",
        "    results = train_agent(agent, env, episodes=500, max_steps=100)\n",
        "\n",
        "    # Visualizaci贸n de resultados\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(results['rewards'])\n",
        "    plt.xlabel('Episodio')\n",
        "    plt.ylabel('Recompensa total')\n",
        "    plt.title('Curva de aprendizaje')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(results['eval_rewards'])\n",
        "    plt.xlabel('Evaluaci贸n')\n",
        "    plt.ylabel('Recompensa promedio')\n",
        "    plt.title('Desempe帽o en evaluaci贸n')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Demostraci贸n final\n",
        "    print(\"\\n--- Demostraci贸n Final ---\")\n",
        "    for episode in range(3):\n",
        "        state = env.reset()\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            env.render()\n",
        "            time.sleep(0.3)  # Pausa para visualizaci贸n\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        print(f\"Episodio {episode+1}: Recompensa total = {total_reward}\")\n",
        "\n",
        "# Ejecuci贸n del programa principal\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la ejecuci贸n: {e}\")"
      ],
      "metadata": {
        "id": "ZJgGiEnJF7cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"FIN\""
      ],
      "metadata": {
        "id": "G08TGpjgF-NM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}