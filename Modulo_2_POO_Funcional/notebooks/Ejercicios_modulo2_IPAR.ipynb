{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🤖 Módulo 2: Programación Funcional y Orientada a Objetos en Python\n",
        "## Guía de Ejercicios para Aprendizaje por Refuerzo\n",
        "\n",
        "![Python & RL](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)\n",
        "![OpenAI Gym](https://img.shields.io/badge/OpenAI_Gym-0081A5?style=for-the-badge&logo=OpenAI&logoColor=white)\n",
        "\n",
        "### 📋 Sociedad Argentina de Estadística\n",
        "**Curso:** Introducción a la Programación en Python para Aprendizaje por Refuerzo  \n",
        "**Docente:** Dr. Darío Ezequiel Díaz  \n",
        "**Marzo-Abril 2025**\n",
        "\n",
        "---\n",
        "\n",
        "> Esta guía de ejercicios está diseñada para desarrollar habilidades en programación funcional y orientada a objetos en Python, con énfasis en su aplicación al campo del Aprendizaje por Refuerzo. Los ejercicios avanzan progresivamente en complejidad, desde conceptos básicos hasta implementaciones avanzadas.\n",
        "\n",
        "### 📝 Instrucciones:\n",
        "- Complete cada ejercicio en las celdas correspondientes\n",
        "- Ejecute el código para verificar su funcionamiento\n",
        "- Los ejercicios están diseñados para reforzar conceptos teóricos vistos en clase\n",
        "- Consulte la documentación oficial de Python cuando sea necesario\n",
        "\n",
        "### 📚 Contenidos:\n",
        "- Funciones y parámetros\n",
        "- Funciones Lambda\n",
        "- Programación orientada a objetos\n",
        "- Listas por comprensión\n",
        "- Decoradores\n",
        "- Manejo de excepciones\n",
        "- Patrones de diseño\n",
        "\n",
        "---\n",
        "\n",
        "*\"En el aprendizaje por refuerzo, como en la programación, la exploración y la explotación deben estar en perfecto equilibrio.\"*\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SKPOBBYaGFqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 1: Funciones con Parámetros por Defecto y Nombrados\n",
        "**Dificultad: Básica**\n",
        "\n",
        "**Objetivo**: Practicar la creación de funciones con parámetros opcionales y valores por defecto.\n",
        "\n",
        "**Descripción**: Crea una función llamada `calcular_precio_final` que calcule el precio de un producto después de aplicar impuestos y descuentos.\n",
        "\n",
        "La función debe aceptar:\n",
        "- `precio_base` (obligatorio)\n",
        "- `impuesto` (por defecto 0.21 - equivale a 21%)\n",
        "- `descuento` (por defecto 0 - sin descuento)\n",
        "\n",
        "Primero se debe aplicar el descuento al precio base, y luego calcular el impuesto sobre ese valor con descuento."
      ],
      "metadata": {
        "id": "-UHO1aCg66k5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zQ_m9es6q6F"
      },
      "outputs": [],
      "source": [
        "# Completa la función calcular_precio_final\n",
        "def calcular_precio_final(precio_base, impuesto=0.21, descuento=0):\n",
        "    # Aplica primero el descuento y luego el impuesto\n",
        "    # Tu código aquí\n",
        "\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Pruebas:\n",
        "print(calcular_precio_final(100))  # Debería ser 121.0\n",
        "print(calcular_precio_final(100, descuento=0.1))  # Debería ser 108.9\n",
        "print(calcular_precio_final(100, 0.05, 0.2))  # Debería ser 84.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2: Funciones Lambda para Ordenamiento\n",
        "**Dificultad: Básica**\n",
        "\n",
        "**Objetivo**: Practicar el uso de funciones lambda con la función `sorted()` para ordenar colecciones complejas.\n",
        "\n",
        "**Descripción**: Utiliza funciones lambda con la función `sorted()` para ordenar una lista de tuplas que representan episodios de entrenamiento de un agente.\n",
        "\n",
        "Cada tupla contiene:\n",
        "- (número_episodio, pasos, recompensa_total)\n",
        "\n",
        "Debes ordenar la lista de 3 formas diferentes:\n",
        "1. Por recompensa (de mayor a menor)\n",
        "2. Por número de pasos (de menor a mayor)\n",
        "3. Por eficiencia (recompensa/pasos, de mayor a menor)"
      ],
      "metadata": {
        "id": "Izt9JenHEyyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de episodios: (número_episodio, pasos, recompensa_total)\n",
        "episodios = [\n",
        "    (1, 145, 24),\n",
        "    (2, 97, 31),\n",
        "    (3, 156, 18),\n",
        "    (4, 82, 42),\n",
        "    (5, 113, 37)\n",
        "]\n",
        "\n",
        "# Ordena los episodios por recompensa (mayor a menor)\n",
        "episodios_por_recompensa = None  # Tu código aquí\n",
        "\n",
        "# Ordena los episodios por número de pasos (menor a mayor)\n",
        "episodios_por_pasos = None  # Tu código aquí\n",
        "\n",
        "# Ordena los episodios por eficiencia (recompensa/pasos, mayor a menor)\n",
        "episodios_por_eficiencia = None  # Tu código aquí\n",
        "\n",
        "# Imprime los resultados\n",
        "print(\"Por recompensa:\", episodios_por_recompensa)\n",
        "print(\"Por pasos:\", episodios_por_pasos)\n",
        "print(\"Por eficiencia:\", episodios_por_eficiencia)"
      ],
      "metadata": {
        "id": "PvoHjToXEzG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 3: Clase Básica para Agente de Aprendizaje por Refuerzo\n",
        "**Dificultad: Básica**\n",
        "\n",
        "**Objetivo**: Practicar la creación de clases, atributos y métodos en POO.\n",
        "\n",
        "**Descripción**: Crea una clase `AgenteRL` que represente un agente básico de aprendizaje por refuerzo con:\n",
        "- Atributos para almacenar parámetros y valores Q\n",
        "- Un método para seleccionar acciones con política epsilon-greedy\n",
        "- Un método para aprender de experiencias\n",
        "- Un método `__str__` para visualizar información del agente\n",
        "\n",
        "La política epsilon-greedy consiste en:\n",
        "- Con probabilidad epsilon: seleccionar una acción aleatoria (exploración)\n",
        "- Con probabilidad 1-epsilon: seleccionar la mejor acción conocida (explotación)"
      ],
      "metadata": {
        "id": "mI6kvMDnE39U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Completa la clase AgenteRL\n",
        "class AgenteRL:\n",
        "    def __init__(self, num_acciones, epsilon=0.1):\n",
        "        # Inicializa atributos: tabla de valores Q, epsilon, etc.\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def seleccionar_accion(self, estado):\n",
        "        # Implementa la política epsilon-greedy:\n",
        "        # - Con probabilidad epsilon: acción aleatoria (exploración)\n",
        "        # - Con probabilidad 1-epsilon: mejor acción conocida (explotación)\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado):\n",
        "        # Implementa actualización simple de valor Q\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def __str__(self):\n",
        "        # Representación en string del agente\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Prueba la clase\n",
        "agente = AgenteRL(num_acciones=4)\n",
        "print(agente)\n",
        "\n",
        "# Prueba selección de acciones\n",
        "estado1 = \"s1\"\n",
        "accion1 = agente.seleccionar_accion(estado1)\n",
        "print(f\"Acción seleccionada para {estado1}: {accion1}\")\n",
        "\n",
        "# Prueba aprendizaje\n",
        "agente.aprender(estado1, accion1, 5, \"s2\")\n",
        "print(f\"Después de aprender una vez: {agente}\")"
      ],
      "metadata": {
        "id": "B1UrfIGgE51c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 4: Listas por Comprensión para Transformación de Datos\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Practicar el uso de listas por comprensión para transformar y filtrar datos.\n",
        "\n",
        "**Descripción**: Utiliza listas por comprensión para procesar un conjunto de observaciones de un entorno de aprendizaje por refuerzo.\n",
        "\n",
        "Cada observación es una tupla con formato:\n",
        "- (estado, acción, recompensa, siguiente_estado, terminado)\n",
        "\n",
        "Debes crear:\n",
        "1. Una lista de todas las recompensas\n",
        "2. Una lista de transiciones (estado, acción) que obtuvieron recompensa positiva\n",
        "3. Un diccionario que mapee cada estado a la recompensa promedio obtenida desde él\n",
        "4. Una lista de estados terminales (donde terminado=True)"
      ],
      "metadata": {
        "id": "5ro-XroqE7kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos de observaciones de un entorno - cada tupla contiene:\n",
        "# (estado, acción, recompensa, siguiente_estado, terminado)\n",
        "observaciones = [\n",
        "    (0, 1, 5, 1, False),\n",
        "    (1, 0, -1, 2, False),\n",
        "    (2, 1, 0, 3, False),\n",
        "    (3, 2, 10, 4, True),\n",
        "    (0, 2, 3, 2, False),\n",
        "    (2, 0, 7, 3, False),\n",
        "    (3, 1, -2, 0, False),\n",
        "    (0, 0, 4, 4, True)\n",
        "]\n",
        "\n",
        "# 1. Crea una lista de todas las recompensas\n",
        "recompensas = None  # Tu código aquí\n",
        "\n",
        "# 2. Crea una lista de transiciones (estado, acción) que obtuvieron recompensa positiva\n",
        "transiciones_positivas = None  # Tu código aquí\n",
        "\n",
        "# 3. Crea un diccionario que mapee cada estado a la recompensa promedio obtenida desde él\n",
        "# Pista: Necesitarás contar recompensas y acciones por estado\n",
        "recompensa_por_estado = None  # Tu código aquí\n",
        "\n",
        "# 4. Crea una lista de estados terminales\n",
        "estados_terminales = None  # Tu código aquí\n",
        "\n",
        "# Imprime los resultados\n",
        "print(\"Recompensas:\", recompensas)\n",
        "print(\"Transiciones con recompensa positiva:\", transiciones_positivas)\n",
        "print(\"Recompensa promedio por estado:\", recompensa_por_estado)\n",
        "print(\"Estados terminales:\", estados_terminales)"
      ],
      "metadata": {
        "id": "EljpcKHPE9XU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 5: Argumentos Variables con *args y **kwargs\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Practicar el uso de argumentos variables en funciones.\n",
        "\n",
        "**Descripción**: Crea una función llamada `crear_informe_agente` que genere un informe para agentes de aprendizaje por refuerzo.\n",
        "\n",
        "La función debe aceptar:\n",
        "- `nombre_agente` (obligatorio)\n",
        "- Cualquier número de episodios con sus recompensas (`*args`) - cada episodio es una tupla (num_episodio, recompensa)\n",
        "- Cualquier número de parámetros de configuración (`**kwargs`) - pueden ser alpha, gamma, epsilon, etc.\n",
        "\n",
        "La función debe calcular la recompensa total, promedio, máxima, mínima, y retornar un diccionario con toda esta información junto con los parámetros recibidos."
      ],
      "metadata": {
        "id": "G6H_yrCPE_D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Completa la función crear_informe_agente\n",
        "def crear_informe_agente(nombre_agente, *args, **kwargs):\n",
        "    # args contiene tuplas (episodio, recompensa)\n",
        "    # kwargs contiene parámetros de configuración\n",
        "\n",
        "    # Calcula la recompensa total y promedio\n",
        "    # Tu código aquí\n",
        "\n",
        "    # Genera y retorna un diccionario con el informe\n",
        "    # Tu código aquí\n",
        "\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Pruebas:\n",
        "informe = crear_informe_agente(\"AgentQ\",\n",
        "                             (1, 10), (2, 15), (3, 5),\n",
        "                             alpha=0.1, gamma=0.99, epsilon=0.2)\n",
        "print(informe)\n",
        "# Debería mostrar un diccionario con nombre_agente, episodios, recompensa_total,\n",
        "# recompensa_promedio, recompensa_máxima, recompensa_mínima y un diccionario con los parámetros"
      ],
      "metadata": {
        "id": "9YOS2GB6FAxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 6: Decorador para Medir Tiempo de Ejecución\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Entender y crear decoradores para modificar el comportamiento de funciones.\n",
        "\n",
        "**Descripción**: Crea un decorador `medir_tiempo` que mida y muestre el tiempo de ejecución de una función. El decorador debe:\n",
        "\n",
        "1. Registrar el tiempo antes de llamar a la función\n",
        "2. Ejecutar la función y almacenar su resultado\n",
        "3. Calcular el tiempo transcurrido\n",
        "4. Imprimir información sobre el tiempo de ejecución\n",
        "5. Retornar el resultado original de la función\n",
        "\n",
        "Prueba el decorador con la función recursiva fibonacci, que suele tener tiempos de ejecución crecientes."
      ],
      "metadata": {
        "id": "0C3OHZlAFExU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Completa el decorador medir_tiempo\n",
        "def medir_tiempo(funcion):\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Aplica el decorador a la función fibonacci\n",
        "@medir_tiempo\n",
        "def fibonacci(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci(n-1) + fibonacci(n-2)\n",
        "\n",
        "# Prueba la función decorada\n",
        "resultado = fibonacci(30)\n",
        "print(f\"Resultado: {resultado}\")"
      ],
      "metadata": {
        "id": "z0ri8pktFCoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 7: Procesamiento de Datos con Map/Filter/Reduce\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Practicar programación funcional aplicando funciones de orden superior.\n",
        "\n",
        "**Descripción**: Procesa un conjunto de datos de recompensas obtenidas por un agente utilizando las funciones `map()`, `filter()` y `reduce()`.\n",
        "\n",
        "En este ejercicio debes:\n",
        "1. Usar `map()` para calcular la recompensa total de cada episodio\n",
        "2. Usar `filter()` para encontrar episodios con recompensa total positiva\n",
        "3. Usar `reduce()` para encontrar el episodio con la mayor recompensa\n",
        "\n",
        "Cada episodio contiene una lista de valores de recompensa recibidos."
      ],
      "metadata": {
        "id": "G7M_ft54FHJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "# Datos de recompensas por episodio\n",
        "recompensas = [\n",
        "    {\"episodio\": 1, \"valores\": [5, -1, 0, 10, -2]},\n",
        "    {\"episodio\": 2, \"valores\": [7, 8, -3, 4, 2]},\n",
        "    {\"episodio\": 3, \"valores\": [-4, 2, 0, 9, 1]},\n",
        "    {\"episodio\": 4, \"valores\": [6, -2, 3, -1, 8]},\n",
        "    {\"episodio\": 5, \"valores\": [1, 5, 3, 7, -6]}\n",
        "]\n",
        "\n",
        "# 1. Usa map para calcular la recompensa total de cada episodio\n",
        "# Tu código aquí\n",
        "\n",
        "# 2. Usa filter para encontrar episodios con recompensa total positiva\n",
        "# Tu código aquí\n",
        "\n",
        "# 3. Usa reduce para encontrar el episodio con la mayor recompensa\n",
        "# Tu código aquí\n",
        "\n",
        "# Imprime los resultados\n",
        "print(\"Recompensas totales:\", recompensas_totales)\n",
        "print(\"Episodios positivos:\", episodios_positivos)\n",
        "print(\"Mejor episodio:\", mejor_episodio)"
      ],
      "metadata": {
        "id": "oLYd6b47FI8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 8: Encapsulación con Propiedades y Validación\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Implementar encapsulación y validación de datos utilizando propiedades en Python.\n",
        "\n",
        "**Descripción**: Mejora una clase `AgenteRL` añadiendo propiedades con validación para parámetros críticos.\n",
        "\n",
        "Las propiedades deben incluir:\n",
        "- `epsilon`: Valor entre 0 y 1 (probabilidad de exploración)\n",
        "- `alpha`: Valor entre 0 y 1 (tasa de aprendizaje)\n",
        "- `gamma`: Valor entre 0 y 1 (factor de descuento)\n",
        "\n",
        "Cada propiedad debe incluir validación para asegurar que los valores estén en el rango correcto,\n",
        "lanzando `ValueError` con un mensaje descriptivo cuando se intente asignar un valor inválido."
      ],
      "metadata": {
        "id": "9IlcAU1lFKhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Completa la clase AgenteRL mejorada con propiedades\n",
        "class AgenteRL:\n",
        "    def __init__(self, num_acciones, epsilon=0.1, alpha=0.1, gamma=0.9):\n",
        "        self._num_acciones = num_acciones\n",
        "        self._q_values = {}  # Diccionario para almacenar valores Q\n",
        "\n",
        "        # Usa propiedades para estos parámetros\n",
        "        self._epsilon = None\n",
        "        self._alpha = None\n",
        "        self._gamma = None\n",
        "\n",
        "        # Asigna valores a través de propiedades para validación\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    # Implementa la propiedad epsilon con validación\n",
        "    @property\n",
        "    def epsilon(self):\n",
        "        # Tu código aquí\n",
        "        pass\n",
        "\n",
        "    @epsilon.setter\n",
        "    def epsilon(self, valor):\n",
        "        # Valida que epsilon esté entre 0 y 1\n",
        "        # Tu código aquí\n",
        "        pass\n",
        "\n",
        "    # Implementa propiedades alpha y gamma con validación similar\n",
        "    # Tu código aquí\n",
        "\n",
        "    def seleccionar_accion(self, estado):\n",
        "        \"\"\"Selecciona una acción usando la política epsilon-greedy.\"\"\"\n",
        "        estado_str = str(estado)\n",
        "\n",
        "        # Inicializa valores Q si es necesario\n",
        "        if estado_str not in self._q_values:\n",
        "            self._q_values[estado_str] = [0.0] * self._num_acciones\n",
        "\n",
        "        # Política epsilon-greedy\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self._num_acciones - 1)  # Exploración\n",
        "        else:\n",
        "            return self._q_values[estado_str].index(max(self._q_values[estado_str]))  # Explotación\n",
        "\n",
        "# Prueba la clase con propiedades\n",
        "import random\n",
        "\n",
        "agente = AgenteRL(num_acciones=4, epsilon=0.2)\n",
        "print(f\"Epsilon: {agente.epsilon}\")\n",
        "\n",
        "# Intenta asignar valores válidos e inválidos\n",
        "try:\n",
        "    agente.epsilon = 0.5\n",
        "    print(f\"Nuevo epsilon: {agente.epsilon}\")\n",
        "\n",
        "    agente.epsilon = 1.5  # Debería lanzar ValueError\n",
        "except ValueError as e:\n",
        "    print(f\"Error capturado: {e}\")"
      ],
      "metadata": {
        "id": "UCMnoTaNFMHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 9: Herencia para Diferentes Tipos de Agentes\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Practicar la herencia y la sobreescritura de métodos en POO.\n",
        "\n",
        "**Descripción**: Crea una jerarquía de clases para diferentes tipos de agentes de aprendizaje por refuerzo,\n",
        "heredando de una clase base común llamada `AgenteBase`.\n",
        "\n",
        "Implementa:\n",
        "1. `AgenteBase`: Clase abstracta con métodos `seleccionar_accion` y `aprender`\n",
        "2. `AgenteAleatorio`: Un agente que selecciona acciones aleatorias y no aprende\n",
        "3. `AgenteQLearning`: Un agente que implementa el algoritmo Q-Learning\n",
        "\n",
        "Cada agente debe tener su propia implementación específica de los métodos heredados."
      ],
      "metadata": {
        "id": "Ah8VzQ1bFNpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Completa la jerarquía de clases de agentes\n",
        "\n",
        "# Clase base\n",
        "class AgenteBase:\n",
        "    def __init__(self, num_acciones, nombre=\"AgenteGenérico\"):\n",
        "        self.num_acciones = num_acciones\n",
        "        self.nombre = nombre\n",
        "        self.experiencia_total = 0\n",
        "\n",
        "    def seleccionar_accion(self, estado):\n",
        "        # Método abstracto que las subclases deben implementar\n",
        "        raise NotImplementedError(\"Las subclases deben implementar seleccionar_accion\")\n",
        "\n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado):\n",
        "        # Método abstracto que las subclases deben implementar\n",
        "        raise NotImplementedError(\"Las subclases deben implementar aprender\")\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.nombre} (acciones: {self.num_acciones}, exp: {self.experiencia_total})\"\n",
        "\n",
        "# Completa la clase AgenteAleatorio\n",
        "class AgenteAleatorio(AgenteBase):\n",
        "    # Un agente que selecciona acciones aleatorias\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Completa la clase AgenteQLearning\n",
        "class AgenteQLearning(AgenteBase):\n",
        "    # Un agente que implementa Q-Learning\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Prueba las clases\n",
        "agente_aleatorio = AgenteAleatorio(4, \"Explorador\")\n",
        "agente_q = AgenteQLearning(4, \"Aprendiz\", alpha=0.2, gamma=0.9, epsilon=0.1)\n",
        "\n",
        "print(agente_aleatorio)\n",
        "print(agente_q)\n",
        "\n",
        "# Prueba métodos\n",
        "estado = \"estado_prueba\"\n",
        "accion_aleatoria = agente_aleatorio.seleccionar_accion(estado)\n",
        "accion_q = agente_q.seleccionar_accion(estado)\n",
        "\n",
        "print(f\"Acción aleatoria: {accion_aleatoria}\")\n",
        "print(f\"Acción Q-Learning: {accion_q}\")\n",
        "\n",
        "# Prueba aprendizaje\n",
        "agente_aleatorio.aprender(estado, accion_aleatoria, 1, \"siguiente_estado\")\n",
        "agente_q.aprender(estado, accion_q, 1, \"siguiente_estado\")"
      ],
      "metadata": {
        "id": "SDKDxLdZFPoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 10: Métodos de Instancia, de Clase y Estáticos\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Entender y practicar los diferentes tipos de métodos en clases Python.\n",
        "\n",
        "**Descripción**: Crea una clase `Experimento` que utilice los tres tipos de métodos para\n",
        "gestionar experimentos de aprendizaje por refuerzo:\n",
        "\n",
        "1. **Método de instancia**: `ejecutar()` - Ejecuta el experimento y almacena resultados\n",
        "2. **Método de clase**: `crear_desde_configuracion()` - Crea un experimento a partir de un diccionario de configuración\n",
        "3. **Método estático**: `calcular_metricas()` - Calcula métricas a partir de un histórico de recompensas\n",
        "\n",
        "El método de clase debe usar el decorador `@classmethod` y el método estático debe usar `@staticmethod`."
      ],
      "metadata": {
        "id": "OadYtdLIFRHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Completa la clase Experimento con los tres tipos de métodos\n",
        "class Experimento:\n",
        "    # Atributo de clase\n",
        "    contador_experimentos = 0\n",
        "\n",
        "    def __init__(self, nombre, agente, entorno):\n",
        "        # Inicializa atributos de instancia\n",
        "        self.nombre = nombre\n",
        "        self.agente = agente\n",
        "        self.entorno = entorno\n",
        "        self.fecha_inicio = None\n",
        "        self.fecha_fin = None\n",
        "        self.resultados = None\n",
        "\n",
        "        # Incrementa contador de clase\n",
        "        Experimento.contador_experimentos += 1\n",
        "        self.id = Experimento.contador_experimentos\n",
        "\n",
        "    # Método de instancia\n",
        "    def ejecutar(self, episodios=100, max_pasos=1000):\n",
        "        \"\"\"Ejecuta el experimento y almacena resultados.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    # Método de clase\n",
        "    @classmethod\n",
        "    def crear_desde_configuracion(cls, config_dict):\n",
        "        \"\"\"\n",
        "        Método de clase que crea un experimento a partir de un diccionario de configuración.\n",
        "        \"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    # Método estático\n",
        "    @staticmethod\n",
        "    def calcular_metricas(historico_recompensas):\n",
        "        \"\"\"\n",
        "        Método estático que calcula métricas a partir de un histórico de recompensas.\n",
        "        No depende del estado del objeto.\n",
        "        \"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Prueba los diferentes tipos de métodos\n",
        "# Simula agente y entorno para pruebas\n",
        "class AgenteSimulado:\n",
        "    def seleccionar_accion(self, estado):\n",
        "        return 0\n",
        "\n",
        "class EntornoSimulado:\n",
        "    def reset(self):\n",
        "        return 0\n",
        "\n",
        "    def paso(self, accion):\n",
        "        return 0, random.randint(1, 10), random.random() < 0.1, {}\n",
        "\n",
        "# Prueba método de instancia\n",
        "agente = AgenteSimulado()\n",
        "entorno = EntornoSimulado()\n",
        "experimento = Experimento(\"Prueba1\", agente, entorno)\n",
        "experimento.ejecutar(episodios=5)\n",
        "print(f\"Resultados: {experimento.resultados}\")\n",
        "\n",
        "# Prueba método de clase\n",
        "config = {\n",
        "    \"nombre\": \"ExperimentoConfig\",\n",
        "    \"agente\": agente,\n",
        "    \"entorno\": entorno\n",
        "}\n",
        "experimento2 = Experimento.crear_desde_configuracion(config)\n",
        "print(f\"Experimento desde config: {experimento2.nombre} (ID: {experimento2.id})\")\n",
        "\n",
        "# Prueba método estático\n",
        "historico = [[5, 8, 10], [6, 7, 9, 12], [3, 5, 8]]\n",
        "metricas = Experimento.calcular_metricas(historico)\n",
        "print(f\"Métricas calculadas: {metricas}\")"
      ],
      "metadata": {
        "id": "RSkWXkflFS4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 11: Polimorfismo para Entornos de Entrenamiento\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Implementar polimorfismo con clases de entornos que comparten una interfaz común.\n",
        "\n",
        "**Descripción**: Crea diferentes clases de entornos que comparten una interfaz común, y una función\n",
        "de entrenamiento que funcione con cualquiera de ellos.\n",
        "\n",
        "Implementa:\n",
        "1. `Entorno`: Clase base abstracta con métodos `reset()`, `paso()` y `obtener_num_acciones()`\n",
        "2. `EntornoSimple`: Entorno con estados lineales (0-9) y dos acciones (izquierda/derecha)\n",
        "3. `EntornoRejilla`: Entorno tipo rejilla 2D con 4 acciones (arriba/derecha/abajo/izquierda)\n",
        "4. Función `entrenar()`: Entrena un agente en cualquier entorno que implemente la interfaz\n",
        "\n",
        "El polimorfismo permite que la función de entrenamiento funcione con diferentes tipos de entornos."
      ],
      "metadata": {
        "id": "1r8-aX8jFUZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Completa la jerarquía de clases para entornos\n",
        "\n",
        "# Clase base abstracta\n",
        "class Entorno(ABC):\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        \"\"\"Reinicia el entorno y retorna el estado inicial.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def paso(self, accion):\n",
        "        \"\"\"\n",
        "        Ejecuta una acción y retorna (siguiente_estado, recompensa, terminado, info).\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def obtener_num_acciones(self):\n",
        "        \"\"\"Retorna el número de acciones posibles.\"\"\"\n",
        "        pass\n",
        "\n",
        "# Completa la clase EntornoSimple\n",
        "class EntornoSimple(Entorno):\n",
        "    # Un entorno con estados del 0 al 9 y 2 acciones (izquierda/derecha)\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Completa la clase EntornoRejilla\n",
        "class EntornoRejilla(Entorno):\n",
        "    # Un entorno tipo rejilla 2D con 4 acciones (arriba/derecha/abajo/izquierda)\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Función polimórfica para entrenar un agente en cualquier entorno\n",
        "def entrenar(agente, entorno, episodios=100, max_pasos=1000):\n",
        "    \"\"\"\n",
        "    Entrena un agente en un entorno.\n",
        "    Funciona con cualquier agente y entorno que implementen las interfaces requeridas.\n",
        "    \"\"\"\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Prueba de polimorfismo\n",
        "from random import randint\n",
        "\n",
        "# Creamos un agente aleatorio simple\n",
        "class AgenteAleatorio:\n",
        "    def __init__(self, num_acciones):\n",
        "        self.num_acciones = num_acciones\n",
        "\n",
        "    def seleccionar_accion(self, estado):\n",
        "        return randint(0, self.num_acciones - 1)\n",
        "\n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado):\n",
        "        pass  # Este agente no aprende\n",
        "\n",
        "# Entrenamos al agente en diferentes entornos\n",
        "entorno1 = EntornoSimple()\n",
        "entorno2 = EntornoRejilla(ancho=5, alto=5)\n",
        "\n",
        "agente1 = AgenteAleatorio(entorno1.obtener_num_acciones())\n",
        "agente2 = AgenteAleatorio(entorno2.obtener_num_acciones())\n",
        "\n",
        "resultado1 = entrenar(agente1, entorno1, episodios=10)\n",
        "resultado2 = entrenar(agente2, entorno2, episodios=10)\n",
        "\n",
        "print(\"Resultados en EntornoSimple:\", resultado1)\n",
        "print(\"Resultados en EntornoRejilla:\", resultado2)"
      ],
      "metadata": {
        "id": "Vp6GQ2ZjFXzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 12: Manejo de Excepciones en Entorno de RL\n",
        "**Dificultad: Intermedia**\n",
        "\n",
        "**Objetivo**: Implementar manejo de excepciones robusto para un entorno de aprendizaje por refuerzo.\n",
        "\n",
        "**Descripción**: Crea excepciones personalizadas y un sistema de manejo de errores para un entorno de RL.\n",
        "\n",
        "Debes implementar:\n",
        "1. Excepciones personalizadas:\n",
        "   - `EntornoError`: Clase base para excepciones del entorno\n",
        "   - `AccionInvalidaError`: Cuando se intenta una acción no válida\n",
        "   - `EstadoInvalidoError`: Cuando se intenta acceder a un estado no válido\n",
        "\n",
        "2. Un entorno robusto `EntornoRobusto` con manejo de excepciones para:\n",
        "   - Validar acciones\n",
        "   - Validar estados\n",
        "   - Manejar errores durante la ejecución de acciones\n",
        "\n",
        "Usa bloques try/except en los métodos relevantes y proporciona mensajes de error descriptivos."
      ],
      "metadata": {
        "id": "1OF6DHDKFZjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Excepciones personalizadas para entorno de RL\n",
        "class EntornoError(Exception):\n",
        "    \"\"\"Clase base para excepciones relacionadas con el entorno.\"\"\"\n",
        "    pass\n",
        "\n",
        "class AccionInvalidaError(EntornoError):\n",
        "    \"\"\"Se lanza cuando se intenta realizar una acción no válida.\"\"\"\n",
        "    pass\n",
        "\n",
        "class EstadoInvalidoError(EntornoError):\n",
        "    \"\"\"Se lanza cuando se intenta acceder a un estado no válido.\"\"\"\n",
        "    pass\n",
        "\n",
        "# Completa la clase EntornoRobusto con manejo de excepciones\n",
        "class EntornoRobusto:\n",
        "    def __init__(self, num_estados=10, num_acciones=4):\n",
        "        self.num_estados = num_estados\n",
        "        self.num_acciones = num_acciones\n",
        "        self.estado_actual = 0\n",
        "        self.terminado = False\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reinicia el entorno al estado inicial.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def validar_accion(self, accion):\n",
        "        \"\"\"Valida que una acción sea válida, lanzando excepción si no lo es.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def validar_estado(self, estado):\n",
        "        \"\"\"Valida que un estado sea válido, lanzando excepción si no lo es.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def paso(self, accion):\n",
        "        \"\"\"\n",
        "        Ejecuta una acción y retorna (siguiente_estado, recompensa, terminado, info).\n",
        "        Incluye manejo de excepciones.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Validar acción\n",
        "            # Tu código aquí\n",
        "\n",
        "            # Lógica del entorno\n",
        "            # Tu código aquí\n",
        "\n",
        "            # Retornar resultado\n",
        "            # Tu código aquí\n",
        "            pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "        except AccionInvalidaError as e:\n",
        "            # Manejo del error\n",
        "            # Tu código aquí\n",
        "            pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "        except Exception as e:\n",
        "            # Manejo de otros errores inesperados\n",
        "            # Tu código aquí\n",
        "            pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Prueba el manejo de excepciones\n",
        "entorno = EntornoRobusto(num_estados=5, num_acciones=2)\n",
        "estado = entorno.reset()\n",
        "print(f\"Estado inicial: {estado}\")\n",
        "\n",
        "# Prueba con acción válida\n",
        "try:\n",
        "    resultado = entorno.paso(1)\n",
        "    print(f\"Paso exitoso: {resultado}\")\n",
        "except EntornoError as e:\n",
        "    print(f\"Error controlado: {e}\")\n",
        "\n",
        "# Prueba con acción inválida\n",
        "try:\n",
        "    resultado = entorno.paso(5)  # Acción fuera de rango\n",
        "    print(f\"Paso exitoso: {resultado}\")\n",
        "except EntornoError as e:\n",
        "    print(f\"Error controlado: {e}\")"
      ],
      "metadata": {
        "id": "IPaiOlUWFbks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 13: Decorador Parametrizado para Control de Intentos\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Crear decoradores parametrizados que añadan funcionalidad específica a funciones.\n",
        "\n",
        "**Descripción**: Implementa un decorador `reintentar` que intentará ejecutar una función varias veces si ocurre una excepción.\n",
        "\n",
        "El decorador debe:\n",
        "1. Aceptar parámetros para configurar el comportamiento:\n",
        "   - `max_intentos`: Número máximo de intentos (por defecto 3)\n",
        "   - `excepciones`: Tupla de tipos de excepciones a capturar (por defecto todas)\n",
        "   - `espera`: Tiempo de espera entre intentos (por defecto 0)\n",
        "2. Aplicar la lógica de reintentos solo si ocurre alguna de las excepciones especificadas\n",
        "3. Registrar cada intento fallido y el error\n",
        "4. Tras alcanzar el número máximo de intentos, relanzar la última excepción\n",
        "\n",
        "Este tipo de decorador es útil en entornos donde pueden ocurrir errores transitorios."
      ],
      "metadata": {
        "id": "J8nO5ZdlFdac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Completa el decorador parametrizado reintentar\n",
        "def reintentar(max_intentos=3, excepciones=(Exception,), espera=0):\n",
        "    # Tu código aquí - recuerda que es un decorador con argumentos\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Función que simula un servicio inestable\n",
        "@reintentar(max_intentos=5, excepciones=(ValueError, RuntimeError), espera=0.5)\n",
        "def servicio_inestable(probabilidad_error=0.7):\n",
        "    \"\"\"\n",
        "    Simula un servicio que falla con cierta probabilidad.\n",
        "    \"\"\"\n",
        "    # Simula un servicio que falla con cierta probabilidad\n",
        "    if random.random() < probabilidad_error:\n",
        "        if random.random() < 0.5:\n",
        "            raise ValueError(\"Error de valor simulado\")\n",
        "        else:\n",
        "            raise RuntimeError(\"Error de ejecución simulado\")\n",
        "    return \"¡Operación exitosa!\"\n",
        "\n",
        "# Prueba el servicio con reintento\n",
        "for i in range(3):  # Prueba 3 veces\n",
        "    try:\n",
        "        resultado = servicio_inestable()\n",
        "        print(f\"Intento {i+1}: {resultado}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Intento {i+1}: Falló después de múltiples intentos: {e}\")"
      ],
      "metadata": {
        "id": "4znG41vnFfks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 14: Jerarquía de Excepciones Personalizadas\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Diseñar una jerarquía completa de excepciones personalizadas para un framework de aprendizaje por refuerzo.\n",
        "\n",
        "**Descripción**: Crea una estructura jerárquica de excepciones para diferentes componentes del framework:\n",
        "\n",
        "1. Estructura básica:\n",
        "   - `RLError`: Clase base para todas las excepciones del framework\n",
        "   - `AgentError`: Errores relacionados con agentes\n",
        "   - `EntornoError`: Errores relacionados con entornos\n",
        "   - `PoliticaError`: Errores relacionados con políticas\n",
        "   - `EntrenamientoError`: Errores relacionados con el proceso de entrenamiento\n",
        "\n",
        "2. Excepciones específicas para cada categoría\n",
        "\n",
        "3. Función `entrenar_agente_robusto` que utiliza diferentes tipos de excepciones para mostrar\n",
        "   cómo pueden manejarse de manera diferenciada.\n",
        "\n",
        "Este enfoque permite un manejo de errores más granular y específico."
      ],
      "metadata": {
        "id": "-zl_5Th-FhZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Completa la jerarquía de excepciones para un framework RL\n",
        "\n",
        "# Excepción base para todo el framework\n",
        "class RLError(Exception):\n",
        "    \"\"\"Clase base para todas las excepciones del framework RL.\"\"\"\n",
        "    pass\n",
        "\n",
        "# ---- Completa las excepciones relacionadas con agentes ----\n",
        "class AgentError(RLError):\n",
        "    \"\"\"Error relacionado con agentes.\"\"\"\n",
        "    pass\n",
        "\n",
        "class AgenteFuncionFaltanteError(AgentError):\n",
        "    \"\"\"Error cuando un agente no implementa una función requerida.\"\"\"\n",
        "    def __init__(self, funcion):\n",
        "        self.funcion = funcion\n",
        "        super().__init__(f\"El agente debe implementar la función '{funcion}'\")\n",
        "\n",
        "# ---- Completa las excepciones relacionadas con entornos ----\n",
        "class EntornoError(RLError):\n",
        "    \"\"\"Error relacionado con entornos.\"\"\"\n",
        "    pass\n",
        "\n",
        "class EntornoInterfazError(EntornoError):\n",
        "    \"\"\"Error cuando un entorno no implementa la interfaz requerida.\"\"\"\n",
        "    pass\n",
        "\n",
        "# ---- Completa las excepciones relacionadas con políticas ----\n",
        "class PoliticaError(RLError):\n",
        "    \"\"\"Error relacionado con políticas.\"\"\"\n",
        "    pass\n",
        "\n",
        "# ---- Completa las excepciones relacionadas con entrenamiento ----\n",
        "class EntrenamientoError(RLError):\n",
        "    \"\"\"Error relacionado con entrenamiento.\"\"\"\n",
        "    pass\n",
        "\n",
        "class EpisodioError(EntrenamientoError):\n",
        "    \"\"\"Error durante un episodio de entrenamiento.\"\"\"\n",
        "    pass\n",
        "\n",
        "# Función de ejemplo que utiliza las excepciones\n",
        "def entrenar_agente_robusto(agente, entorno, num_episodios):\n",
        "    \"\"\"\n",
        "    Función de ejemplo que usa el sistema de excepciones para un entrenamiento robusto.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not hasattr(agente, 'seleccionar_accion'):\n",
        "            raise AgenteFuncionFaltanteError(\"seleccionar_accion\")\n",
        "\n",
        "        if not hasattr(entorno, 'reset') or not hasattr(entorno, 'paso'):\n",
        "            raise EntornoInterfazError(\"El entorno debe implementar reset() y paso()\")\n",
        "\n",
        "        resultados = []\n",
        "        for episodio in range(num_episodios):\n",
        "            try:\n",
        "                recompensa_total = ejecutar_episodio(agente, entorno)\n",
        "                resultados.append(recompensa_total)\n",
        "            except EpisodioError as e:\n",
        "                print(f\"Error en episodio {episodio}: {e}\")\n",
        "                # Continúa con el siguiente episodio\n",
        "\n",
        "        return resultados\n",
        "\n",
        "    except AgentError as e:\n",
        "        print(f\"Error crítico en el agente: {e}\")\n",
        "        return None\n",
        "    except EntornoError as e:\n",
        "        print(f\"Error crítico en el entorno: {e}\")\n",
        "        return None\n",
        "    except RLError as e:\n",
        "        print(f\"Error general en el framework: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error inesperado: {e}\")\n",
        "        raise  # Re-lanza excepciones no controladas\n",
        "\n",
        "# Función auxiliar para el ejemplo\n",
        "def ejecutar_episodio(agente, entorno):\n",
        "    \"\"\"Ejecuta un episodio de entrenamiento.\"\"\"\n",
        "    estado = entorno.reset()\n",
        "    recompensa_total = 0\n",
        "    terminado = False\n",
        "\n",
        "    while not terminado:\n",
        "        try:\n",
        "            accion = agente.seleccionar_accion(estado)\n",
        "            siguiente_estado, recompensa, terminado, _ = entorno.paso(accion)\n",
        "            agente.aprender(estado, accion, recompensa, siguiente_estado)\n",
        "            estado = siguiente_estado\n",
        "            recompensa_total += recompensa\n",
        "        except Exception as e:\n",
        "            # Convertimos la excepción a un tipo específico de nuestro framework\n",
        "            raise EpisodioError(f\"Error durante el episodio: {e}\")\n",
        "\n",
        "    return recompensa_total\n",
        "\n",
        "# Prueba con algunas excepciones\n",
        "class AgenteIncompleto:\n",
        "    # Falta el método seleccionar_accion\n",
        "    pass\n",
        "\n",
        "class EntornoIncompleto:\n",
        "    def reset(self):\n",
        "        return 0\n",
        "    # Falta el método paso\n",
        "\n",
        "# Escenarios de prueba\n",
        "try:\n",
        "    entrenar_agente_robusto(AgenteIncompleto(), EntornoIncompleto(), 10)\n",
        "except Exception as e:\n",
        "    print(f\"Error de prueba: {e}\")"
      ],
      "metadata": {
        "id": "zNDWm5-YFjDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 15: Uso Avanzado de Try-Except-Finally\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Implementar un sistema robusto de manejo de errores con try-except-finally para garantizar limpieza de recursos.\n",
        "\n",
        "**Descripción**: Crea un sistema de guardado de modelos para agentes de RL que use bloques `try-except-finally` para garantizar la persistencia de datos incluso en caso de errores.\n",
        "\n",
        "La clase `ModelManager` debe:\n",
        "1. Manejar el guardado seguro de modelos usando archivos temporales\n",
        "2. Garantizar limpieza de recursos incluso en caso de fallo\n",
        "3. Implementar manejo específico para diferentes tipos de errores\n",
        "4. Proporcionar funciones para cargar, listar y eliminar modelos\n",
        "\n",
        "Este enfoque garantiza que no se pierdan datos ni queden recursos sin liberar, incluso cuando ocurren errores."
      ],
      "metadata": {
        "id": "QEDyy0dLFk8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Completa la clase ModelManager para gestión robusta de modelos\n",
        "class ModelManager:\n",
        "    def __init__(self, directorio=\"modelos\"):\n",
        "        self.directorio = directorio\n",
        "        self.archivo_temporal = None\n",
        "\n",
        "        # Crea el directorio si no existe\n",
        "        if not os.path.exists(directorio):\n",
        "            os.makedirs(directorio)\n",
        "\n",
        "    def guardar_modelo(self, agente, nombre_archivo=None):\n",
        "        \"\"\"\n",
        "        Guarda un modelo de agente de forma segura, con protección contra corrupción.\n",
        "        Usa try-except-finally para garantizar limpieza de recursos.\n",
        "        \"\"\"\n",
        "        # Tu código aquí: implementa guardado seguro usando try-except-finally\n",
        "        # Recuerda utilizar un archivo temporal primero, y renombrarlo solo si todo va bien\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def cargar_modelo(self, nombre_archivo):\n",
        "        \"\"\"\n",
        "        Carga un modelo de agente con manejo de errores.\n",
        "        Incluye verificación de integridad básica.\n",
        "        \"\"\"\n",
        "        # Tu código aquí: implementa carga segura usando try-except\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def listar_modelos(self):\n",
        "        \"\"\"Lista todos los modelos guardados en el directorio.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def eliminar_modelo(self, nombre_archivo):\n",
        "        \"\"\"Elimina un modelo con confirmación de éxito.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Agente simple para pruebas\n",
        "class AgenteDemo:\n",
        "    def __init__(self, nombre=\"AgentePrueba\", parametros=None):\n",
        "        self.nombre = nombre\n",
        "        self.parametros = parametros or {\"alpha\": 0.1, \"gamma\": 0.9}\n",
        "        self.q_table = {str(i): [random.random() for _ in range(4)] for i in range(5)}\n",
        "        self.timestamp = time.time()\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Agente '{self.nombre}' (timestamp: {self.timestamp})\"\n",
        "\n",
        "# Prueba el sistema de guardado y carga\n",
        "manager = ModelManager()\n",
        "\n",
        "# Crea y guarda un agente\n",
        "agente_original = AgenteDemo(\"Explorador\")\n",
        "print(f\"Agente original: {agente_original}\")\n",
        "\n",
        "# Guarda el modelo\n",
        "try:\n",
        "    ruta = manager.guardar_modelo(agente_original, \"explorador_v1\")\n",
        "    print(f\"Modelo guardado en: {ruta}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al guardar: {e}\")\n",
        "\n",
        "# Lista modelos disponibles\n",
        "modelos = manager.listar_modelos()\n",
        "print(f\"Modelos disponibles: {modelos}\")\n",
        "\n",
        "# Carga el modelo\n",
        "try:\n",
        "    agente_cargado = manager.cargar_modelo(\"explorador_v1\")\n",
        "    print(f\"Modelo cargado: {agente_cargado}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar: {e}\")\n",
        "\n",
        "# Simula una corrupción o error durante guardado\n",
        "try:\n",
        "    # Forzamos un error durante guardado\n",
        "    class AgenteProblematico(AgenteDemo):\n",
        "        def __getstate__(self):\n",
        "            # Este método se llama durante la serialización con pickle\n",
        "            raise RuntimeError(\"¡Error simulado durante serialización!\")\n",
        "\n",
        "    agente_malo = AgenteProblematico(\"AgenteMalo\")\n",
        "    manager.guardar_modelo(agente_malo, \"no_deberia_existir\")\n",
        "except Exception as e:\n",
        "    print(f\"Error esperado capturado: {e}\")\n",
        "\n",
        "# Verificamos que los archivos temporales se hayan limpiado\n",
        "archivos = os.listdir(manager.directorio)\n",
        "print(f\"Archivos en directorio: {archivos}\")"
      ],
      "metadata": {
        "id": "C5tjmxfjFnHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 16: Patrón Singleton para Configuración de Experimentos\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Implementar el patrón de diseño Singleton para mantener una configuración global.\n",
        "\n",
        "**Descripción**: Crea una clase `ConfigExperimentos` que use el patrón Singleton para mantener una configuración global de experimentos de aprendizaje por refuerzo.\n",
        "\n",
        "La clase debe:\n",
        "1. Implementar el patrón Singleton (solo puede existir una instancia)\n",
        "2. Almacenar parámetros de configuración globales como seed, número de episodios, etc.\n",
        "3. Permitir registrar y recuperar configuraciones de entornos\n",
        "4. Proporcionar método para reiniciar la configuración a valores por defecto\n",
        "\n",
        "Este patrón evita la duplicación de configuraciones y garantiza que todos los componentes\n",
        "del sistema accedan a los mismos parámetros."
      ],
      "metadata": {
        "id": "m_9Nlnz1Fo0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Completa el Singleton para configuración de experimentos\n",
        "\n",
        "class ConfigExperimentos:\n",
        "    \"\"\"\n",
        "    Singleton para gestionar la configuración global de experimentos.\n",
        "    \"\"\"\n",
        "    _instancia = None\n",
        "\n",
        "    def __new__(cls):\n",
        "        # Implementa el patrón Singleton\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def __init__(self):\n",
        "        # Solo se ejecuta una vez para la única instancia\n",
        "        # Valores por defecto\n",
        "        if not hasattr(self, 'inicializado'):\n",
        "            self.seed = 42\n",
        "            self.num_episodios = 1000\n",
        "            self.max_pasos = 500\n",
        "            self.directorio_resultados = \"./resultados\"\n",
        "            self.hiperparametros = {\n",
        "                \"alpha\": 0.1,\n",
        "                \"gamma\": 0.99,\n",
        "                \"epsilon\": 0.1\n",
        "            }\n",
        "            self.entornos_registrados = []\n",
        "            self.inicializado = True\n",
        "\n",
        "    def registrar_entorno(self, nombre_entorno, configuracion):\n",
        "        \"\"\"Registra un nuevo entorno con su configuración.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def obtener_entorno(self, nombre_entorno):\n",
        "        \"\"\"Obtiene la configuración de un entorno registrado.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def reiniciar_configuracion(self):\n",
        "        \"\"\"Reinicia la configuración a valores por defecto.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Representación en texto de la configuración actual.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Prueba el Singleton\n",
        "# 1. Obtenemos la instancia y modificamos valores\n",
        "config1 = ConfigExperimentos()\n",
        "config1.seed = 123\n",
        "config1.hiperparametros[\"alpha\"] = 0.05\n",
        "config1.registrar_entorno(\"CartPole\", {\"version\": \"v1\", \"render_mode\": \"rgb_array\"})\n",
        "\n",
        "# 2. Obtenemos otra \"instancia\" (que debe ser la misma)\n",
        "config2 = ConfigExperimentos()\n",
        "print(f\"¿Son el mismo objeto? {config1 is config2}\")  # Debe ser True\n",
        "print(f\"Seed en config2: {config2.seed}\")  # Debe mostrar 123\n",
        "print(f\"Alpha en config2: {config2.hiperparametros['alpha']}\")  # Debe mostrar 0.05\n",
        "\n",
        "# 3. Verificamos que los entornos registrados son accesibles\n",
        "entorno = config2.obtener_entorno(\"CartPole\")\n",
        "print(f\"Entorno CartPole: {entorno}\")"
      ],
      "metadata": {
        "id": "KHEqsAYyFq9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 17: Patrón Factory para Crear Diferentes Tipos de Entornos\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Implementar el patrón de diseño Factory para crear diferentes tipos de objetos.\n",
        "\n",
        "**Descripción**: Crea una Factory para generar diferentes tipos de entornos de aprendizaje por refuerzo.\n",
        "\n",
        "El sistema debe incluir:\n",
        "1. Clase base abstracta `Entorno` definiendo la interfaz común\n",
        "2. Implementaciones de diferentes tipos de entornos:\n",
        "   - `EntornoDiscreto`: Con espacio de estados discreto y finito\n",
        "   - `EntornoRejilla`: Entorno tipo rejilla 2D\n",
        "   - `EntornoDinamico`: Entorno con dinámica cambiante\n",
        "3. `EntornoFactory`: Factory básica para crear entornos a partir de su tipo\n",
        "4. `RegistroEntornos`: Factory avanzada con registro dinámico de tipos de entornos\n",
        "\n",
        "Este patrón facilita la creación de diferentes tipos de objetos sin exponer la lógica\n",
        "de instanciación al cliente."
      ],
      "metadata": {
        "id": "t3T0xrDHFuQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import random\n",
        "\n",
        "# Completa la jerarquía de clases de entornos y su factory\n",
        "\n",
        "# Clase base abstracta para entornos\n",
        "class Entorno(ABC):\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        \"\"\"Reinicia el entorno y retorna el estado inicial.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def paso(self, accion):\n",
        "        \"\"\"Ejecuta acción y retorna (siguiente_estado, recompensa, terminado, info).\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def obtener_num_acciones(self):\n",
        "        \"\"\"Retorna el número de acciones posibles.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def obtener_nombre(self):\n",
        "        \"\"\"Retorna el nombre del entorno.\"\"\"\n",
        "        pass\n",
        "\n",
        "# Implementa diferentes tipos de entornos\n",
        "\n",
        "class EntornoDiscreto(Entorno):\n",
        "    \"\"\"Entorno con espacio de estados discreto y finito.\"\"\"\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "class EntornoRejilla(Entorno):\n",
        "    \"\"\"Entorno tipo rejilla 2D.\"\"\"\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "class EntornoDinamico(Entorno):\n",
        "    \"\"\"Entorno con dinámica cambiante.\"\"\"\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Implementa la factory de entornos\n",
        "\n",
        "class EntornoFactory:\n",
        "    \"\"\"Factory para crear diferentes tipos de entornos.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def crear_entorno(tipo, **kwargs):\n",
        "        \"\"\"\n",
        "        Crea un entorno del tipo especificado con los parámetros dados.\n",
        "\n",
        "        Args:\n",
        "            tipo: Tipo de entorno (\"discreto\", \"rejilla\", \"dinamico\")\n",
        "            **kwargs: Parámetros específicos para cada tipo de entorno\n",
        "\n",
        "        Returns:\n",
        "            Instancia del entorno solicitado\n",
        "        \"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Factory más avanzada con registro dinámico de entornos\n",
        "\n",
        "class RegistroEntornos:\n",
        "    \"\"\"Factory con registro dinámico de tipos de entornos.\"\"\"\n",
        "\n",
        "    _entornos_registrados = {}\n",
        "\n",
        "    @classmethod\n",
        "    def registrar(cls, nombre, clase_entorno):\n",
        "        \"\"\"Registra un nuevo tipo de entorno.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    @classmethod\n",
        "    def crear_entorno(cls, nombre, **kwargs):\n",
        "        \"\"\"Crea un entorno del tipo registrado.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    @classmethod\n",
        "    def listar_entornos(cls):\n",
        "        \"\"\"Lista todos los tipos de entornos registrados.\"\"\"\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Registramos los entornos disponibles\n",
        "RegistroEntornos.registrar(\"discreto\", EntornoDiscreto)\n",
        "RegistroEntornos.registrar(\"rejilla\", EntornoRejilla)\n",
        "RegistroEntornos.registrar(\"dinamico\", EntornoDinamico)\n",
        "\n",
        "# Prueba la factory simple\n",
        "entorno1 = EntornoFactory.crear_entorno(\"discreto\", num_estados=10, num_acciones=3)\n",
        "entorno2 = EntornoFactory.crear_entorno(\"rejilla\", ancho=5, alto=5)\n",
        "\n",
        "print(f\"Entorno 1: {entorno1.obtener_nombre()}, acciones: {entorno1.obtener_num_acciones()}\")\n",
        "print(f\"Entorno 2: {entorno2.obtener_nombre()}, acciones: {entorno2.obtener_num_acciones()}\")\n",
        "\n",
        "# Prueba el registro dinámico\n",
        "entornos_disponibles = RegistroEntornos.listar_entornos()\n",
        "print(f\"Entornos registrados: {entornos_disponibles}\")\n",
        "\n",
        "entorno3 = RegistroEntornos.crear_entorno(\"dinamico\", dificultad=\"alta\")\n",
        "print(f\"Entorno 3: {entorno3.obtener_nombre()}\")\n",
        "\n",
        "# Intenta crear un tipo no registrado\n",
        "try:\n",
        "    entorno_invalido = RegistroEntornos.crear_entorno(\"inexistente\")\n",
        "except Exception as e:\n",
        "    print(f\"Error esperado: {e}\")"
      ],
      "metadata": {
        "id": "dLWgVSPqFwUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 18: Patrón Strategy para Políticas de Exploración\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Implementar el patrón de diseño Strategy para crear comportamientos intercambiables.\n",
        "\n",
        "**Descripción**: Crea un sistema basado en el patrón Strategy para políticas de exploración intercambiables en un agente de aprendizaje por refuerzo.\n",
        "\n",
        "El sistema debe incluir:\n",
        "1. Interfaz `EstrategiaExploracion` con métodos comunes\n",
        "2. Implementaciones concretas de estrategias:\n",
        "   - `EpsilonGreedy`: Basada en probabilidad epsilon para exploración\n",
        "   - `Softmax`: Usa distribución de probabilidad basada en valores Q\n",
        "   - `UCB`: Balance exploración/explotación basado en Upper Confidence Bound\n",
        "3. Clase `AgenteConEstrategia` que utilice estas estrategias de forma intercambiable\n",
        "\n",
        "Este patrón permite cambiar algoritmos \"en tiempo de ejecución\", ideal para experimentar\n",
        "con diferentes políticas de exploración."
      ],
      "metadata": {
        "id": "3klrN-i5Fx8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Completa la jerarquía de estrategias de exploración\n",
        "\n",
        "# Interfaz para estrategias de exploración\n",
        "class EstrategiaExploracion(ABC):\n",
        "    \"\"\"Interfaz para implementar diferentes estrategias de exploración.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def seleccionar_accion(self, estado, valores_q):\n",
        "        \"\"\"\n",
        "        Selecciona una acción basada en los valores Q y la estrategia.\n",
        "\n",
        "        Args:\n",
        "            estado: El estado actual\n",
        "            valores_q: Lista/array de valores Q para cada acción posible\n",
        "\n",
        "        Returns:\n",
        "            Índice de la acción seleccionada\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def actualizar(self, **kwargs):\n",
        "        \"\"\"Actualiza parámetros internos de la estrategia.\"\"\"\n",
        "        pass\n",
        "\n",
        "# Implementa estrategias concretas\n",
        "\n",
        "class EpsilonGreedy(EstrategiaExploracion):\n",
        "    \"\"\"Estrategia epsilon-greedy con decaimiento opcional.\"\"\"\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "class Softmax(EstrategiaExploracion):\n",
        "    \"\"\"Estrategia Softmax/Boltzmann usando distribución de probabilidad.\"\"\"\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "class UCB(EstrategiaExploracion):\n",
        "    \"\"\"Estrategia Upper Confidence Bound para balance exploración/explotación.\"\"\"\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# Agente flexible que utiliza una estrategia de exploración\n",
        "class AgenteConEstrategia:\n",
        "    \"\"\"Agente de RL que puede cambiar de estrategia de exploración.\"\"\"\n",
        "\n",
        "    def __init__(self, num_acciones, estrategia_exploracion):\n",
        "        self.num_acciones = num_acciones\n",
        "        self.q_table = {}  # Tabla de valores Q: {estado: [valores para cada acción]}\n",
        "        self.estrategia = estrategia_exploracion\n",
        "\n",
        "    def seleccionar_accion(self, estado):\n",
        "        \"\"\"Selecciona una acción usando la estrategia actual.\"\"\"\n",
        "        # Convertimos el estado a string para usarlo como clave\n",
        "        estado_str = str(estado)\n",
        "\n",
        "        # Si es la primera vez que vemos este estado, inicializamos valores\n",
        "        if estado_str not in self.q_table:\n",
        "            self.q_table[estado_str] = [0.0] * self.num_acciones\n",
        "\n",
        "        # Delegamos la selección a la estrategia de exploración\n",
        "        return self.estrategia.seleccionar_accion(estado, self.q_table[estado_str])\n",
        "\n",
        "    def aprender(self, estado, accion, recompensa, siguiente_estado, alpha=0.1, gamma=0.9):\n",
        "        \"\"\"Actualiza tabla Q con aprendizaje básico.\"\"\"\n",
        "        estado_str = str(estado)\n",
        "        sig_estado_str = str(siguiente_estado)\n",
        "\n",
        "        # Inicializamos valores si es necesario\n",
        "        if estado_str not in self.q_table:\n",
        "            self.q_table[estado_str] = [0.0] * self.num_acciones\n",
        "        if sig_estado_str not in self.q_table:\n",
        "            self.q_table[sig_estado_str] = [0.0] * self.num_acciones\n",
        "\n",
        "        # Actualización Q-Learning\n",
        "        q_actual = self.q_table[estado_str][accion]\n",
        "        max_q_siguiente = max(self.q_table[sig_estado_str])\n",
        "\n",
        "        # Fórmula Q-Learning\n",
        "        self.q_table[estado_str][accion] = q_actual + alpha * (\n",
        "            recompensa + gamma * max_q_siguiente - q_actual)\n",
        "\n",
        "    def cambiar_estrategia(self, nueva_estrategia):\n",
        "        \"\"\"Cambia la estrategia de exploración en tiempo de ejecución.\"\"\"\n",
        "        self.estrategia = nueva_estrategia\n",
        "        return self\n",
        "\n",
        "    def actualizar_estrategia(self, **kwargs):\n",
        "        \"\"\"Actualiza parámetros de la estrategia actual.\"\"\"\n",
        "        return self.estrategia.actualizar(**kwargs)\n",
        "\n",
        "# Prueba el patrón Strategy con las diferentes estrategias\n",
        "# Simulamos un estado y valores Q para pruebas\n",
        "estado_prueba = [1, 2, 3]\n",
        "valores_q_prueba = [0.1, 0.5, 0.3, 0.7]\n",
        "\n",
        "# Creamos las estrategias\n",
        "epsilon_greedy = EpsilonGreedy(epsilon=0.3, decaimiento=0.95)\n",
        "softmax = Softmax(temperatura=1.0)\n",
        "ucb = UCB(c=2.0)\n",
        "\n",
        "# Creamos agente con estrategia epsilon-greedy\n",
        "agente = AgenteConEstrategia(num_acciones=4, estrategia_exploracion=epsilon_greedy)\n",
        "\n",
        "# Simulamos algunas selecciones con epsilon-greedy\n",
        "print(\"Estrategia: Epsilon-Greedy\")\n",
        "for i in range(5):\n",
        "    accion = agente.seleccionar_accion(estado_prueba)\n",
        "    print(f\"Selección {i+1}: Acción {accion}\")\n",
        "    agente.actualizar_estrategia()  # Aplica decaimiento\n",
        "\n",
        "# Cambiamos a estrategia Softmax\n",
        "agente.cambiar_estrategia(softmax)\n",
        "print(\"\\nEstrategia: Softmax\")\n",
        "for i in range(5):\n",
        "    accion = agente.seleccionar_accion(estado_prueba)\n",
        "    print(f\"Selección {i+1}: Acción {accion}\")\n",
        "    agente.actualizar_estrategia(temperatura=max(0.1, softmax.temperatura * 0.9))\n",
        "\n",
        "# Cambiamos a estrategia UCB\n",
        "agente.cambiar_estrategia(ucb)\n",
        "print(\"\\nEstrategia: UCB\")\n",
        "for i in range(5):\n",
        "    accion = agente.seleccionar_accion(estado_prueba)\n",
        "    print(f\"Selección {i+1}: Acción {accion}\")"
      ],
      "metadata": {
        "id": "mpYvGDmlFzzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 19: Desarrollo de una Función para Entrenar Agentes de RL\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Integrar diferentes conceptos para crear una función completa de entrenamiento de agentes RL.\n",
        "\n",
        "**Descripción**: Desarrolla una función completa para entrenar agentes de RL integrando varios conceptos vistos en el módulo:\n",
        "- Manejo de excepciones\n",
        "- Parámetros con valores por defecto\n",
        "- Algoritmos de aprendizaje\n",
        "- Control de progreso\n",
        "\n",
        "La función `entrenar_agente` debe:\n",
        "1. Entrenar un agente en un entorno durante múltiples episodios\n",
        "2. Gestionar parámetros como tasas de aprendizaje, factores de descuento, etc.\n",
        "3. Implementar decaimiento de epsilon para exploración\n",
        "4. Registrar y retornar métricas de progreso\n",
        "5. Incluir manejo de excepciones para errores durante el entrenamiento\n",
        "6. Proporcionar evaluaciones periódicas para medir el progreso real\n",
        "\n",
        "Este tipo de función es el corazón de cualquier sistema de aprendizaje por refuerzo."
      ],
      "metadata": {
        "id": "IHCFsqSyF1Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def entrenar_agente(agente, entorno, num_episodios=500, max_pasos=1000,\n",
        "                   gamma=0.99, alpha=0.1, epsilon_inicial=1.0, epsilon_final=0.01,\n",
        "                   decaimiento_epsilon=0.995, intervalo_eval=10, mostrar_progreso=True):\n",
        "    \"\"\"\n",
        "    Función completa para entrenar agentes de aprendizaje por refuerzo.\n",
        "\n",
        "    Args:\n",
        "        agente: Objeto agente con métodos seleccionar_accion y aprender\n",
        "        entorno: Objeto entorno con métodos reset y paso\n",
        "        num_episodios: Número de episodios de entrenamiento\n",
        "        max_pasos: Número máximo de pasos por episodio\n",
        "        gamma: Factor de descuento para el aprendizaje\n",
        "        alpha: Tasa de aprendizaje\n",
        "        epsilon_inicial: Valor inicial de epsilon para exploración\n",
        "        epsilon_final: Valor mínimo de epsilon\n",
        "        decaimiento_epsilon: Factor de decaimiento de epsilon\n",
        "        intervalo_eval: Cada cuántos episodios evaluar sin exploración\n",
        "        mostrar_progreso: Si mostrar información durante el entrenamiento\n",
        "\n",
        "    Returns:\n",
        "        dict: Diccionario con resultados del entrenamiento\n",
        "    \"\"\"\n",
        "    # Estructuras para almacenar resultados\n",
        "    todas_recompensas = []\n",
        "    recompensas_evaluacion = []\n",
        "    epsilon = epsilon_inicial\n",
        "    tiempo_inicio = time.time()\n",
        "\n",
        "    # Ventana deslizante para promediar recompensas recientes\n",
        "    ventana_recompensas = deque(maxlen=50)\n",
        "\n",
        "    # Función para ejecutar un episodio de evaluación (sin exploración)\n",
        "    def ejecutar_evaluacion():\n",
        "        estado = entorno.reset()\n",
        "        recompensa_total = 0\n",
        "        terminado = False\n",
        "\n",
        "        for paso in range(max_pasos):\n",
        "            # Durante evaluación, siempre seleccionamos la mejor acción\n",
        "            # Tu código aquí\n",
        "            pass\n",
        "\n",
        "            if terminado:\n",
        "                break\n",
        "\n",
        "        return recompensa_total\n",
        "\n",
        "    # Bucle principal de entrenamiento\n",
        "    try:\n",
        "        for episodio in range(1, num_episodios + 1):\n",
        "            # Reiniciamos el entorno\n",
        "            estado = entorno.reset()\n",
        "            recompensa_episodio = 0\n",
        "            pasos_episodio = 0\n",
        "            terminado = False\n",
        "\n",
        "            # Bucle del episodio\n",
        "            for paso in range(max_pasos):\n",
        "                # Seleccionamos acción según la política actual (con exploración)\n",
        "                # Tu código aquí\n",
        "\n",
        "                # Ejecutamos la acción en el entorno\n",
        "                # Tu código aquí\n",
        "\n",
        "                # El agente aprende de la experiencia\n",
        "                # Tu código aquí\n",
        "\n",
        "                # Actualizamos estado y contadores\n",
        "                # Tu código aquí\n",
        "\n",
        "                if terminado:\n",
        "                    break\n",
        "\n",
        "            # Guardamos estadísticas del episodio\n",
        "            # Tu código aquí\n",
        "\n",
        "            # Actualizamos épsilon con decaimiento\n",
        "            # Tu código aquí\n",
        "\n",
        "            # Evaluación periódica sin exploración\n",
        "            if episodio % intervalo_eval == 0:\n",
        "                # Tu código aquí\n",
        "                pass\n",
        "\n",
        "            # Mostramos progreso\n",
        "            if mostrar_progreso and (episodio % 10 == 0 or episodio == 1):\n",
        "                # Tu código aquí\n",
        "                pass\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nEntrenamiento interrumpido por el usuario.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError durante el entrenamiento: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Calculamos estadísticas finales\n",
        "    tiempo_total = time.time() - tiempo_inicio\n",
        "\n",
        "    # Resultados finales\n",
        "    resultados = {\n",
        "        \"tiempo_total\": tiempo_total,\n",
        "        \"num_episodios\": episodio,\n",
        "        \"recompensas\": todas_recompensas,\n",
        "        \"recompensas_eval\": recompensas_evaluacion,\n",
        "        \"epsilon_final\": epsilon,\n",
        "        \"recompensa_mejor\": max(ventana_recompensas) if ventana_recompensas else 0,\n",
        "        \"recompensa_promedio\": np.mean(ventana_recompensas) if ventana_recompensas else 0\n",
        "    }\n",
        "\n",
        "    if mostrar_progreso:\n",
        "        print(f\"\\nEntrenamiento completado en {tiempo_total:.2f} segundos\")\n",
        "        print(f\"Recompensa promedio final: {resultados['recompensa_promedio']:.2f}\")\n",
        "\n",
        "    return resultados\n",
        "\n",
        "# Ejemplo de uso\n",
        "# Estas clases son solo para la demostración, deberían implementarse completamente\n",
        "class AgenteDemo:\n",
        "    def seleccionar_accion(self, estado, epsilon=0):\n",
        "        # Implementación simplificada para demo\n",
        "        return random.randint(0, 1)\n",
        "\n",
        "    def aprender(self, estado, accion, recompensa, sig_estado, alpha, gamma):\n",
        "        # Implementación simplificada para demo\n",
        "        pass\n",
        "\n",
        "class EntornoDemo:\n",
        "    def reset(self):\n",
        "        return [0, 0]\n",
        "\n",
        "    def paso(self, accion):\n",
        "        # Simplificado para demostración\n",
        "        recompensa = random.random()\n",
        "        sig_estado = [random.random(), random.random()]\n",
        "        terminado = random.random() < 0.1\n",
        "        return sig_estado, recompensa, terminado, {}\n",
        "\n",
        "# Ejecutamos entrenamiento de demostración\n",
        "agente_demo = AgenteDemo()\n",
        "entorno_demo = EntornoDemo()\n",
        "\n",
        "# Parámetros simplificados para la demo\n",
        "resultados = entrenar_agente(\n",
        "    agente_demo,\n",
        "    entorno_demo,\n",
        "    num_episodios=50,  # Número reducido para demostración\n",
        "    max_pasos=50,      # Pasos reducidos para demostración\n",
        "    intervalo_eval=10,\n",
        "    mostrar_progreso=True\n",
        ")\n",
        "\n",
        "# Visualizamos resultados\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(resultados[\"recompensas\"])\n",
        "plt.xlabel(\"Episodios\")\n",
        "plt.ylabel(\"Recompensa total\")\n",
        "plt.title(\"Curva de aprendizaje\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qmMCKGYFF3dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 20: Implementación Completa de un Sistema de Agentes y Entornos\n",
        "**Dificultad: Avanzada**\n",
        "\n",
        "**Objetivo**: Integrar todos los conceptos aprendidos en un sistema completo de aprendizaje por refuerzo.\n",
        "\n",
        "**Descripción**: Crea un sistema completo que combine excepciones personalizadas, patrones de diseño,\n",
        "y programación orientada a objetos para implementar un framework de aprendizaje por refuerzo.\n",
        "\n",
        "El sistema debe incluir:\n",
        "1. Jerarquía de excepciones para diferentes componentes\n",
        "2. Estrategias de exploración usando el patrón Strategy\n",
        "3. Clases de entorno y agente con interfaces bien definidas\n",
        "4. Funciones de entrenamiento y evaluación\n",
        "5. Visualización de resultados\n",
        "\n",
        "Este ejercicio final integra todos los conceptos del módulo en un solo sistema coherente."
      ],
      "metadata": {
        "id": "81NOxf_-F4_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# ------------- Excepciones Personalizadas -------------\n",
        "class RLError(Exception):\n",
        "    \"\"\"Error base para el framework de RL.\"\"\"\n",
        "    pass\n",
        "\n",
        "class AgentError(RLError):\n",
        "    \"\"\"Error relacionado con agentes.\"\"\"\n",
        "    pass\n",
        "\n",
        "class EnvironmentError(RLError):\n",
        "    \"\"\"Error relacionado con entornos.\"\"\"\n",
        "    pass\n",
        "\n",
        "# ------------- Estrategias de Exploración (Patrón Strategy) -------------\n",
        "class ExplorationStrategy(ABC):\n",
        "    @abstractmethod\n",
        "    def select_action(self, state, q_values):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def update(self, **kwargs):\n",
        "        pass\n",
        "\n",
        "class EpsilonGreedy(ExplorationStrategy):\n",
        "    def __init__(self, epsilon=0.1, decay=0.995, epsilon_min=0.01):\n",
        "        self.epsilon = epsilon\n",
        "        self.decay = decay\n",
        "        self.epsilon_min = epsilon_min\n",
        "\n",
        "    def select_action(self, state, q_values):\n",
        "        # Implementa la estrategia epsilon-greedy\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        # Actualiza epsilon con decaimiento\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# ------------- Interfaces Base -------------\n",
        "class Environment(ABC):\n",
        "    @abstractmethod\n",
        "    def reset(self):\n",
        "        \"\"\"Reinicia el entorno y retorna estado inicial.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step(self, action):\n",
        "        \"\"\"Ejecuta acción y retorna (next_state, reward, done, info).\"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def action_space(self):\n",
        "        \"\"\"Retorna el espacio de acciones.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def observation_space(self):\n",
        "        \"\"\"Retorna el espacio de observaciones.\"\"\"\n",
        "        pass\n",
        "\n",
        "class Agent(ABC):\n",
        "    @abstractmethod\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Selecciona acción basada en estado actual.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Actualiza conocimiento basado en experiencia.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def save(self, path):\n",
        "        \"\"\"Guarda el modelo del agente.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def load(self, path):\n",
        "        \"\"\"Carga el modelo del agente.\"\"\"\n",
        "        pass\n",
        "\n",
        "# ------------- Implementaciones Concretas -------------\n",
        "\n",
        "# GridWorld: un entorno de rejilla 2D simple\n",
        "class GridWorld(Environment):\n",
        "    def __init__(self, width=5, height=5, max_steps=100):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.max_steps = max_steps\n",
        "        self.target_position = (height - 1, width - 1)  # Esquina inferior derecha\n",
        "        self.agent_position = None\n",
        "        self.steps = 0\n",
        "        self._action_space = 4  # Arriba, Derecha, Abajo, Izquierda\n",
        "        self._observation_space = width * height\n",
        "\n",
        "    def reset(self):\n",
        "        # Reinicia entorno y retorna estado inicial\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def step(self, action):\n",
        "        # Ejecuta acción y retorna resultado\n",
        "        # Tu código aquí: mueve al agente según la acción\n",
        "        # y calcula recompensa y estado terminal\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    @property\n",
        "    def action_space(self):\n",
        "        return self._action_space\n",
        "\n",
        "    @property\n",
        "    def observation_space(self):\n",
        "        return self._observation_space\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"Convierte posición 2D a un índice de estado.\"\"\"\n",
        "        x, y = self.agent_position\n",
        "        return x * self.width + y\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"Visualiza el entorno en consola.\"\"\"\n",
        "        # Tu código aquí: muestra la rejilla con agente y objetivo\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# QLearningAgent: agente que implementa el algoritmo Q-Learning\n",
        "class QLearningAgent(Agent):\n",
        "    def __init__(self, action_space, observation_space, **kwargs):\n",
        "        self.action_space = action_space\n",
        "        self.observation_space = observation_space\n",
        "        self.q_table = {}\n",
        "\n",
        "        # Hiperparámetros\n",
        "        self.alpha = kwargs.get('alpha', 0.1)  # Tasa de aprendizaje\n",
        "        self.gamma = kwargs.get('gamma', 0.99)  # Factor de descuento\n",
        "\n",
        "        # Estrategia de exploración (patrón Strategy)\n",
        "        self.exploration = kwargs.get('exploration',\n",
        "                                     EpsilonGreedy(epsilon=kwargs.get('epsilon', 0.1)))\n",
        "\n",
        "    def select_action(self, state):\n",
        "        # Selecciona acción usando estrategia de exploración\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        # Actualiza tabla Q según algoritmo Q-Learning\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def save(self, path):\n",
        "        # Guarda la tabla Q en un archivo\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "    def load(self, path):\n",
        "        # Carga la tabla Q desde un archivo\n",
        "        # Tu código aquí\n",
        "        pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# ------------- Funciones de Entrenamiento -------------\n",
        "def train_agent(agent, env, episodes=1000, max_steps=100,\n",
        "               verbose=True, eval_interval=100):\n",
        "    \"\"\"\n",
        "    Entrena un agente en un entorno.\n",
        "\n",
        "    Args:\n",
        "        agent: Instancia de Agent\n",
        "        env: Instancia de Environment\n",
        "        episodes: Número de episodios\n",
        "        max_steps: Pasos máximos por episodio\n",
        "        verbose: Si mostrar progreso\n",
        "        eval_interval: Intervalo para evaluación sin exploración\n",
        "\n",
        "    Returns:\n",
        "        dict: Resultados del entrenamiento\n",
        "    \"\"\"\n",
        "    # Implementa la función de entrenamiento completa\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "def evaluate_agent(agent, env, episodes=10, max_steps=100):\n",
        "    \"\"\"\n",
        "    Evalúa un agente sin exploración.\n",
        "\n",
        "    Args:\n",
        "        agent: Instancia de Agent\n",
        "        env: Instancia de Environment\n",
        "        episodes: Número de episodios\n",
        "        max_steps: Pasos máximos por episodio\n",
        "\n",
        "    Returns:\n",
        "        float: Recompensa promedio\n",
        "    \"\"\"\n",
        "    # Implementa la función de evaluación\n",
        "    # Tu código aquí\n",
        "    pass  # Elimina esta línea al implementar tu solución\n",
        "\n",
        "# ------------- Función Principal -------------\n",
        "def main():\n",
        "    # Creación de entorno y agente\n",
        "    env = GridWorld(width=5, height=5)\n",
        "\n",
        "    # Estrategia de exploración\n",
        "    exploration = EpsilonGreedy(epsilon=1.0, decay=0.995, epsilon_min=0.01)\n",
        "\n",
        "    # Agente\n",
        "    agent = QLearningAgent(\n",
        "        action_space=env.action_space,\n",
        "        observation_space=env.observation_space,\n",
        "        alpha=0.1,\n",
        "        gamma=0.99,\n",
        "        exploration=exploration\n",
        "    )\n",
        "\n",
        "    # Entrenamiento\n",
        "    results = train_agent(agent, env, episodes=500, max_steps=100)\n",
        "\n",
        "    # Visualización de resultados\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(results['rewards'])\n",
        "    plt.xlabel('Episodio')\n",
        "    plt.ylabel('Recompensa total')\n",
        "    plt.title('Curva de aprendizaje')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(results['eval_rewards'])\n",
        "    plt.xlabel('Evaluación')\n",
        "    plt.ylabel('Recompensa promedio')\n",
        "    plt.title('Desempeño en evaluación')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Demostración final\n",
        "    print(\"\\n--- Demostración Final ---\")\n",
        "    for episode in range(3):\n",
        "        state = env.reset()\n",
        "        env.render()\n",
        "\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            env.render()\n",
        "            time.sleep(0.3)  # Pausa para visualización\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        print(f\"Episodio {episode+1}: Recompensa total = {total_reward}\")\n",
        "\n",
        "# Ejecución del programa principal\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la ejecución: {e}\")"
      ],
      "metadata": {
        "id": "ZJgGiEnJF7cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"FIN\""
      ],
      "metadata": {
        "id": "G08TGpjgF-NM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}