# -*- coding: utf-8 -*-
"""Ejercicios_modulo2_IPAR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bhmCrsN4FZfCOGKeiGrHC0k6AAomgsMr

#  M贸dulo 2: Programaci贸n Funcional y Orientada a Objetos en Python
## Gu铆a de Ejercicios para Aprendizaje por Refuerzo

![Python & RL](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![OpenAI Gym](https://img.shields.io/badge/OpenAI_Gym-0081A5?style=for-the-badge&logo=OpenAI&logoColor=white)

###  Sociedad Argentina de Estad铆stica
**Curso:** Introducci贸n a la Programaci贸n en Python para Aprendizaje por Refuerzo  
**Docente:** Dr. Dar铆o Ezequiel D铆az  
**Marzo-Abril 2025**

---

> Esta gu铆a de ejercicios est谩 dise帽ada para desarrollar habilidades en programaci贸n funcional y orientada a objetos en Python, con 茅nfasis en su aplicaci贸n al campo del Aprendizaje por Refuerzo. Los ejercicios avanzan progresivamente en complejidad, desde conceptos b谩sicos hasta implementaciones avanzadas.

###  Instrucciones:
- Complete cada ejercicio en las celdas correspondientes
- Ejecute el c贸digo para verificar su funcionamiento
- Los ejercicios est谩n dise帽ados para reforzar conceptos te贸ricos vistos en clase
- Consulte la documentaci贸n oficial de Python cuando sea necesario

###  Contenidos:
- Funciones y par谩metros
- Funciones Lambda
- Programaci贸n orientada a objetos
- Listas por comprensi贸n
- Decoradores
- Manejo de excepciones
- Patrones de dise帽o

---

*"En el aprendizaje por refuerzo, como en la programaci贸n, la exploraci贸n y la explotaci贸n deben estar en perfecto equilibrio."*

---

# Ejercicio 1: Funciones con Par谩metros por Defecto y Nombrados
**Dificultad: B谩sica**

**Objetivo**: Practicar la creaci贸n de funciones con par谩metros opcionales y valores por defecto.

**Descripci贸n**: Crea una funci贸n llamada `calcular_precio_final` que calcule el precio de un producto despu茅s de aplicar impuestos y descuentos.

La funci贸n debe aceptar:
- `precio_base` (obligatorio)
- `impuesto` (por defecto 0.21 - equivale a 21%)
- `descuento` (por defecto 0 - sin descuento)

Primero se debe aplicar el descuento al precio base, y luego calcular el impuesto sobre ese valor con descuento.
"""

# Completa la funci贸n calcular_precio_final
def calcular_precio_final(precio_base, impuesto=0.21, descuento=0):
    # Aplica primero el descuento y luego el impuesto
    # Tu c贸digo aqu铆

    pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Pruebas:
print(calcular_precio_final(100))  # Deber铆a ser 121.0
print(calcular_precio_final(100, descuento=0.1))  # Deber铆a ser 108.9
print(calcular_precio_final(100, 0.05, 0.2))  # Deber铆a ser 84.0

"""# Ejercicio 2: Funciones Lambda para Ordenamiento
**Dificultad: B谩sica**

**Objetivo**: Practicar el uso de funciones lambda con la funci贸n `sorted()` para ordenar colecciones complejas.

**Descripci贸n**: Utiliza funciones lambda con la funci贸n `sorted()` para ordenar una lista de tuplas que representan episodios de entrenamiento de un agente.

Cada tupla contiene:
- (n煤mero_episodio, pasos, recompensa_total)

Debes ordenar la lista de 3 formas diferentes:
1. Por recompensa (de mayor a menor)
2. Por n煤mero de pasos (de menor a mayor)
3. Por eficiencia (recompensa/pasos, de mayor a menor)
"""

# Lista de episodios: (n煤mero_episodio, pasos, recompensa_total)
episodios = [
    (1, 145, 24),
    (2, 97, 31),
    (3, 156, 18),
    (4, 82, 42),
    (5, 113, 37)
]

# Ordena los episodios por recompensa (mayor a menor)
episodios_por_recompensa = None  # Tu c贸digo aqu铆

# Ordena los episodios por n煤mero de pasos (menor a mayor)
episodios_por_pasos = None  # Tu c贸digo aqu铆

# Ordena los episodios por eficiencia (recompensa/pasos, mayor a menor)
episodios_por_eficiencia = None  # Tu c贸digo aqu铆

# Imprime los resultados
print("Por recompensa:", episodios_por_recompensa)
print("Por pasos:", episodios_por_pasos)
print("Por eficiencia:", episodios_por_eficiencia)

"""# Ejercicio 3: Clase B谩sica para Agente de Aprendizaje por Refuerzo
**Dificultad: B谩sica**

**Objetivo**: Practicar la creaci贸n de clases, atributos y m茅todos en POO.

**Descripci贸n**: Crea una clase `AgenteRL` que represente un agente b谩sico de aprendizaje por refuerzo con:
- Atributos para almacenar par谩metros y valores Q
- Un m茅todo para seleccionar acciones con pol铆tica epsilon-greedy
- Un m茅todo para aprender de experiencias
- Un m茅todo `__str__` para visualizar informaci贸n del agente

La pol铆tica epsilon-greedy consiste en:
- Con probabilidad epsilon: seleccionar una acci贸n aleatoria (exploraci贸n)
- Con probabilidad 1-epsilon: seleccionar la mejor acci贸n conocida (explotaci贸n)
"""

import random

# Completa la clase AgenteRL
class AgenteRL:
    def __init__(self, num_acciones, epsilon=0.1):
        # Inicializa atributos: tabla de valores Q, epsilon, etc.
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def seleccionar_accion(self, estado):
        # Implementa la pol铆tica epsilon-greedy:
        # - Con probabilidad epsilon: acci贸n aleatoria (exploraci贸n)
        # - Con probabilidad 1-epsilon: mejor acci贸n conocida (explotaci贸n)
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def aprender(self, estado, accion, recompensa, siguiente_estado):
        # Implementa actualizaci贸n simple de valor Q
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def __str__(self):
        # Representaci贸n en string del agente
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Prueba la clase
agente = AgenteRL(num_acciones=4)
print(agente)

# Prueba selecci贸n de acciones
estado1 = "s1"
accion1 = agente.seleccionar_accion(estado1)
print(f"Acci贸n seleccionada para {estado1}: {accion1}")

# Prueba aprendizaje
agente.aprender(estado1, accion1, 5, "s2")
print(f"Despu茅s de aprender una vez: {agente}")

"""# Ejercicio 4: Listas por Comprensi贸n para Transformaci贸n de Datos
**Dificultad: Intermedia**

**Objetivo**: Practicar el uso de listas por comprensi贸n para transformar y filtrar datos.

**Descripci贸n**: Utiliza listas por comprensi贸n para procesar un conjunto de observaciones de un entorno de aprendizaje por refuerzo.

Cada observaci贸n es una tupla con formato:
- (estado, acci贸n, recompensa, siguiente_estado, terminado)

Debes crear:
1. Una lista de todas las recompensas
2. Una lista de transiciones (estado, acci贸n) que obtuvieron recompensa positiva
3. Un diccionario que mapee cada estado a la recompensa promedio obtenida desde 茅l
4. Una lista de estados terminales (donde terminado=True)
"""

# Datos de observaciones de un entorno - cada tupla contiene:
# (estado, acci贸n, recompensa, siguiente_estado, terminado)
observaciones = [
    (0, 1, 5, 1, False),
    (1, 0, -1, 2, False),
    (2, 1, 0, 3, False),
    (3, 2, 10, 4, True),
    (0, 2, 3, 2, False),
    (2, 0, 7, 3, False),
    (3, 1, -2, 0, False),
    (0, 0, 4, 4, True)
]

# 1. Crea una lista de todas las recompensas
recompensas = None  # Tu c贸digo aqu铆

# 2. Crea una lista de transiciones (estado, acci贸n) que obtuvieron recompensa positiva
transiciones_positivas = None  # Tu c贸digo aqu铆

# 3. Crea un diccionario que mapee cada estado a la recompensa promedio obtenida desde 茅l
# Pista: Necesitar谩s contar recompensas y acciones por estado
recompensa_por_estado = None  # Tu c贸digo aqu铆

# 4. Crea una lista de estados terminales
estados_terminales = None  # Tu c贸digo aqu铆

# Imprime los resultados
print("Recompensas:", recompensas)
print("Transiciones con recompensa positiva:", transiciones_positivas)
print("Recompensa promedio por estado:", recompensa_por_estado)
print("Estados terminales:", estados_terminales)

"""# Ejercicio 5: Argumentos Variables con *args y **kwargs
**Dificultad: Intermedia**

**Objetivo**: Practicar el uso de argumentos variables en funciones.

**Descripci贸n**: Crea una funci贸n llamada `crear_informe_agente` que genere un informe para agentes de aprendizaje por refuerzo.

La funci贸n debe aceptar:
- `nombre_agente` (obligatorio)
- Cualquier n煤mero de episodios con sus recompensas (`*args`) - cada episodio es una tupla (num_episodio, recompensa)
- Cualquier n煤mero de par谩metros de configuraci贸n (`**kwargs`) - pueden ser alpha, gamma, epsilon, etc.

La funci贸n debe calcular la recompensa total, promedio, m谩xima, m铆nima, y retornar un diccionario con toda esta informaci贸n junto con los par谩metros recibidos.
"""

# Completa la funci贸n crear_informe_agente
def crear_informe_agente(nombre_agente, *args, **kwargs):
    # args contiene tuplas (episodio, recompensa)
    # kwargs contiene par谩metros de configuraci贸n

    # Calcula la recompensa total y promedio
    # Tu c贸digo aqu铆

    # Genera y retorna un diccionario con el informe
    # Tu c贸digo aqu铆

    pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Pruebas:
informe = crear_informe_agente("AgentQ",
                             (1, 10), (2, 15), (3, 5),
                             alpha=0.1, gamma=0.99, epsilon=0.2)
print(informe)
# Deber铆a mostrar un diccionario con nombre_agente, episodios, recompensa_total,
# recompensa_promedio, recompensa_m谩xima, recompensa_m铆nima y un diccionario con los par谩metros

"""# Ejercicio 6: Decorador para Medir Tiempo de Ejecuci贸n
**Dificultad: Intermedia**

**Objetivo**: Entender y crear decoradores para modificar el comportamiento de funciones.

**Descripci贸n**: Crea un decorador `medir_tiempo` que mida y muestre el tiempo de ejecuci贸n de una funci贸n. El decorador debe:

1. Registrar el tiempo antes de llamar a la funci贸n
2. Ejecutar la funci贸n y almacenar su resultado
3. Calcular el tiempo transcurrido
4. Imprimir informaci贸n sobre el tiempo de ejecuci贸n
5. Retornar el resultado original de la funci贸n

Prueba el decorador con la funci贸n recursiva fibonacci, que suele tener tiempos de ejecuci贸n crecientes.
"""

import time

# Completa el decorador medir_tiempo
def medir_tiempo(funcion):
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Aplica el decorador a la funci贸n fibonacci
@medir_tiempo
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Prueba la funci贸n decorada
resultado = fibonacci(30)
print(f"Resultado: {resultado}")

"""# Ejercicio 7: Procesamiento de Datos con Map/Filter/Reduce
**Dificultad: Intermedia**

**Objetivo**: Practicar programaci贸n funcional aplicando funciones de orden superior.

**Descripci贸n**: Procesa un conjunto de datos de recompensas obtenidas por un agente utilizando las funciones `map()`, `filter()` y `reduce()`.

En este ejercicio debes:
1. Usar `map()` para calcular la recompensa total de cada episodio
2. Usar `filter()` para encontrar episodios con recompensa total positiva
3. Usar `reduce()` para encontrar el episodio con la mayor recompensa

Cada episodio contiene una lista de valores de recompensa recibidos.
"""

from functools import reduce

# Datos de recompensas por episodio
recompensas = [
    {"episodio": 1, "valores": [5, -1, 0, 10, -2]},
    {"episodio": 2, "valores": [7, 8, -3, 4, 2]},
    {"episodio": 3, "valores": [-4, 2, 0, 9, 1]},
    {"episodio": 4, "valores": [6, -2, 3, -1, 8]},
    {"episodio": 5, "valores": [1, 5, 3, 7, -6]}
]

# 1. Usa map para calcular la recompensa total de cada episodio
# Tu c贸digo aqu铆

# 2. Usa filter para encontrar episodios con recompensa total positiva
# Tu c贸digo aqu铆

# 3. Usa reduce para encontrar el episodio con la mayor recompensa
# Tu c贸digo aqu铆

# Imprime los resultados
print("Recompensas totales:", recompensas_totales)
print("Episodios positivos:", episodios_positivos)
print("Mejor episodio:", mejor_episodio)

"""# Ejercicio 8: Encapsulaci贸n con Propiedades y Validaci贸n
**Dificultad: Intermedia**

**Objetivo**: Implementar encapsulaci贸n y validaci贸n de datos utilizando propiedades en Python.

**Descripci贸n**: Mejora una clase `AgenteRL` a帽adiendo propiedades con validaci贸n para par谩metros cr铆ticos.

Las propiedades deben incluir:
- `epsilon`: Valor entre 0 y 1 (probabilidad de exploraci贸n)
- `alpha`: Valor entre 0 y 1 (tasa de aprendizaje)
- `gamma`: Valor entre 0 y 1 (factor de descuento)

Cada propiedad debe incluir validaci贸n para asegurar que los valores est茅n en el rango correcto,
lanzando `ValueError` con un mensaje descriptivo cuando se intente asignar un valor inv谩lido.
"""

# Completa la clase AgenteRL mejorada con propiedades
class AgenteRL:
    def __init__(self, num_acciones, epsilon=0.1, alpha=0.1, gamma=0.9):
        self._num_acciones = num_acciones
        self._q_values = {}  # Diccionario para almacenar valores Q

        # Usa propiedades para estos par谩metros
        self._epsilon = None
        self._alpha = None
        self._gamma = None

        # Asigna valores a trav茅s de propiedades para validaci贸n
        self.epsilon = epsilon
        self.alpha = alpha
        self.gamma = gamma

    # Implementa la propiedad epsilon con validaci贸n
    @property
    def epsilon(self):
        # Tu c贸digo aqu铆
        pass

    @epsilon.setter
    def epsilon(self, valor):
        # Valida que epsilon est茅 entre 0 y 1
        # Tu c贸digo aqu铆
        pass

    # Implementa propiedades alpha y gamma con validaci贸n similar
    # Tu c贸digo aqu铆

    def seleccionar_accion(self, estado):
        """Selecciona una acci贸n usando la pol铆tica epsilon-greedy."""
        estado_str = str(estado)

        # Inicializa valores Q si es necesario
        if estado_str not in self._q_values:
            self._q_values[estado_str] = [0.0] * self._num_acciones

        # Pol铆tica epsilon-greedy
        if random.random() < self.epsilon:
            return random.randint(0, self._num_acciones - 1)  # Exploraci贸n
        else:
            return self._q_values[estado_str].index(max(self._q_values[estado_str]))  # Explotaci贸n

# Prueba la clase con propiedades
import random

agente = AgenteRL(num_acciones=4, epsilon=0.2)
print(f"Epsilon: {agente.epsilon}")

# Intenta asignar valores v谩lidos e inv谩lidos
try:
    agente.epsilon = 0.5
    print(f"Nuevo epsilon: {agente.epsilon}")

    agente.epsilon = 1.5  # Deber铆a lanzar ValueError
except ValueError as e:
    print(f"Error capturado: {e}")

"""# Ejercicio 9: Herencia para Diferentes Tipos de Agentes
**Dificultad: Intermedia**

**Objetivo**: Practicar la herencia y la sobreescritura de m茅todos en POO.

**Descripci贸n**: Crea una jerarqu铆a de clases para diferentes tipos de agentes de aprendizaje por refuerzo,
heredando de una clase base com煤n llamada `AgenteBase`.

Implementa:
1. `AgenteBase`: Clase abstracta con m茅todos `seleccionar_accion` y `aprender`
2. `AgenteAleatorio`: Un agente que selecciona acciones aleatorias y no aprende
3. `AgenteQLearning`: Un agente que implementa el algoritmo Q-Learning

Cada agente debe tener su propia implementaci贸n espec铆fica de los m茅todos heredados.
"""

import random

# Completa la jerarqu铆a de clases de agentes

# Clase base
class AgenteBase:
    def __init__(self, num_acciones, nombre="AgenteGen茅rico"):
        self.num_acciones = num_acciones
        self.nombre = nombre
        self.experiencia_total = 0

    def seleccionar_accion(self, estado):
        # M茅todo abstracto que las subclases deben implementar
        raise NotImplementedError("Las subclases deben implementar seleccionar_accion")

    def aprender(self, estado, accion, recompensa, siguiente_estado):
        # M茅todo abstracto que las subclases deben implementar
        raise NotImplementedError("Las subclases deben implementar aprender")

    def __str__(self):
        return f"{self.nombre} (acciones: {self.num_acciones}, exp: {self.experiencia_total})"

# Completa la clase AgenteAleatorio
class AgenteAleatorio(AgenteBase):
    # Un agente que selecciona acciones aleatorias
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Completa la clase AgenteQLearning
class AgenteQLearning(AgenteBase):
    # Un agente que implementa Q-Learning
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Prueba las clases
agente_aleatorio = AgenteAleatorio(4, "Explorador")
agente_q = AgenteQLearning(4, "Aprendiz", alpha=0.2, gamma=0.9, epsilon=0.1)

print(agente_aleatorio)
print(agente_q)

# Prueba m茅todos
estado = "estado_prueba"
accion_aleatoria = agente_aleatorio.seleccionar_accion(estado)
accion_q = agente_q.seleccionar_accion(estado)

print(f"Acci贸n aleatoria: {accion_aleatoria}")
print(f"Acci贸n Q-Learning: {accion_q}")

# Prueba aprendizaje
agente_aleatorio.aprender(estado, accion_aleatoria, 1, "siguiente_estado")
agente_q.aprender(estado, accion_q, 1, "siguiente_estado")

"""# Ejercicio 10: M茅todos de Instancia, de Clase y Est谩ticos
**Dificultad: Intermedia**

**Objetivo**: Entender y practicar los diferentes tipos de m茅todos en clases Python.

**Descripci贸n**: Crea una clase `Experimento` que utilice los tres tipos de m茅todos para
gestionar experimentos de aprendizaje por refuerzo:

1. **M茅todo de instancia**: `ejecutar()` - Ejecuta el experimento y almacena resultados
2. **M茅todo de clase**: `crear_desde_configuracion()` - Crea un experimento a partir de un diccionario de configuraci贸n
3. **M茅todo est谩tico**: `calcular_metricas()` - Calcula m茅tricas a partir de un hist贸rico de recompensas

El m茅todo de clase debe usar el decorador `@classmethod` y el m茅todo est谩tico debe usar `@staticmethod`.
"""

import time
import random
import numpy as np

# Completa la clase Experimento con los tres tipos de m茅todos
class Experimento:
    # Atributo de clase
    contador_experimentos = 0

    def __init__(self, nombre, agente, entorno):
        # Inicializa atributos de instancia
        self.nombre = nombre
        self.agente = agente
        self.entorno = entorno
        self.fecha_inicio = None
        self.fecha_fin = None
        self.resultados = None

        # Incrementa contador de clase
        Experimento.contador_experimentos += 1
        self.id = Experimento.contador_experimentos

    # M茅todo de instancia
    def ejecutar(self, episodios=100, max_pasos=1000):
        """Ejecuta el experimento y almacena resultados."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    # M茅todo de clase
    @classmethod
    def crear_desde_configuracion(cls, config_dict):
        """
        M茅todo de clase que crea un experimento a partir de un diccionario de configuraci贸n.
        """
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    # M茅todo est谩tico
    @staticmethod
    def calcular_metricas(historico_recompensas):
        """
        M茅todo est谩tico que calcula m茅tricas a partir de un hist贸rico de recompensas.
        No depende del estado del objeto.
        """
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Prueba los diferentes tipos de m茅todos
# Simula agente y entorno para pruebas
class AgenteSimulado:
    def seleccionar_accion(self, estado):
        return 0

class EntornoSimulado:
    def reset(self):
        return 0

    def paso(self, accion):
        return 0, random.randint(1, 10), random.random() < 0.1, {}

# Prueba m茅todo de instancia
agente = AgenteSimulado()
entorno = EntornoSimulado()
experimento = Experimento("Prueba1", agente, entorno)
experimento.ejecutar(episodios=5)
print(f"Resultados: {experimento.resultados}")

# Prueba m茅todo de clase
config = {
    "nombre": "ExperimentoConfig",
    "agente": agente,
    "entorno": entorno
}
experimento2 = Experimento.crear_desde_configuracion(config)
print(f"Experimento desde config: {experimento2.nombre} (ID: {experimento2.id})")

# Prueba m茅todo est谩tico
historico = [[5, 8, 10], [6, 7, 9, 12], [3, 5, 8]]
metricas = Experimento.calcular_metricas(historico)
print(f"M茅tricas calculadas: {metricas}")

"""# Ejercicio 11: Polimorfismo para Entornos de Entrenamiento
**Dificultad: Intermedia**

**Objetivo**: Implementar polimorfismo con clases de entornos que comparten una interfaz com煤n.

**Descripci贸n**: Crea diferentes clases de entornos que comparten una interfaz com煤n, y una funci贸n
de entrenamiento que funcione con cualquiera de ellos.

Implementa:
1. `Entorno`: Clase base abstracta con m茅todos `reset()`, `paso()` y `obtener_num_acciones()`
2. `EntornoSimple`: Entorno con estados lineales (0-9) y dos acciones (izquierda/derecha)
3. `EntornoRejilla`: Entorno tipo rejilla 2D con 4 acciones (arriba/derecha/abajo/izquierda)
4. Funci贸n `entrenar()`: Entrena un agente en cualquier entorno que implemente la interfaz

El polimorfismo permite que la funci贸n de entrenamiento funcione con diferentes tipos de entornos.
"""

import random
from abc import ABC, abstractmethod

# Completa la jerarqu铆a de clases para entornos

# Clase base abstracta
class Entorno(ABC):
    @abstractmethod
    def reset(self):
        """Reinicia el entorno y retorna el estado inicial."""
        pass

    @abstractmethod
    def paso(self, accion):
        """
        Ejecuta una acci贸n y retorna (siguiente_estado, recompensa, terminado, info).
        """
        pass

    @abstractmethod
    def obtener_num_acciones(self):
        """Retorna el n煤mero de acciones posibles."""
        pass

# Completa la clase EntornoSimple
class EntornoSimple(Entorno):
    # Un entorno con estados del 0 al 9 y 2 acciones (izquierda/derecha)
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Completa la clase EntornoRejilla
class EntornoRejilla(Entorno):
    # Un entorno tipo rejilla 2D con 4 acciones (arriba/derecha/abajo/izquierda)
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Funci贸n polim贸rfica para entrenar un agente en cualquier entorno
def entrenar(agente, entorno, episodios=100, max_pasos=1000):
    """
    Entrena un agente en un entorno.
    Funciona con cualquier agente y entorno que implementen las interfaces requeridas.
    """
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Prueba de polimorfismo
from random import randint

# Creamos un agente aleatorio simple
class AgenteAleatorio:
    def __init__(self, num_acciones):
        self.num_acciones = num_acciones

    def seleccionar_accion(self, estado):
        return randint(0, self.num_acciones - 1)

    def aprender(self, estado, accion, recompensa, siguiente_estado):
        pass  # Este agente no aprende

# Entrenamos al agente en diferentes entornos
entorno1 = EntornoSimple()
entorno2 = EntornoRejilla(ancho=5, alto=5)

agente1 = AgenteAleatorio(entorno1.obtener_num_acciones())
agente2 = AgenteAleatorio(entorno2.obtener_num_acciones())

resultado1 = entrenar(agente1, entorno1, episodios=10)
resultado2 = entrenar(agente2, entorno2, episodios=10)

print("Resultados en EntornoSimple:", resultado1)
print("Resultados en EntornoRejilla:", resultado2)

"""# Ejercicio 12: Manejo de Excepciones en Entorno de RL
**Dificultad: Intermedia**

**Objetivo**: Implementar manejo de excepciones robusto para un entorno de aprendizaje por refuerzo.

**Descripci贸n**: Crea excepciones personalizadas y un sistema de manejo de errores para un entorno de RL.

Debes implementar:
1. Excepciones personalizadas:
   - `EntornoError`: Clase base para excepciones del entorno
   - `AccionInvalidaError`: Cuando se intenta una acci贸n no v谩lida
   - `EstadoInvalidoError`: Cuando se intenta acceder a un estado no v谩lido

2. Un entorno robusto `EntornoRobusto` con manejo de excepciones para:
   - Validar acciones
   - Validar estados
   - Manejar errores durante la ejecuci贸n de acciones

Usa bloques try/except en los m茅todos relevantes y proporciona mensajes de error descriptivos.
"""

# Excepciones personalizadas para entorno de RL
class EntornoError(Exception):
    """Clase base para excepciones relacionadas con el entorno."""
    pass

class AccionInvalidaError(EntornoError):
    """Se lanza cuando se intenta realizar una acci贸n no v谩lida."""
    pass

class EstadoInvalidoError(EntornoError):
    """Se lanza cuando se intenta acceder a un estado no v谩lido."""
    pass

# Completa la clase EntornoRobusto con manejo de excepciones
class EntornoRobusto:
    def __init__(self, num_estados=10, num_acciones=4):
        self.num_estados = num_estados
        self.num_acciones = num_acciones
        self.estado_actual = 0
        self.terminado = False

    def reset(self):
        """Reinicia el entorno al estado inicial."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def validar_accion(self, accion):
        """Valida que una acci贸n sea v谩lida, lanzando excepci贸n si no lo es."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def validar_estado(self, estado):
        """Valida que un estado sea v谩lido, lanzando excepci贸n si no lo es."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def paso(self, accion):
        """
        Ejecuta una acci贸n y retorna (siguiente_estado, recompensa, terminado, info).
        Incluye manejo de excepciones.
        """
        try:
            # Validar acci贸n
            # Tu c贸digo aqu铆

            # L贸gica del entorno
            # Tu c贸digo aqu铆

            # Retornar resultado
            # Tu c贸digo aqu铆
            pass  # Elimina esta l铆nea al implementar tu soluci贸n

        except AccionInvalidaError as e:
            # Manejo del error
            # Tu c贸digo aqu铆
            pass  # Elimina esta l铆nea al implementar tu soluci贸n

        except Exception as e:
            # Manejo de otros errores inesperados
            # Tu c贸digo aqu铆
            pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Prueba el manejo de excepciones
entorno = EntornoRobusto(num_estados=5, num_acciones=2)
estado = entorno.reset()
print(f"Estado inicial: {estado}")

# Prueba con acci贸n v谩lida
try:
    resultado = entorno.paso(1)
    print(f"Paso exitoso: {resultado}")
except EntornoError as e:
    print(f"Error controlado: {e}")

# Prueba con acci贸n inv谩lida
try:
    resultado = entorno.paso(5)  # Acci贸n fuera de rango
    print(f"Paso exitoso: {resultado}")
except EntornoError as e:
    print(f"Error controlado: {e}")

"""# Ejercicio 13: Decorador Parametrizado para Control de Intentos
**Dificultad: Avanzada**

**Objetivo**: Crear decoradores parametrizados que a帽adan funcionalidad espec铆fica a funciones.

**Descripci贸n**: Implementa un decorador `reintentar` que intentar谩 ejecutar una funci贸n varias veces si ocurre una excepci贸n.

El decorador debe:
1. Aceptar par谩metros para configurar el comportamiento:
   - `max_intentos`: N煤mero m谩ximo de intentos (por defecto 3)
   - `excepciones`: Tupla de tipos de excepciones a capturar (por defecto todas)
   - `espera`: Tiempo de espera entre intentos (por defecto 0)
2. Aplicar la l贸gica de reintentos solo si ocurre alguna de las excepciones especificadas
3. Registrar cada intento fallido y el error
4. Tras alcanzar el n煤mero m谩ximo de intentos, relanzar la 煤ltima excepci贸n

Este tipo de decorador es 煤til en entornos donde pueden ocurrir errores transitorios.
"""

import random
import time

# Completa el decorador parametrizado reintentar
def reintentar(max_intentos=3, excepciones=(Exception,), espera=0):
    # Tu c贸digo aqu铆 - recuerda que es un decorador con argumentos
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Funci贸n que simula un servicio inestable
@reintentar(max_intentos=5, excepciones=(ValueError, RuntimeError), espera=0.5)
def servicio_inestable(probabilidad_error=0.7):
    """
    Simula un servicio que falla con cierta probabilidad.
    """
    # Simula un servicio que falla con cierta probabilidad
    if random.random() < probabilidad_error:
        if random.random() < 0.5:
            raise ValueError("Error de valor simulado")
        else:
            raise RuntimeError("Error de ejecuci贸n simulado")
    return "隆Operaci贸n exitosa!"

# Prueba el servicio con reintento
for i in range(3):  # Prueba 3 veces
    try:
        resultado = servicio_inestable()
        print(f"Intento {i+1}: {resultado}")
    except Exception as e:
        print(f"Intento {i+1}: Fall贸 despu茅s de m煤ltiples intentos: {e}")

"""# Ejercicio 14: Jerarqu铆a de Excepciones Personalizadas
**Dificultad: Avanzada**

**Objetivo**: Dise帽ar una jerarqu铆a completa de excepciones personalizadas para un framework de aprendizaje por refuerzo.

**Descripci贸n**: Crea una estructura jer谩rquica de excepciones para diferentes componentes del framework:

1. Estructura b谩sica:
   - `RLError`: Clase base para todas las excepciones del framework
   - `AgentError`: Errores relacionados con agentes
   - `EntornoError`: Errores relacionados con entornos
   - `PoliticaError`: Errores relacionados con pol铆ticas
   - `EntrenamientoError`: Errores relacionados con el proceso de entrenamiento

2. Excepciones espec铆ficas para cada categor铆a

3. Funci贸n `entrenar_agente_robusto` que utiliza diferentes tipos de excepciones para mostrar
   c贸mo pueden manejarse de manera diferenciada.

Este enfoque permite un manejo de errores m谩s granular y espec铆fico.
"""

# Completa la jerarqu铆a de excepciones para un framework RL

# Excepci贸n base para todo el framework
class RLError(Exception):
    """Clase base para todas las excepciones del framework RL."""
    pass

# ---- Completa las excepciones relacionadas con agentes ----
class AgentError(RLError):
    """Error relacionado con agentes."""
    pass

class AgenteFuncionFaltanteError(AgentError):
    """Error cuando un agente no implementa una funci贸n requerida."""
    def __init__(self, funcion):
        self.funcion = funcion
        super().__init__(f"El agente debe implementar la funci贸n '{funcion}'")

# ---- Completa las excepciones relacionadas con entornos ----
class EntornoError(RLError):
    """Error relacionado con entornos."""
    pass

class EntornoInterfazError(EntornoError):
    """Error cuando un entorno no implementa la interfaz requerida."""
    pass

# ---- Completa las excepciones relacionadas con pol铆ticas ----
class PoliticaError(RLError):
    """Error relacionado con pol铆ticas."""
    pass

# ---- Completa las excepciones relacionadas con entrenamiento ----
class EntrenamientoError(RLError):
    """Error relacionado con entrenamiento."""
    pass

class EpisodioError(EntrenamientoError):
    """Error durante un episodio de entrenamiento."""
    pass

# Funci贸n de ejemplo que utiliza las excepciones
def entrenar_agente_robusto(agente, entorno, num_episodios):
    """
    Funci贸n de ejemplo que usa el sistema de excepciones para un entrenamiento robusto.
    """
    try:
        if not hasattr(agente, 'seleccionar_accion'):
            raise AgenteFuncionFaltanteError("seleccionar_accion")

        if not hasattr(entorno, 'reset') or not hasattr(entorno, 'paso'):
            raise EntornoInterfazError("El entorno debe implementar reset() y paso()")

        resultados = []
        for episodio in range(num_episodios):
            try:
                recompensa_total = ejecutar_episodio(agente, entorno)
                resultados.append(recompensa_total)
            except EpisodioError as e:
                print(f"Error en episodio {episodio}: {e}")
                # Contin煤a con el siguiente episodio

        return resultados

    except AgentError as e:
        print(f"Error cr铆tico en el agente: {e}")
        return None
    except EntornoError as e:
        print(f"Error cr铆tico en el entorno: {e}")
        return None
    except RLError as e:
        print(f"Error general en el framework: {e}")
        return None
    except Exception as e:
        print(f"Error inesperado: {e}")
        raise  # Re-lanza excepciones no controladas

# Funci贸n auxiliar para el ejemplo
def ejecutar_episodio(agente, entorno):
    """Ejecuta un episodio de entrenamiento."""
    estado = entorno.reset()
    recompensa_total = 0
    terminado = False

    while not terminado:
        try:
            accion = agente.seleccionar_accion(estado)
            siguiente_estado, recompensa, terminado, _ = entorno.paso(accion)
            agente.aprender(estado, accion, recompensa, siguiente_estado)
            estado = siguiente_estado
            recompensa_total += recompensa
        except Exception as e:
            # Convertimos la excepci贸n a un tipo espec铆fico de nuestro framework
            raise EpisodioError(f"Error durante el episodio: {e}")

    return recompensa_total

# Prueba con algunas excepciones
class AgenteIncompleto:
    # Falta el m茅todo seleccionar_accion
    pass

class EntornoIncompleto:
    def reset(self):
        return 0
    # Falta el m茅todo paso

# Escenarios de prueba
try:
    entrenar_agente_robusto(AgenteIncompleto(), EntornoIncompleto(), 10)
except Exception as e:
    print(f"Error de prueba: {e}")

"""# Ejercicio 15: Uso Avanzado de Try-Except-Finally
**Dificultad: Avanzada**

**Objetivo**: Implementar un sistema robusto de manejo de errores con try-except-finally para garantizar limpieza de recursos.

**Descripci贸n**: Crea un sistema de guardado de modelos para agentes de RL que use bloques `try-except-finally` para garantizar la persistencia de datos incluso en caso de errores.

La clase `ModelManager` debe:
1. Manejar el guardado seguro de modelos usando archivos temporales
2. Garantizar limpieza de recursos incluso en caso de fallo
3. Implementar manejo espec铆fico para diferentes tipos de errores
4. Proporcionar funciones para cargar, listar y eliminar modelos

Este enfoque garantiza que no se pierdan datos ni queden recursos sin liberar, incluso cuando ocurren errores.
"""

import os
import pickle
import time
import random

# Completa la clase ModelManager para gesti贸n robusta de modelos
class ModelManager:
    def __init__(self, directorio="modelos"):
        self.directorio = directorio
        self.archivo_temporal = None

        # Crea el directorio si no existe
        if not os.path.exists(directorio):
            os.makedirs(directorio)

    def guardar_modelo(self, agente, nombre_archivo=None):
        """
        Guarda un modelo de agente de forma segura, con protecci贸n contra corrupci贸n.
        Usa try-except-finally para garantizar limpieza de recursos.
        """
        # Tu c贸digo aqu铆: implementa guardado seguro usando try-except-finally
        # Recuerda utilizar un archivo temporal primero, y renombrarlo solo si todo va bien
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def cargar_modelo(self, nombre_archivo):
        """
        Carga un modelo de agente con manejo de errores.
        Incluye verificaci贸n de integridad b谩sica.
        """
        # Tu c贸digo aqu铆: implementa carga segura usando try-except
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def listar_modelos(self):
        """Lista todos los modelos guardados en el directorio."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def eliminar_modelo(self, nombre_archivo):
        """Elimina un modelo con confirmaci贸n de 茅xito."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Agente simple para pruebas
class AgenteDemo:
    def __init__(self, nombre="AgentePrueba", parametros=None):
        self.nombre = nombre
        self.parametros = parametros or {"alpha": 0.1, "gamma": 0.9}
        self.q_table = {str(i): [random.random() for _ in range(4)] for i in range(5)}
        self.timestamp = time.time()

    def __str__(self):
        return f"Agente '{self.nombre}' (timestamp: {self.timestamp})"

# Prueba el sistema de guardado y carga
manager = ModelManager()

# Crea y guarda un agente
agente_original = AgenteDemo("Explorador")
print(f"Agente original: {agente_original}")

# Guarda el modelo
try:
    ruta = manager.guardar_modelo(agente_original, "explorador_v1")
    print(f"Modelo guardado en: {ruta}")
except Exception as e:
    print(f"Error al guardar: {e}")

# Lista modelos disponibles
modelos = manager.listar_modelos()
print(f"Modelos disponibles: {modelos}")

# Carga el modelo
try:
    agente_cargado = manager.cargar_modelo("explorador_v1")
    print(f"Modelo cargado: {agente_cargado}")
except Exception as e:
    print(f"Error al cargar: {e}")

# Simula una corrupci贸n o error durante guardado
try:
    # Forzamos un error durante guardado
    class AgenteProblematico(AgenteDemo):
        def __getstate__(self):
            # Este m茅todo se llama durante la serializaci贸n con pickle
            raise RuntimeError("隆Error simulado durante serializaci贸n!")

    agente_malo = AgenteProblematico("AgenteMalo")
    manager.guardar_modelo(agente_malo, "no_deberia_existir")
except Exception as e:
    print(f"Error esperado capturado: {e}")

# Verificamos que los archivos temporales se hayan limpiado
archivos = os.listdir(manager.directorio)
print(f"Archivos en directorio: {archivos}")

"""# Ejercicio 16: Patr贸n Singleton para Configuraci贸n de Experimentos
**Dificultad: Avanzada**

**Objetivo**: Implementar el patr贸n de dise帽o Singleton para mantener una configuraci贸n global.

**Descripci贸n**: Crea una clase `ConfigExperimentos` que use el patr贸n Singleton para mantener una configuraci贸n global de experimentos de aprendizaje por refuerzo.

La clase debe:
1. Implementar el patr贸n Singleton (solo puede existir una instancia)
2. Almacenar par谩metros de configuraci贸n globales como seed, n煤mero de episodios, etc.
3. Permitir registrar y recuperar configuraciones de entornos
4. Proporcionar m茅todo para reiniciar la configuraci贸n a valores por defecto

Este patr贸n evita la duplicaci贸n de configuraciones y garantiza que todos los componentes
del sistema accedan a los mismos par谩metros.
"""

# Completa el Singleton para configuraci贸n de experimentos

class ConfigExperimentos:
    """
    Singleton para gestionar la configuraci贸n global de experimentos.
    """
    _instancia = None

    def __new__(cls):
        # Implementa el patr贸n Singleton
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def __init__(self):
        # Solo se ejecuta una vez para la 煤nica instancia
        # Valores por defecto
        if not hasattr(self, 'inicializado'):
            self.seed = 42
            self.num_episodios = 1000
            self.max_pasos = 500
            self.directorio_resultados = "./resultados"
            self.hiperparametros = {
                "alpha": 0.1,
                "gamma": 0.99,
                "epsilon": 0.1
            }
            self.entornos_registrados = []
            self.inicializado = True

    def registrar_entorno(self, nombre_entorno, configuracion):
        """Registra un nuevo entorno con su configuraci贸n."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def obtener_entorno(self, nombre_entorno):
        """Obtiene la configuraci贸n de un entorno registrado."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def reiniciar_configuracion(self):
        """Reinicia la configuraci贸n a valores por defecto."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def __str__(self):
        """Representaci贸n en texto de la configuraci贸n actual."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Prueba el Singleton
# 1. Obtenemos la instancia y modificamos valores
config1 = ConfigExperimentos()
config1.seed = 123
config1.hiperparametros["alpha"] = 0.05
config1.registrar_entorno("CartPole", {"version": "v1", "render_mode": "rgb_array"})

# 2. Obtenemos otra "instancia" (que debe ser la misma)
config2 = ConfigExperimentos()
print(f"驴Son el mismo objeto? {config1 is config2}")  # Debe ser True
print(f"Seed en config2: {config2.seed}")  # Debe mostrar 123
print(f"Alpha en config2: {config2.hiperparametros['alpha']}")  # Debe mostrar 0.05

# 3. Verificamos que los entornos registrados son accesibles
entorno = config2.obtener_entorno("CartPole")
print(f"Entorno CartPole: {entorno}")

"""# Ejercicio 17: Patr贸n Factory para Crear Diferentes Tipos de Entornos
**Dificultad: Avanzada**

**Objetivo**: Implementar el patr贸n de dise帽o Factory para crear diferentes tipos de objetos.

**Descripci贸n**: Crea una Factory para generar diferentes tipos de entornos de aprendizaje por refuerzo.

El sistema debe incluir:
1. Clase base abstracta `Entorno` definiendo la interfaz com煤n
2. Implementaciones de diferentes tipos de entornos:
   - `EntornoDiscreto`: Con espacio de estados discreto y finito
   - `EntornoRejilla`: Entorno tipo rejilla 2D
   - `EntornoDinamico`: Entorno con din谩mica cambiante
3. `EntornoFactory`: Factory b谩sica para crear entornos a partir de su tipo
4. `RegistroEntornos`: Factory avanzada con registro din谩mico de tipos de entornos

Este patr贸n facilita la creaci贸n de diferentes tipos de objetos sin exponer la l贸gica
de instanciaci贸n al cliente.
"""

from abc import ABC, abstractmethod
import random

# Completa la jerarqu铆a de clases de entornos y su factory

# Clase base abstracta para entornos
class Entorno(ABC):
    @abstractmethod
    def reset(self):
        """Reinicia el entorno y retorna el estado inicial."""
        pass

    @abstractmethod
    def paso(self, accion):
        """Ejecuta acci贸n y retorna (siguiente_estado, recompensa, terminado, info)."""
        pass

    @abstractmethod
    def obtener_num_acciones(self):
        """Retorna el n煤mero de acciones posibles."""
        pass

    @abstractmethod
    def obtener_nombre(self):
        """Retorna el nombre del entorno."""
        pass

# Implementa diferentes tipos de entornos

class EntornoDiscreto(Entorno):
    """Entorno con espacio de estados discreto y finito."""
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

class EntornoRejilla(Entorno):
    """Entorno tipo rejilla 2D."""
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

class EntornoDinamico(Entorno):
    """Entorno con din谩mica cambiante."""
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Implementa la factory de entornos

class EntornoFactory:
    """Factory para crear diferentes tipos de entornos."""

    @staticmethod
    def crear_entorno(tipo, **kwargs):
        """
        Crea un entorno del tipo especificado con los par谩metros dados.

        Args:
            tipo: Tipo de entorno ("discreto", "rejilla", "dinamico")
            **kwargs: Par谩metros espec铆ficos para cada tipo de entorno

        Returns:
            Instancia del entorno solicitado
        """
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Factory m谩s avanzada con registro din谩mico de entornos

class RegistroEntornos:
    """Factory con registro din谩mico de tipos de entornos."""

    _entornos_registrados = {}

    @classmethod
    def registrar(cls, nombre, clase_entorno):
        """Registra un nuevo tipo de entorno."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    @classmethod
    def crear_entorno(cls, nombre, **kwargs):
        """Crea un entorno del tipo registrado."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    @classmethod
    def listar_entornos(cls):
        """Lista todos los tipos de entornos registrados."""
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Registramos los entornos disponibles
RegistroEntornos.registrar("discreto", EntornoDiscreto)
RegistroEntornos.registrar("rejilla", EntornoRejilla)
RegistroEntornos.registrar("dinamico", EntornoDinamico)

# Prueba la factory simple
entorno1 = EntornoFactory.crear_entorno("discreto", num_estados=10, num_acciones=3)
entorno2 = EntornoFactory.crear_entorno("rejilla", ancho=5, alto=5)

print(f"Entorno 1: {entorno1.obtener_nombre()}, acciones: {entorno1.obtener_num_acciones()}")
print(f"Entorno 2: {entorno2.obtener_nombre()}, acciones: {entorno2.obtener_num_acciones()}")

# Prueba el registro din谩mico
entornos_disponibles = RegistroEntornos.listar_entornos()
print(f"Entornos registrados: {entornos_disponibles}")

entorno3 = RegistroEntornos.crear_entorno("dinamico", dificultad="alta")
print(f"Entorno 3: {entorno3.obtener_nombre()}")

# Intenta crear un tipo no registrado
try:
    entorno_invalido = RegistroEntornos.crear_entorno("inexistente")
except Exception as e:
    print(f"Error esperado: {e}")

"""# Ejercicio 18: Patr贸n Strategy para Pol铆ticas de Exploraci贸n
**Dificultad: Avanzada**

**Objetivo**: Implementar el patr贸n de dise帽o Strategy para crear comportamientos intercambiables.

**Descripci贸n**: Crea un sistema basado en el patr贸n Strategy para pol铆ticas de exploraci贸n intercambiables en un agente de aprendizaje por refuerzo.

El sistema debe incluir:
1. Interfaz `EstrategiaExploracion` con m茅todos comunes
2. Implementaciones concretas de estrategias:
   - `EpsilonGreedy`: Basada en probabilidad epsilon para exploraci贸n
   - `Softmax`: Usa distribuci贸n de probabilidad basada en valores Q
   - `UCB`: Balance exploraci贸n/explotaci贸n basado en Upper Confidence Bound
3. Clase `AgenteConEstrategia` que utilice estas estrategias de forma intercambiable

Este patr贸n permite cambiar algoritmos "en tiempo de ejecuci贸n", ideal para experimentar
con diferentes pol铆ticas de exploraci贸n.
"""

import random
import math
import numpy as np
from abc import ABC, abstractmethod

# Completa la jerarqu铆a de estrategias de exploraci贸n

# Interfaz para estrategias de exploraci贸n
class EstrategiaExploracion(ABC):
    """Interfaz para implementar diferentes estrategias de exploraci贸n."""

    @abstractmethod
    def seleccionar_accion(self, estado, valores_q):
        """
        Selecciona una acci贸n basada en los valores Q y la estrategia.

        Args:
            estado: El estado actual
            valores_q: Lista/array de valores Q para cada acci贸n posible

        Returns:
            ndice de la acci贸n seleccionada
        """
        pass

    @abstractmethod
    def actualizar(self, **kwargs):
        """Actualiza par谩metros internos de la estrategia."""
        pass

# Implementa estrategias concretas

class EpsilonGreedy(EstrategiaExploracion):
    """Estrategia epsilon-greedy con decaimiento opcional."""
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

class Softmax(EstrategiaExploracion):
    """Estrategia Softmax/Boltzmann usando distribuci贸n de probabilidad."""
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

class UCB(EstrategiaExploracion):
    """Estrategia Upper Confidence Bound para balance exploraci贸n/explotaci贸n."""
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

# Agente flexible que utiliza una estrategia de exploraci贸n
class AgenteConEstrategia:
    """Agente de RL que puede cambiar de estrategia de exploraci贸n."""

    def __init__(self, num_acciones, estrategia_exploracion):
        self.num_acciones = num_acciones
        self.q_table = {}  # Tabla de valores Q: {estado: [valores para cada acci贸n]}
        self.estrategia = estrategia_exploracion

    def seleccionar_accion(self, estado):
        """Selecciona una acci贸n usando la estrategia actual."""
        # Convertimos el estado a string para usarlo como clave
        estado_str = str(estado)

        # Si es la primera vez que vemos este estado, inicializamos valores
        if estado_str not in self.q_table:
            self.q_table[estado_str] = [0.0] * self.num_acciones

        # Delegamos la selecci贸n a la estrategia de exploraci贸n
        return self.estrategia.seleccionar_accion(estado, self.q_table[estado_str])

    def aprender(self, estado, accion, recompensa, siguiente_estado, alpha=0.1, gamma=0.9):
        """Actualiza tabla Q con aprendizaje b谩sico."""
        estado_str = str(estado)
        sig_estado_str = str(siguiente_estado)

        # Inicializamos valores si es necesario
        if estado_str not in self.q_table:
            self.q_table[estado_str] = [0.0] * self.num_acciones
        if sig_estado_str not in self.q_table:
            self.q_table[sig_estado_str] = [0.0] * self.num_acciones

        # Actualizaci贸n Q-Learning
        q_actual = self.q_table[estado_str][accion]
        max_q_siguiente = max(self.q_table[sig_estado_str])

        # F贸rmula Q-Learning
        self.q_table[estado_str][accion] = q_actual + alpha * (
            recompensa + gamma * max_q_siguiente - q_actual)

    def cambiar_estrategia(self, nueva_estrategia):
        """Cambia la estrategia de exploraci贸n en tiempo de ejecuci贸n."""
        self.estrategia = nueva_estrategia
        return self

    def actualizar_estrategia(self, **kwargs):
        """Actualiza par谩metros de la estrategia actual."""
        return self.estrategia.actualizar(**kwargs)

# Prueba el patr贸n Strategy con las diferentes estrategias
# Simulamos un estado y valores Q para pruebas
estado_prueba = [1, 2, 3]
valores_q_prueba = [0.1, 0.5, 0.3, 0.7]

# Creamos las estrategias
epsilon_greedy = EpsilonGreedy(epsilon=0.3, decaimiento=0.95)
softmax = Softmax(temperatura=1.0)
ucb = UCB(c=2.0)

# Creamos agente con estrategia epsilon-greedy
agente = AgenteConEstrategia(num_acciones=4, estrategia_exploracion=epsilon_greedy)

# Simulamos algunas selecciones con epsilon-greedy
print("Estrategia: Epsilon-Greedy")
for i in range(5):
    accion = agente.seleccionar_accion(estado_prueba)
    print(f"Selecci贸n {i+1}: Acci贸n {accion}")
    agente.actualizar_estrategia()  # Aplica decaimiento

# Cambiamos a estrategia Softmax
agente.cambiar_estrategia(softmax)
print("\nEstrategia: Softmax")
for i in range(5):
    accion = agente.seleccionar_accion(estado_prueba)
    print(f"Selecci贸n {i+1}: Acci贸n {accion}")
    agente.actualizar_estrategia(temperatura=max(0.1, softmax.temperatura * 0.9))

# Cambiamos a estrategia UCB
agente.cambiar_estrategia(ucb)
print("\nEstrategia: UCB")
for i in range(5):
    accion = agente.seleccionar_accion(estado_prueba)
    print(f"Selecci贸n {i+1}: Acci贸n {accion}")

"""# Ejercicio 19: Desarrollo de una Funci贸n para Entrenar Agentes de RL
**Dificultad: Avanzada**

**Objetivo**: Integrar diferentes conceptos para crear una funci贸n completa de entrenamiento de agentes RL.

**Descripci贸n**: Desarrolla una funci贸n completa para entrenar agentes de RL integrando varios conceptos vistos en el m贸dulo:
- Manejo de excepciones
- Par谩metros con valores por defecto
- Algoritmos de aprendizaje
- Control de progreso

La funci贸n `entrenar_agente` debe:
1. Entrenar un agente en un entorno durante m煤ltiples episodios
2. Gestionar par谩metros como tasas de aprendizaje, factores de descuento, etc.
3. Implementar decaimiento de epsilon para exploraci贸n
4. Registrar y retornar m茅tricas de progreso
5. Incluir manejo de excepciones para errores durante el entrenamiento
6. Proporcionar evaluaciones peri贸dicas para medir el progreso real

Este tipo de funci贸n es el coraz贸n de cualquier sistema de aprendizaje por refuerzo.
"""

import time
import random
import numpy as np
from collections import deque
import matplotlib.pyplot as plt

def entrenar_agente(agente, entorno, num_episodios=500, max_pasos=1000,
                   gamma=0.99, alpha=0.1, epsilon_inicial=1.0, epsilon_final=0.01,
                   decaimiento_epsilon=0.995, intervalo_eval=10, mostrar_progreso=True):
    """
    Funci贸n completa para entrenar agentes de aprendizaje por refuerzo.

    Args:
        agente: Objeto agente con m茅todos seleccionar_accion y aprender
        entorno: Objeto entorno con m茅todos reset y paso
        num_episodios: N煤mero de episodios de entrenamiento
        max_pasos: N煤mero m谩ximo de pasos por episodio
        gamma: Factor de descuento para el aprendizaje
        alpha: Tasa de aprendizaje
        epsilon_inicial: Valor inicial de epsilon para exploraci贸n
        epsilon_final: Valor m铆nimo de epsilon
        decaimiento_epsilon: Factor de decaimiento de epsilon
        intervalo_eval: Cada cu谩ntos episodios evaluar sin exploraci贸n
        mostrar_progreso: Si mostrar informaci贸n durante el entrenamiento

    Returns:
        dict: Diccionario con resultados del entrenamiento
    """
    # Estructuras para almacenar resultados
    todas_recompensas = []
    recompensas_evaluacion = []
    epsilon = epsilon_inicial
    tiempo_inicio = time.time()

    # Ventana deslizante para promediar recompensas recientes
    ventana_recompensas = deque(maxlen=50)

    # Funci贸n para ejecutar un episodio de evaluaci贸n (sin exploraci贸n)
    def ejecutar_evaluacion():
        estado = entorno.reset()
        recompensa_total = 0
        terminado = False

        for paso in range(max_pasos):
            # Durante evaluaci贸n, siempre seleccionamos la mejor acci贸n
            # Tu c贸digo aqu铆
            pass

            if terminado:
                break

        return recompensa_total

    # Bucle principal de entrenamiento
    try:
        for episodio in range(1, num_episodios + 1):
            # Reiniciamos el entorno
            estado = entorno.reset()
            recompensa_episodio = 0
            pasos_episodio = 0
            terminado = False

            # Bucle del episodio
            for paso in range(max_pasos):
                # Seleccionamos acci贸n seg煤n la pol铆tica actual (con exploraci贸n)
                # Tu c贸digo aqu铆

                # Ejecutamos la acci贸n en el entorno
                # Tu c贸digo aqu铆

                # El agente aprende de la experiencia
                # Tu c贸digo aqu铆

                # Actualizamos estado y contadores
                # Tu c贸digo aqu铆

                if terminado:
                    break

            # Guardamos estad铆sticas del episodio
            # Tu c贸digo aqu铆

            # Actualizamos 茅psilon con decaimiento
            # Tu c贸digo aqu铆

            # Evaluaci贸n peri贸dica sin exploraci贸n
            if episodio % intervalo_eval == 0:
                # Tu c贸digo aqu铆
                pass

            # Mostramos progreso
            if mostrar_progreso and (episodio % 10 == 0 or episodio == 1):
                # Tu c贸digo aqu铆
                pass

    except KeyboardInterrupt:
        print("\nEntrenamiento interrumpido por el usuario.")
    except Exception as e:
        print(f"\nError durante el entrenamiento: {e}")
        raise

    # Calculamos estad铆sticas finales
    tiempo_total = time.time() - tiempo_inicio

    # Resultados finales
    resultados = {
        "tiempo_total": tiempo_total,
        "num_episodios": episodio,
        "recompensas": todas_recompensas,
        "recompensas_eval": recompensas_evaluacion,
        "epsilon_final": epsilon,
        "recompensa_mejor": max(ventana_recompensas) if ventana_recompensas else 0,
        "recompensa_promedio": np.mean(ventana_recompensas) if ventana_recompensas else 0
    }

    if mostrar_progreso:
        print(f"\nEntrenamiento completado en {tiempo_total:.2f} segundos")
        print(f"Recompensa promedio final: {resultados['recompensa_promedio']:.2f}")

    return resultados

# Ejemplo de uso
# Estas clases son solo para la demostraci贸n, deber铆an implementarse completamente
class AgenteDemo:
    def seleccionar_accion(self, estado, epsilon=0):
        # Implementaci贸n simplificada para demo
        return random.randint(0, 1)

    def aprender(self, estado, accion, recompensa, sig_estado, alpha, gamma):
        # Implementaci贸n simplificada para demo
        pass

class EntornoDemo:
    def reset(self):
        return [0, 0]

    def paso(self, accion):
        # Simplificado para demostraci贸n
        recompensa = random.random()
        sig_estado = [random.random(), random.random()]
        terminado = random.random() < 0.1
        return sig_estado, recompensa, terminado, {}

# Ejecutamos entrenamiento de demostraci贸n
agente_demo = AgenteDemo()
entorno_demo = EntornoDemo()

# Par谩metros simplificados para la demo
resultados = entrenar_agente(
    agente_demo,
    entorno_demo,
    num_episodios=50,  # N煤mero reducido para demostraci贸n
    max_pasos=50,      # Pasos reducidos para demostraci贸n
    intervalo_eval=10,
    mostrar_progreso=True
)

# Visualizamos resultados
plt.figure(figsize=(10, 5))
plt.plot(resultados["recompensas"])
plt.xlabel("Episodios")
plt.ylabel("Recompensa total")
plt.title("Curva de aprendizaje")
plt.grid(True)
plt.show()

"""# Ejercicio 20: Implementaci贸n Completa de un Sistema de Agentes y Entornos
**Dificultad: Avanzada**

**Objetivo**: Integrar todos los conceptos aprendidos en un sistema completo de aprendizaje por refuerzo.

**Descripci贸n**: Crea un sistema completo que combine excepciones personalizadas, patrones de dise帽o,
y programaci贸n orientada a objetos para implementar un framework de aprendizaje por refuerzo.

El sistema debe incluir:
1. Jerarqu铆a de excepciones para diferentes componentes
2. Estrategias de exploraci贸n usando el patr贸n Strategy
3. Clases de entorno y agente con interfaces bien definidas
4. Funciones de entrenamiento y evaluaci贸n
5. Visualizaci贸n de resultados

Este ejercicio final integra todos los conceptos del m贸dulo en un solo sistema coherente.
"""

import random
import time
import numpy as np
import matplotlib.pyplot as plt
from abc import ABC, abstractmethod

# ------------- Excepciones Personalizadas -------------
class RLError(Exception):
    """Error base para el framework de RL."""
    pass

class AgentError(RLError):
    """Error relacionado con agentes."""
    pass

class EnvironmentError(RLError):
    """Error relacionado con entornos."""
    pass

# ------------- Estrategias de Exploraci贸n (Patr贸n Strategy) -------------
class ExplorationStrategy(ABC):
    @abstractmethod
    def select_action(self, state, q_values):
        pass

    @abstractmethod
    def update(self, **kwargs):
        pass

class EpsilonGreedy(ExplorationStrategy):
    def __init__(self, epsilon=0.1, decay=0.995, epsilon_min=0.01):
        self.epsilon = epsilon
        self.decay = decay
        self.epsilon_min = epsilon_min

    def select_action(self, state, q_values):
        # Implementa la estrategia epsilon-greedy
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def update(self, **kwargs):
        # Actualiza epsilon con decaimiento
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

# ------------- Interfaces Base -------------
class Environment(ABC):
    @abstractmethod
    def reset(self):
        """Reinicia el entorno y retorna estado inicial."""
        pass

    @abstractmethod
    def step(self, action):
        """Ejecuta acci贸n y retorna (next_state, reward, done, info)."""
        pass

    @property
    @abstractmethod
    def action_space(self):
        """Retorna el espacio de acciones."""
        pass

    @property
    @abstractmethod
    def observation_space(self):
        """Retorna el espacio de observaciones."""
        pass

class Agent(ABC):
    @abstractmethod
    def select_action(self, state):
        """Selecciona acci贸n basada en estado actual."""
        pass

    @abstractmethod
    def learn(self, state, action, reward, next_state, done):
        """Actualiza conocimiento basado en experiencia."""
        pass

    @abstractmethod
    def save(self, path):
        """Guarda el modelo del agente."""
        pass

    @abstractmethod
    def load(self, path):
        """Carga el modelo del agente."""
        pass

# ------------- Implementaciones Concretas -------------

# GridWorld: un entorno de rejilla 2D simple
class GridWorld(Environment):
    def __init__(self, width=5, height=5, max_steps=100):
        self.width = width
        self.height = height
        self.max_steps = max_steps
        self.target_position = (height - 1, width - 1)  # Esquina inferior derecha
        self.agent_position = None
        self.steps = 0
        self._action_space = 4  # Arriba, Derecha, Abajo, Izquierda
        self._observation_space = width * height

    def reset(self):
        # Reinicia entorno y retorna estado inicial
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def step(self, action):
        # Ejecuta acci贸n y retorna resultado
        # Tu c贸digo aqu铆: mueve al agente seg煤n la acci贸n
        # y calcula recompensa y estado terminal
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    @property
    def action_space(self):
        return self._action_space

    @property
    def observation_space(self):
        return self._observation_space

    def _get_state(self):
        """Convierte posici贸n 2D a un 铆ndice de estado."""
        x, y = self.agent_position
        return x * self.width + y

    def render(self):
        """Visualiza el entorno en consola."""
        # Tu c贸digo aqu铆: muestra la rejilla con agente y objetivo
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

# QLearningAgent: agente que implementa el algoritmo Q-Learning
class QLearningAgent(Agent):
    def __init__(self, action_space, observation_space, **kwargs):
        self.action_space = action_space
        self.observation_space = observation_space
        self.q_table = {}

        # Hiperpar谩metros
        self.alpha = kwargs.get('alpha', 0.1)  # Tasa de aprendizaje
        self.gamma = kwargs.get('gamma', 0.99)  # Factor de descuento

        # Estrategia de exploraci贸n (patr贸n Strategy)
        self.exploration = kwargs.get('exploration',
                                     EpsilonGreedy(epsilon=kwargs.get('epsilon', 0.1)))

    def select_action(self, state):
        # Selecciona acci贸n usando estrategia de exploraci贸n
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def learn(self, state, action, reward, next_state, done):
        # Actualiza tabla Q seg煤n algoritmo Q-Learning
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def save(self, path):
        # Guarda la tabla Q en un archivo
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

    def load(self, path):
        # Carga la tabla Q desde un archivo
        # Tu c贸digo aqu铆
        pass  # Elimina esta l铆nea al implementar tu soluci贸n

# ------------- Funciones de Entrenamiento -------------
def train_agent(agent, env, episodes=1000, max_steps=100,
               verbose=True, eval_interval=100):
    """
    Entrena un agente en un entorno.

    Args:
        agent: Instancia de Agent
        env: Instancia de Environment
        episodes: N煤mero de episodios
        max_steps: Pasos m谩ximos por episodio
        verbose: Si mostrar progreso
        eval_interval: Intervalo para evaluaci贸n sin exploraci贸n

    Returns:
        dict: Resultados del entrenamiento
    """
    # Implementa la funci贸n de entrenamiento completa
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

def evaluate_agent(agent, env, episodes=10, max_steps=100):
    """
    Eval煤a un agente sin exploraci贸n.

    Args:
        agent: Instancia de Agent
        env: Instancia de Environment
        episodes: N煤mero de episodios
        max_steps: Pasos m谩ximos por episodio

    Returns:
        float: Recompensa promedio
    """
    # Implementa la funci贸n de evaluaci贸n
    # Tu c贸digo aqu铆
    pass  # Elimina esta l铆nea al implementar tu soluci贸n

# ------------- Funci贸n Principal -------------
def main():
    # Creaci贸n de entorno y agente
    env = GridWorld(width=5, height=5)

    # Estrategia de exploraci贸n
    exploration = EpsilonGreedy(epsilon=1.0, decay=0.995, epsilon_min=0.01)

    # Agente
    agent = QLearningAgent(
        action_space=env.action_space,
        observation_space=env.observation_space,
        alpha=0.1,
        gamma=0.99,
        exploration=exploration
    )

    # Entrenamiento
    results = train_agent(agent, env, episodes=500, max_steps=100)

    # Visualizaci贸n de resultados
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(results['rewards'])
    plt.xlabel('Episodio')
    plt.ylabel('Recompensa total')
    plt.title('Curva de aprendizaje')
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(results['eval_rewards'])
    plt.xlabel('Evaluaci贸n')
    plt.ylabel('Recompensa promedio')
    plt.title('Desempe帽o en evaluaci贸n')
    plt.grid(True)

    plt.tight_layout()
    plt.show()

    # Demostraci贸n final
    print("\n--- Demostraci贸n Final ---")
    for episode in range(3):
        state = env.reset()
        env.render()

        total_reward = 0
        done = False

        while not done:
            action = agent.select_action(state)
            next_state, reward, done, info = env.step(action)
            total_reward += reward

            env.render()
            time.sleep(0.3)  # Pausa para visualizaci贸n

            state = next_state

        print(f"Episodio {episode+1}: Recompensa total = {total_reward}")

# Ejecuci贸n del programa principal
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"Error en la ejecuci贸n: {e}")

"FIN"