# -*- coding: utf-8 -*-
"""Módulo_VI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OZxg908BtGakh2uKMaXwG1BjZgMsBfud
"""

# Commented out IPython magic to ensure Python compatibility.
"""
# Módulo 6: Introducción a OpenAI Gym y Simulaciones

Este cuaderno introduce los conceptos fundamentales del aprendizaje por refuerzo utilizando Gymnasium,
el sucesor de OpenAI Gym, que proporciona entornos estandarizados para el desarrollo
y evaluación de algoritmos de aprendizaje por refuerzo.

## Objetivos de Aprendizaje:

1. Comprender los conceptos básicos de Gymnasium/OpenAI Gym
2. Instalar y configurar entornos de simulación
3. Explorar espacios de observación y acción
4. Implementar agentes con políticas simples (aleatorias y heurísticas)
5. Ejecutar episodios completos y recolectar datos
6. Evaluar el desempeño de los agentes
7. Integrar bibliotecas como TensorFlow y PyTorch para agentes avanzados
8. Desarrollar un agente de control en un entorno de simulación
"""

# Comenzamos instalando las bibliotecas necesarias
# La barra ! al principio permite ejecutar comandos de terminal en el notebook
!pip install gymnasium
!pip install "gymnasium[classic_control]" # Instalamos entornos clásicos como CartPole
!pip install matplotlib
!pip install numpy

# Importamos las bibliotecas básicas
import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym  # Usamos gymnasium, el sucesor de OpenAI Gym
import time
from collections import deque
import random
from IPython.display import HTML, display  # Para mostrar animaciones

# Configuración para las visualizaciones
# %matplotlib inline
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = (12, 6)

"""
## 1. Conceptos Básicos de Gymnasium (OpenAI Gym)

Gymnasium es una biblioteca que proporciona una interfaz estándar para trabajar con entornos
de aprendizaje por refuerzo. Algunos conceptos clave:

- **Entorno (Environment)**: Simulación con la que interactúa el agente
- **Agente (Agent)**: Sistema que toma decisiones y actúa en el entorno
- **Estado (State)**: Representación del entorno en un momento dado
- **Acción (Action)**: Decisión que toma el agente
- **Recompensa (Reward)**: Valor numérico que recibe el agente después de cada acción
- **Episodio (Episode)**: Secuencia de estados, acciones y recompensas hasta un estado terminal

### Estructura básica de un programa con Gymnasium:

1. Crear el entorno
2. Observar el estado inicial
3. En cada paso:
   - Elegir una acción basada en el estado actual
   - Ejecutar la acción en el entorno
   - Observar el nuevo estado y la recompensa
   - Actualizar la política del agente
4. Repetir hasta alcanzar un estado terminal o un número máximo de pasos
"""

# Veamos los entornos disponibles en Gymnasium
all_envs = gym.envs.registry.keys()
env_list = sorted(list(all_envs))

# Mostramos algunos de los entornos disponibles (primeros 10)
print("Algunos entornos disponibles en Gymnasium:")
for i, env_id in enumerate(env_list[:10]):
    print(f"{i+1}. {env_id}")
print("...")

"""
## 2. Instalación y Configuración del Entorno

Ya hemos instalado las bibliotecas necesarias al inicio del cuaderno.
Ahora, vamos a crear nuestro primer entorno: CartPole-v1.

En este entorno, un carro se mueve horizontalmente con un péndulo (poste)
balanceándose encima. El objetivo es mantener el péndulo en posición vertical
moviendo el carro hacia la izquierda o la derecha.
"""

# Creamos el entorno CartPole
env = gym.make('CartPole-v1', render_mode="rgb_array")

# Obtenemos información sobre el entorno
print(f"Espacio de observación: {env.observation_space}")
print(f"Espacio de acción: {env.action_space}")
print(f"Recompensa por paso: 1.0 (valor fijo para CartPole-v1)")
print(f"Objetivo: Mantener el balance durante 500 pasos")

# Función para renderizar el entorno
def render_env(env):
    """Renderiza el entorno actual y muestra la imagen"""
    img = env.render()
    plt.figure(figsize=(8, 6))
    plt.imshow(img)
    plt.axis('off')
    plt.show()

# Inicializamos el entorno y mostramos el estado inicial
observation, info = env.reset(seed=42)  # Usar semilla para reproducibilidad
print(f"Observación inicial: {observation}")
render_env(env)

"""
## 3. Uso de Entornos Simulados: Espacios de Observación y Acción

### Espacio de Observación

En CartPole-v1, la observación es un array de 4 números:
1. Posición del carro (entre -4.8 y 4.8)
2. Velocidad del carro (sin límites explícitos)
3. Ángulo del péndulo (entre -0.418 y 0.418 radianes)
4. Velocidad angular del péndulo (sin límites explícitos)

### Espacio de Acción

En CartPole-v1, el espacio de acción es discreto con 2 acciones posibles:
- 0: Empujar el carro hacia la izquierda
- 1: Empujar el carro hacia la derecha

### Veamos cómo funcionan estos espacios en la práctica:
"""

# Exploramos el espacio de observación
obs_space = env.observation_space
print(f"Dimensiones del espacio de observación: {obs_space.shape}")
print(f"Límite inferior: {obs_space.low}")
print(f"Límite superior: {obs_space.high}")

# Exploramos el espacio de acción
action_space = env.action_space
print(f"Tipo de espacio de acción: {action_space}")
print(f"Número de acciones posibles: {action_space.n}")

# Tomamos algunas acciones aleatorias para ver cómo evoluciona el entorno
print("\nTomando 5 acciones aleatorias:")
observation, info = env.reset(seed=42)
for i in range(5):
    action = env.action_space.sample()  # Acción aleatoria
    observation, reward, terminated, truncated, info = env.step(action)
    done = terminated or truncated

    print(f"Paso {i+1}:")
    print(f"  Acción: {action} ({'Izquierda' if action == 0 else 'Derecha'})")
    print(f"  Nueva observación: {observation}")
    print(f"  Recompensa: {reward}")
    print(f"  Episodio terminado: {done}")
    render_env(env)
    if done:
        break

"""
## 4. Implementación de Agentes Básicos

Ahora vamos a implementar dos tipos de agentes básicos:

1. **Agente Aleatorio**: Toma acciones completamente al azar
2. **Agente Heurístico Simple**: Utiliza una regla sencilla basada en la observación

### 4.1 Agente Aleatorio
"""

def random_policy(observation):
    """Política aleatoria: selecciona una acción al azar"""
    return env.action_space.sample()

# Evaluamos el agente aleatorio
def evaluate_policy(policy, env, n_episodes=100, max_steps=1000):
    """Evalúa una política en un entorno durante varios episodios"""
    episode_rewards = []
    episode_steps = []

    for episode in range(n_episodes):
        observation, info = env.reset()
        total_reward = 0
        steps = 0

        for step in range(max_steps):
            action = policy(observation)
            observation, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

            total_reward += reward
            steps += 1

            if done:
                break

        episode_rewards.append(total_reward)
        episode_steps.append(steps)

    return {
        'mean_reward': np.mean(episode_rewards),
        'std_reward': np.std(episode_rewards),
        'mean_steps': np.mean(episode_steps),
        'std_steps': np.std(episode_steps),
        'episode_rewards': episode_rewards,
        'episode_steps': episode_steps
    }

# Evaluamos la política aleatoria
random_results = evaluate_policy(random_policy, env, n_episodes=10)
print(f"Recompensa media (política aleatoria): {random_results['mean_reward']:.2f} ± {random_results['std_reward']:.2f}")
print(f"Pasos medios por episodio: {random_results['mean_steps']:.2f} ± {random_results['std_steps']:.2f}")

"""
### 4.2 Agente Heurístico Simple

Para CartPole, podemos diseñar una política heurística simple basada en la física del problema:
- Si el péndulo se inclina hacia la derecha (ángulo positivo), movemos el carro hacia la derecha
- Si el péndulo se inclina hacia la izquierda (ángulo negativo), movemos el carro hacia la izquierda
"""

def heuristic_policy(observation):
    """
    Política heurística simple para CartPole:
    - Si el péndulo se inclina hacia la derecha (ángulo > 0), mover a la derecha
    - Si el péndulo se inclina hacia la izquierda (ángulo < 0), mover a la izquierda
    """
    angle = observation[2]  # El tercer elemento es el ángulo del péndulo
    return 1 if angle > 0 else 0

# Evaluamos la política heurística
heuristic_results = evaluate_policy(heuristic_policy, env, n_episodes=10)
print(f"Recompensa media (política heurística): {heuristic_results['mean_reward']:.2f} ± {heuristic_results['std_reward']:.2f}")
print(f"Pasos medios por episodio: {heuristic_results['mean_steps']:.2f} ± {heuristic_results['std_steps']:.2f}")

# Comparamos visualmente el rendimiento de ambas políticas
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.hist(random_results['episode_rewards'], bins=10, alpha=0.7, label='Aleatoria')
plt.hist(heuristic_results['episode_rewards'], bins=10, alpha=0.7, label='Heurística')
plt.xlabel('Recompensa total')
plt.ylabel('Frecuencia')
plt.legend()
plt.title('Distribución de recompensas')

plt.subplot(1, 2, 2)
plt.hist(random_results['episode_steps'], bins=10, alpha=0.7, label='Aleatoria')
plt.hist(heuristic_results['episode_steps'], bins=10, alpha=0.7, label='Heurística')
plt.xlabel('Pasos por episodio')
plt.ylabel('Frecuencia')
plt.legend()
plt.title('Distribución de pasos')

plt.tight_layout()
plt.show()

"""
## 5. Interacción con Entornos: Episodios y Recolección de Datos

Ahora veremos cómo ejecutar episodios completos y recolectar datos para su análisis.
Implementaremos una función para ejecutar un episodio completo y recopilar información.
"""

def run_episode(env, policy, render=True, max_steps=1000):
    """
    Ejecuta un episodio completo usando una política dada
    y recopila datos sobre estados, acciones y recompensas
    """
    observation, info = env.reset()
    episode_data = {
        'observations': [observation],
        'actions': [],
        'rewards': [],
        'cumulative_reward': 0
    }

    frames = []
    if render:
        frames.append(env.render())

    for step in range(max_steps):
        action = policy(observation)
        observation, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated

        episode_data['observations'].append(observation)
        episode_data['actions'].append(action)
        episode_data['rewards'].append(reward)
        episode_data['cumulative_reward'] += reward

        if render:
            frames.append(env.render())

        if done:
            break

    episode_data['frames'] = frames
    episode_data['steps'] = len(episode_data['actions'])

    return episode_data

# Ejecutamos un episodio con la política heurística
episode_data = run_episode(env, heuristic_policy)

print(f"Duración del episodio: {episode_data['steps']} pasos")
print(f"Recompensa total: {episode_data['cumulative_reward']}")

# Mostramos la evolución de algunas variables clave durante el episodio
observations = np.array(episode_data['observations'])

plt.figure(figsize=(12, 8))

plt.subplot(2, 2, 1)
plt.plot(observations[:, 0])
plt.xlabel('Paso')
plt.ylabel('Posición del carro')
plt.title('Posición del carro vs tiempo')

plt.subplot(2, 2, 2)
plt.plot(observations[:, 1])
plt.xlabel('Paso')
plt.ylabel('Velocidad del carro')
plt.title('Velocidad del carro vs tiempo')

plt.subplot(2, 2, 3)
plt.plot(observations[:, 2])
plt.xlabel('Paso')
plt.ylabel('Ángulo del péndulo')
plt.title('Ángulo del péndulo vs tiempo')

plt.subplot(2, 2, 4)
plt.plot(observations[:, 3])
plt.xlabel('Paso')
plt.ylabel('Velocidad angular')
plt.title('Velocidad angular vs tiempo')

plt.tight_layout()
plt.show()

# Visualizamos la distribución de acciones
actions = np.array(episode_data['actions'])
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.bar(['Izquierda', 'Derecha'], [np.sum(actions == 0), np.sum(actions == 1)])
plt.title('Distribución de acciones')
plt.ylabel('Frecuencia')

plt.subplot(1, 2, 2)
plt.plot(np.cumsum(episode_data['rewards']))
plt.xlabel('Paso')
plt.ylabel('Recompensa acumulada')
plt.title('Recompensa acumulada vs tiempo')
plt.grid(True)

plt.tight_layout()
plt.show()

"""
### Visualización animada del episodio

Ahora crearemos una función para visualizar la animación del episodio.
Esto nos permite ver el comportamiento del agente en acción.
"""

from matplotlib import animation

def display_episode_animation(frames, interval=50):
    """Muestra una animación de los frames del episodio"""
    plt.figure(figsize=(8, 6))
    patch = plt.imshow(frames[0])
    plt.axis('off')

    def animate(i):
        patch.set_data(frames[i])
        return [patch]

    anim = animation.FuncAnimation(
        plt.gcf(), animate, frames=len(frames),
        interval=interval, blit=True)

    return HTML(anim.to_jshtml())

# Mostramos una animación del episodio (usando los primeros 100 frames para brevedad)
max_frames = min(100, len(episode_data['frames']))
display_episode_animation(episode_data['frames'][:max_frames])

"""
## 6. Evaluación del Desempeño de los Agentes

Para evaluar el desempeño de un agente, podemos considerar varias métricas:

1. **Recompensa total**: Suma de todas las recompensas recibidas durante un episodio
2. **Duración del episodio**: Número de pasos hasta que el episodio termina
3. **Estabilidad**: Varianza en las recompensas o duración entre diferentes episodios
4. **Tasa de éxito**: Porcentaje de episodios que alcanzan un umbral de recompensa

Implementemos funciones para calcular estas métricas:
"""

def evaluate_agent(policy, env, n_evaluations=100, max_steps=1000):
    """
    Evalúa el desempeño de un agente en términos de múltiples métricas
    """
    episode_rewards = []
    episode_durations = []

    for i in range(n_evaluations):
        data = run_episode(env, policy, render=False, max_steps=max_steps)
        episode_rewards.append(data['cumulative_reward'])
        episode_durations.append(data['steps'])

    metrics = {
        'mean_reward': np.mean(episode_rewards),
        'median_reward': np.median(episode_rewards),
        'std_reward': np.std(episode_rewards),
        'min_reward': np.min(episode_rewards),
        'max_reward': np.max(episode_rewards),
        'mean_duration': np.mean(episode_durations),
        'median_duration': np.median(episode_durations),
        'std_duration': np.std(episode_durations),
        'success_rate': np.mean([r >= 195 for r in episode_rewards]) * 100,  # CartPole se considera resuelto con recompensa ≥ 195
    }

    return metrics

# Evaluamos ambas políticas
random_metrics = evaluate_agent(random_policy, env, n_evaluations=20)
heuristic_metrics = evaluate_agent(heuristic_policy, env, n_evaluations=20)

# Mostramos los resultados
print("Métricas para la política aleatoria:")
for key, value in random_metrics.items():
    print(f"  {key}: {value:.2f}")

print("\nMétricas para la política heurística:")
for key, value in heuristic_metrics.items():
    print(f"  {key}: {value:.2f}")

# Visualizamos la comparación
metrics_to_plot = ['mean_reward', 'std_reward', 'mean_duration', 'success_rate']
indices = np.arange(len(metrics_to_plot))
width = 0.35

fig, ax = plt.subplots(figsize=(12, 6))

random_values = [random_metrics[m] for m in metrics_to_plot]
heuristic_values = [heuristic_metrics[m] for m in metrics_to_plot]

rects1 = ax.bar(indices - width/2, random_values, width, label='Política aleatoria')
rects2 = ax.bar(indices + width/2, heuristic_values, width, label='Política heurística')

ax.set_xticks(indices)
ax.set_xticklabels(metrics_to_plot)
ax.legend()

plt.title('Comparación de métricas entre políticas')
plt.tight_layout()
plt.show()

"""
## 7. Integración con TensorFlow: Agente de Policy Gradient

Ahora vamos a implementar un agente más avanzado utilizando TensorFlow.
Instalaremos TensorFlow si aún no está disponible.
"""

# Instalamos TensorFlow si es necesario
try:
    import tensorflow as tf
    print(f"TensorFlow ya está instalado, versión: {tf.__version__}")
except ImportError:
    print("Instalando TensorFlow...")
    !pip install tensorflow
    import tensorflow as tf
    print(f"TensorFlow instalado, versión: {tf.__version__}")

# Configuramos semillas para reproducibilidad
np.random.seed(42)
tf.random.set_seed(42)

# Definimos la arquitectura de la red neuronal para la política
def build_policy_model(input_shape, n_actions):
    """Construye un modelo de red neuronal para la política"""
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(24, activation='relu', input_shape=input_shape),
        tf.keras.layers.Dense(24, activation='relu'),
        tf.keras.layers.Dense(n_actions, activation='softmax')
    ])
    return model

# Implementamos la clase del agente Policy Gradient
class PolicyGradientAgent:
    def __init__(self, input_shape, n_actions, learning_rate=0.01):
        self.model = build_policy_model(input_shape, n_actions)
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
        self.n_actions = n_actions

        # Para almacenar la experiencia de cada episodio
        self.states = []
        self.actions = []
        self.rewards = []

    def choose_action(self, state):
        """Selecciona una acción basada en el estado actual usando la política"""
        state = np.reshape(state, [1, -1])
        probabilities = self.model.predict(state, verbose=0)[0]
        action = np.random.choice(self.n_actions, p=probabilities)
        return action

    def store_transition(self, state, action, reward):
        """Almacena la transición (estado, acción, recompensa)"""
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)

    def discount_rewards(self, rewards, gamma=0.99):
        """Calcula las recompensas descontadas"""
        discounted_rewards = np.zeros_like(rewards, dtype=np.float32)
        running_add = 0
        for t in reversed(range(len(rewards))):
            running_add = running_add * gamma + rewards[t]
            discounted_rewards[t] = running_add

        # Normalización de las recompensas
        mean = np.mean(discounted_rewards)
        std = np.std(discounted_rewards) if np.std(discounted_rewards) > 0 else 1
        discounted_rewards = (discounted_rewards - mean) / std
        return discounted_rewards

    def train(self, gamma=0.99):
        """Entrena el modelo usando el algoritmo de Policy Gradient"""
        if len(self.states) == 0:
            return

        # Convertimos a arrays de numpy
        states = np.vstack(self.states)
        actions = np.array(self.actions)
        discounted_rewards = self.discount_rewards(self.rewards, gamma)

        # Entrenamiento con gradiente de política
        with tf.GradientTape() as tape:
            # Forward pass
            logits = self.model(states)

            # Construimos la función de pérdida
            action_masks = tf.one_hot(actions, self.n_actions)
            neg_log_prob = tf.reduce_sum(
                -tf.math.log(tf.clip_by_value(logits, 1e-10, 1.0)) * action_masks,
                axis=1
            )
            loss = tf.reduce_mean(neg_log_prob * discounted_rewards)

        # Backpropagation
        grads = tape.gradient(loss, self.model.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))

        # Limpiamos la experiencia después del entrenamiento
        self.states = []
        self.actions = []
        self.rewards = []

        return loss.numpy()

# Función para entrenar el agente
def train_pg_agent(env, agent, n_episodes=200, max_steps=500, gamma=0.99,
                  print_interval=20, solving_threshold=195):
    """Entrena un agente con Policy Gradient en un entorno"""
    history = {
        'episode_rewards': [],
        'episode_lengths': [],
        'losses': []
    }

    for episode in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_loss = 0

        for step in range(max_steps):
            action = agent.choose_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            agent.store_transition(state, action, reward)
            episode_reward += reward
            state = next_state

            if done:
                break

        # Entrenamos después de cada episodio
        loss = agent.train(gamma)
        if loss is not None:
            episode_loss = loss

        # Registramos los resultados
        history['episode_rewards'].append(episode_reward)
        history['episode_lengths'].append(step + 1)
        history['losses'].append(episode_loss)

        # Imprimimos el progreso
        if (episode + 1) % print_interval == 0:
            avg_reward = np.mean(history['episode_rewards'][-print_interval:])
            print(f"Episodio {episode + 1}: Recompensa media = {avg_reward:.2f}, Último episodio = {episode_reward:.2f}")

            # Verificamos si el entorno está resuelto
            if avg_reward >= solving_threshold:
                print(f"\nEntorno resuelto en el episodio {episode + 1}!")
                break

    return history

# Entrenamos al agente
input_shape = (env.observation_space.shape[0],)
n_actions = env.action_space.n
pg_agent = PolicyGradientAgent(input_shape, n_actions)

# Usamos un número menor de episodios para demostración
training_history = train_pg_agent(env, pg_agent, n_episodes=100, print_interval=10)

# Visualizamos el progreso del entrenamiento
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(training_history['episode_rewards'])
plt.axhline(y=195, color='r', linestyle='--', label='Objetivo')
plt.xlabel('Episodio')
plt.ylabel('Recompensa total')
plt.title('Recompensa por episodio')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(training_history['episode_lengths'])
plt.xlabel('Episodio')
plt.ylabel('Duración del episodio')
plt.title('Duración del episodio')

plt.tight_layout()
plt.show()

# Definimos la política del agente entrenado
def pg_policy(observation):
    """Política aprendida por el agente de Policy Gradient"""
    action = pg_agent.choose_action(observation)
    return action

# Evaluamos el agente entrenado
pg_metrics = evaluate_agent(pg_policy, env, n_evaluations=20)

print("\nMétricas para la política aprendida (Policy Gradient):")
for key, value in pg_metrics.items():
    print(f"  {key}: {value:.2f}")

# Comparamos las tres políticas
metrics_to_plot = ['mean_reward', 'std_reward', 'mean_duration', 'success_rate']
indices = np.arange(len(metrics_to_plot))
width = 0.25

fig, ax = plt.subplots(figsize=(14, 6))

random_values = [random_metrics[m] for m in metrics_to_plot]
heuristic_values = [heuristic_metrics[m] for m in metrics_to_plot]
pg_values = [pg_metrics[m] for m in metrics_to_plot]

rects1 = ax.bar(indices - width, random_values, width, label='Política aleatoria')
rects2 = ax.bar(indices, heuristic_values, width, label='Política heurística')
rects3 = ax.bar(indices + width, pg_values, width, label='Policy Gradient')

ax.set_xticks(indices)
ax.set_xticklabels(metrics_to_plot)
ax.legend()

plt.title('Comparación de métricas entre políticas')
plt.tight_layout()
plt.show()

"""
## 8. Implementación con PyTorch

Ahora implementaremos otro agente usando PyTorch en lugar de TensorFlow.
Primero instalamos PyTorch si es necesario.
"""

# Instalamos PyTorch si es necesario
try:
    import torch
    print(f"PyTorch ya está instalado, versión: {torch.__version__}")
except ImportError:
    print("Instalando PyTorch...")
    !pip install torch
    import torch
    print(f"PyTorch instalado, versión: {torch.__version__}")

import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

# Configuramos semillas para reproducibilidad
np.random.seed(42)
torch.manual_seed(42)

# Definimos la arquitectura de la red neuronal para PyTorch
class PolicyNetwork(nn.Module):
    def __init__(self, input_size, output_size):
        super(PolicyNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 24),
            nn.ReLU(),
            nn.Linear(24, 24),
            nn.ReLU(),
            nn.Linear(24, output_size),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.network(x)

# Implementamos la clase del agente Policy Gradient con PyTorch
class PyTorchPolicyGradientAgent:
    def __init__(self, input_size, output_size, learning_rate=0.01):
        self.policy = PolicyNetwork(input_size, output_size)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)
        self.n_actions = output_size

        # Para almacenar la experiencia de cada episodio
        self.log_probs = []
        self.rewards = []

    def choose_action(self, state):
        """Selecciona una acción basada en el estado actual usando la política"""
        state = torch.FloatTensor(state).unsqueeze(0)
        probabilities = self.policy(state)
        m = Categorical(probabilities)
        action = m.sample()
        self.log_probs.append(m.log_prob(action))
        return action.item()

    def store_reward(self, reward):
        """Almacena la recompensa"""
        self.rewards.append(reward)

    def discount_rewards(self, rewards, gamma=0.99):
        """Calcula las recompensas descontadas"""
        discounted_rewards = []
        running_add = 0
        for r in reversed(rewards):
            running_add = running_add * gamma + r
            discounted_rewards.insert(0, running_add)

        # Normalización de las recompensas
        discounted_rewards = torch.FloatTensor(discounted_rewards)
        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)
        return discounted_rewards

    def train(self, gamma=0.99):
        """Entrena el modelo usando el algoritmo de Policy Gradient"""
        if len(self.rewards) == 0:
            return

        # Calculamos las recompensas descontadas
        discounted_rewards = self.discount_rewards(self.rewards, gamma)

        # Calculamos la pérdida
        policy_loss = []
        for log_prob, R in zip(self.log_probs, discounted_rewards):
            policy_loss.append(-log_prob * R)

        policy_loss = torch.cat(policy_loss).sum()

        # Backpropagation
        self.optimizer.zero_grad()
        policy_loss.backward()
        self.optimizer.step()

        # Limpiamos la experiencia después del entrenamiento
        self.log_probs = []
        self.rewards = []

        return policy_loss.item()

# Función para entrenar el agente PyTorch
def train_pytorch_pg_agent(env, agent, n_episodes=200, max_steps=500, gamma=0.99,
                          print_interval=20, solving_threshold=195):
    """Entrena un agente con Policy Gradient en PyTorch"""
    history = {
        'episode_rewards': [],
        'episode_lengths': [],
        'losses': []
    }

    for episode in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_loss = 0

        for step in range(max_steps):
            action = agent.choose_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            agent.store_reward(reward)
            episode_reward += reward
            state = next_state

            if done:
                break

        # Entrenamos después de cada episodio
        loss = agent.train(gamma)
        if loss is not None:
            episode_loss = loss

        # Registramos los resultados
        history['episode_rewards'].append(episode_reward)
        history['episode_lengths'].append(step + 1)
        history['losses'].append(episode_loss)

        # Imprimimos el progreso
        if (episode + 1) % print_interval == 0:
            avg_reward = np.mean(history['episode_rewards'][-print_interval:])
            print(f"Episodio {episode + 1}: Recompensa media = {avg_reward:.2f}, Último episodio = {episode_reward:.2f}")

            # Verificamos si el entorno está resuelto
            if avg_reward >= solving_threshold:
                print(f"\nEntorno resuelto en el episodio {episode + 1}!")
                break

    return history

# Entrenamos al agente PyTorch
input_size = env.observation_space.shape[0]
output_size = env.action_space.n
pytorch_pg_agent = PyTorchPolicyGradientAgent(input_size, output_size)

# Usamos un número menor de episodios para demostración
pytorch_training_history = train_pytorch_pg_agent(env, pytorch_pg_agent, n_episodes=100, print_interval=10)

# Visualizamos el progreso del entrenamiento
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(pytorch_training_history['episode_rewards'])
plt.axhline(y=195, color='r', linestyle='--', label='Objetivo')
plt.xlabel('Episodio')
plt.ylabel('Recompensa total')
plt.title('Recompensa por episodio (PyTorch)')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(pytorch_training_history['episode_lengths'])
plt.xlabel('Episodio')
plt.ylabel('Duración del episodio')
plt.title('Duración del episodio (PyTorch)')

plt.tight_layout()
plt.show()

# Definimos la política del agente PyTorch
def pytorch_pg_policy(observation):
    """Política aprendida por el agente de Policy Gradient con PyTorch"""
    with torch.no_grad():
        state = torch.FloatTensor(observation).unsqueeze(0)
        probabilities = pytorch_pg_agent.policy(state)
        action = torch.argmax(probabilities).item()
    return action

# Evaluamos el agente PyTorch
pytorch_pg_metrics = evaluate_agent(pytorch_pg_policy, env, n_evaluations=20)

print("\nMétricas para la política aprendida (PyTorch Policy Gradient):")
for key, value in pytorch_pg_metrics.items():
    print(f"  {key}: {value:.2f}")

"""
## 9. Ejercicio Práctico: Agente Q-Learning para MountainCar

Vamos a implementar un algoritmo de Q-Learning para resolver el entorno MountainCar-v0,
donde un coche debe ganar suficiente impulso para llegar a la cima de una montaña.
"""

# Creamos el entorno MountainCar
mountain_car_env = gym.make('MountainCar-v0', render_mode="rgb_array")

# Obtenemos información sobre el entorno
print(f"Espacio de observación: {mountain_car_env.observation_space}")
print(f"Espacio de acción: {mountain_car_env.action_space}")
print(f"Recompensa por paso: -1.0 (penalización por tiempo en MountainCar-v0)")
print(f"Objetivo: Alcanzar la bandera con la menor cantidad de pasos posible")

# Renderizamos el estado inicial
observation, info = mountain_car_env.reset(seed=42)
print(f"Observación inicial: {observation}")

# Función para renderizar el entorno MountainCar
def render_mountain_car(env):
    img = env.render()
    plt.figure(figsize=(8, 6))
    plt.imshow(img)
    plt.axis('off')
    plt.show()

# Mostramos el estado inicial
render_mountain_car(mountain_car_env)

class QLearningAgent:
    def __init__(self, observation_space, action_space, learning_rate=0.1,
                 discount_factor=0.95, exploration_rate=1.0,
                 exploration_decay=0.995, min_exploration_rate=0.01):

        # Discretizamos el espacio de observación para crear la tabla Q
        self.obs_bins = [20, 20]  # Número de bins para cada dimensión
        self.obs_high = observation_space.high
        self.obs_low = observation_space.low

        # Inicializamos la tabla Q con valores cero
        self.q_table = np.zeros(self.obs_bins + [action_space.n])

        # Hiperparámetros
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = exploration_rate
        self.epsilon_decay = exploration_decay
        self.epsilon_min = min_exploration_rate

        self.action_space = action_space

    def discretize_state(self, state):
        """Convierte un estado continuo en un índice discreto para la tabla Q"""
        state_adj = (state - self.obs_low) / (self.obs_high - self.obs_low)
        state_adj = np.clip(state_adj, 0, 0.999)
        discretized = np.array(
            [int(s * b) for s, b in zip(state_adj, self.obs_bins)],
            dtype=int
        )
        return tuple(discretized)

    def choose_action(self, state):
        """Selecciona una acción usando la política epsilon-greedy"""
        if np.random.random() < self.epsilon:
            return self.action_space.sample()  # Exploración
        else:
            discretized_state = self.discretize_state(state)
            # Explotación: selecciona la acción con mayor valor Q
            return np.argmax(self.q_table[discretized_state])

    def learn(self, state, action, reward, next_state, done):
        """Actualiza la tabla Q usando la ecuación de Q-Learning"""
        discretized_state = self.discretize_state(state)
        discretized_next_state = self.discretize_state(next_state)

        # Valor Q actual
        q_current = self.q_table[discretized_state][action]

        # Mejor valor Q para el siguiente estado
        if done:
            q_target = reward
        else:
            q_target = reward + self.gamma * np.max(self.q_table[discretized_next_state])

        # Actualiza el valor Q
        self.q_table[discretized_state][action] += self.lr * (q_target - q_current)

        # Disminuye la tasa de exploración
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# Función para entrenar el agente Q-Learning
def train_q_learning_agent(env, agent, n_episodes=500, max_steps=200):
    """Entrena un agente Q-Learning en el entorno"""
    history = {
        'episode_rewards': [],
        'episode_lengths': [],
        'exploration_rates': []
    }

    for episode in range(n_episodes):
        state, info = env.reset()
        total_reward = 0

        for step in range(max_steps):
            action = agent.choose_action(state)
            next_state, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

            # Para MountainCar, modificamos la recompensa para acelerar el aprendizaje
            # Damos una recompensa positiva basada en la altura (posición) y velocidad
            position, velocity = next_state
            # Recompensa por altura: cuanto más alto, mejor
            reward_height = position - (-0.5)  # -0.5 es aproximadamente el punto más bajo
            # Recompensa por velocidad en la dirección correcta
            reward_velocity = abs(velocity) * np.sign(position * velocity)

            modified_reward = reward + 10*reward_height + 5*reward_velocity

            agent.learn(state, action, modified_reward, next_state, done)
            state = next_state
            total_reward += reward

            if done:
                break

        # Registramos los resultados
        history['episode_rewards'].append(total_reward)
        history['episode_lengths'].append(step + 1)
        history['exploration_rates'].append(agent.epsilon)

        # Imprimimos el progreso cada 100 episodios
        if (episode + 1) % 50 == 0:
            avg_reward = np.mean(history['episode_rewards'][-50:])
            print(f"Episodio {episode + 1}: Recompensa media = {avg_reward:.2f}, Epsilon = {agent.epsilon:.3f}")

            # Si resolvemos el entorno, terminamos el entrenamiento
            if avg_reward >= -160:  # MountainCar se considera resuelto con recompensa >= -160
                print(f"\nEntorno casi resuelto en el episodio {episode + 1}!")
                break

    return history

# Inicializamos y entrenamos el agente Q-Learning
q_agent = QLearningAgent(
    mountain_car_env.observation_space,
    mountain_car_env.action_space,
    learning_rate=0.1,
    discount_factor=0.99,
    exploration_rate=1.0,
    exploration_decay=0.995
)

# Entrenamos el agente (esto puede tomar algunos minutos)
# Usamos menos episodios para demostración
q_learning_history = train_q_learning_agent(mountain_car_env, q_agent, n_episodes=300)

# Visualizamos el progreso del entrenamiento
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.plot(q_learning_history['episode_rewards'])
plt.xlabel('Episodio')
plt.ylabel('Recompensa total')
plt.title('Recompensa por episodio')

plt.subplot(1, 3, 2)
plt.plot(q_learning_history['episode_lengths'])
plt.xlabel('Episodio')
plt.ylabel('Duración del episodio')
plt.title('Duración del episodio')

plt.subplot(1, 3, 3)
plt.plot(q_learning_history['exploration_rates'])
plt.xlabel('Episodio')
plt.ylabel('Tasa de exploración (ε)')
plt.title('Tasa de exploración vs episodio')

plt.tight_layout()
plt.show()

# Definimos la política del agente Q-Learning
def q_learning_policy(state):
    """Política aprendida por el agente Q-Learning"""
    discretized_state = q_agent.discretize_state(state)
    return np.argmax(q_agent.q_table[discretized_state])

# Ejecutamos un episodio con la política aprendida
q_episode_data = run_episode(mountain_car_env, q_learning_policy, render=True)

print(f"Duración del episodio: {q_episode_data['steps']} pasos")
print(f"Recompensa total: {q_episode_data['cumulative_reward']}")

# Mostramos una animación del episodio
max_frames = min(200, len(q_episode_data['frames']))
display_episode_animation(q_episode_data['frames'][:max_frames])

"""
## Conclusiones

En este módulo, hemos aprendido los conceptos básicos del aprendizaje por refuerzo utilizando Gymnasium:

1. Instalamos y configuramos entornos de simulación
2. Exploramos los espacios de observación y acción
3. Implementamos agentes con políticas simples (aleatorias y heurísticas)
4. Ejecutamos episodios completos y recolectamos datos
5. Evaluamos el desempeño de los agentes
6. Integramos bibliotecas como TensorFlow y PyTorch para implementar agentes avanzados
7. Desarrollamos un agente de control en el entorno MountainCar-v0

Estos conceptos son fundamentales para el desarrollo de algoritmos de aprendizaje por refuerzo más avanzados
que pueden abordar problemas complejos en diversas áreas como robótica, juegos, optimización de sistemas
y muchas otras aplicaciones prácticas.

## Ejercicios adicionales

1. Implementa un agente DQN (Deep Q-Network) para resolver el entorno CartPole-v1
2. Adapta el agente Policy Gradient para resolver el entorno LunarLander-v2
3. Experimenta con diferentes arquitecturas de redes neuronales y hiperparámetros
4. Implementa una política que combine conocimiento experto con aprendizaje automático
5. Desarrolla un agente que pueda transferir conocimiento entre diferentes entornos

## Referencias y recursos adicionales

- [Documentación oficial de Gymnasium](https://gymnasium.farama.org/)
- [Sutton & Barto: Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html)
- [Deep Reinforcement Learning Tutorial](https://spinningup.openai.com/)
- [TensorFlow RL Tutorial](https://www.tensorflow.org/tutorials/reinforcement_learning)
- [PyTorch RL Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)
"""

"""**Ejercicio 1: Implementación de un agente DQN (Deep Q-Network) para CartPole-v1**

El algoritmo DQN, desarrollado por DeepMind, constituyó un avance revolucionario en el aprendizaje por refuerzo al combinar Q-learning con redes neuronales profundas. Este enfoque permitió superar las limitaciones de las representaciones tabulares en espacios de estados continuos.

**Componentes clave del algoritmo DQN:**

*Redes Neuronal Q*: Aproxima la función Q(s,a) usando una red neuronal

*Memoria de Experiencia (Experience Replay):* Almacena y reutiliza transiciones pasadas

*Red Objetivo (Target Network):* Estabiliza el entrenamiento

*Exploración ε-greedy:* Balance entre exploración y explotación
"""

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
import matplotlib.pyplot as plt
from collections import deque

# Configuración de semillas para reproducibilidad
np.random.seed(42)
torch.manual_seed(42)
random.seed(42)

# Definimos la arquitectura de la red DQN
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim)
        )

    def forward(self, x):
        return self.network(x)

# Buffer de experiencia para almacenar y muestrear transiciones
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def add(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        # Convertir a tensores de PyTorch
        states = torch.FloatTensor(np.array(states))
        actions = torch.LongTensor(np.array(actions))
        rewards = torch.FloatTensor(np.array(rewards))
        next_states = torch.FloatTensor(np.array(next_states))
        dones = torch.FloatTensor(np.array(dones))

        return states, actions, rewards, next_states, dones

    def __len__(self):
        return len(self.buffer)

# Agente DQN
class DQNAgent:
    def __init__(self, state_dim, action_dim, learning_rate=0.001, gamma=0.99,
                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,
                 buffer_size=10000, batch_size=64, target_update=10):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma  # Factor de descuento

        # Parámetros para exploración épsilon-greedy
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay

        # Tamaño de lote para entrenamiento
        self.batch_size = batch_size

        # Frecuencia de actualización de la red objetivo
        self.target_update = target_update
        self.update_counter = 0

        # Memoria de experiencias
        self.memory = ReplayBuffer(buffer_size)

        # Redes Q (principal y objetivo)
        self.q_network = DQN(state_dim, action_dim)
        self.target_network = DQN(state_dim, action_dim)
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.target_network.eval()  # Modo de evaluación (no calculamos gradientes)

        # Optimizador
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

        # Función de pérdida
        self.criterion = nn.MSELoss()

    def select_action(self, state):
        """Selecciona una acción usando la política épsilon-greedy"""
        if np.random.rand() < self.epsilon:
            return random.randrange(self.action_dim)  # Exploración

        # Explotación: selecciona la acción con mayor valor Q
        state = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            q_values = self.q_network(state)
        return q_values.argmax().item()

    def update_epsilon(self):
        """Actualiza el valor de épsilon (decae con el tiempo)"""
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

    def train(self):
        """Entrena la red con un lote de experiencias"""
        if len(self.memory) < self.batch_size:
            return 0  # No hay suficientes datos para entrenar

        # Muestrear lote de experiencias
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)

        # Calcular valores Q actuales (para las acciones tomadas)
        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        # Calcular valores Q objetivo
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values

        # Calcular pérdida y optimizar
        loss = self.criterion(q_values, target_q_values)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # Actualizar la red objetivo periódicamente
        self.update_counter += 1
        if self.update_counter % self.target_update == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())

        return loss.item()

# Función para entrenar el agente
def train_dqn(env, agent, n_episodes=500, max_steps=500, print_interval=20):
    """Entrena un agente DQN y devuelve el historial de entrenamiento"""
    episode_rewards = []
    episode_lengths = []
    losses = []

    # Métricas para verificar si resolvemos el entorno
    last_100_rewards = deque(maxlen=100)

    for episode in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_loss = 0

        for step in range(max_steps):
            # Seleccionar y ejecutar acción
            action = agent.select_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # Guardar experiencia en el buffer
            agent.memory.add(state, action, reward, next_state, done)

            # Actualizar estado actual y acumular recompensa
            state = next_state
            episode_reward += reward

            # Entrenar la red
            loss = agent.train()
            if loss > 0:
                episode_loss += loss

            if done:
                break

        # Actualizar épsilon para el siguiente episodio
        agent.update_epsilon()

        # Registrar resultados del episodio
        episode_rewards.append(episode_reward)
        episode_lengths.append(step + 1)
        losses.append(episode_loss / (step + 1) if step > 0 else 0)

        # Guardar para calcular la media de los últimos 100 episodios
        last_100_rewards.append(episode_reward)
        mean_100_reward = np.mean(last_100_rewards)

        # Imprimir progreso
        if (episode + 1) % print_interval == 0:
            print(f"Episodio {episode + 1}/{n_episodes}, Recompensa: {episode_reward:.2f}, "
                  f"Media últimos 100: {mean_100_reward:.2f}, Epsilon: {agent.epsilon:.3f}")

            # Verificar si hemos resuelto el entorno (CartPole-v1 se considera resuelto con recompensa ≥ 475)
            if mean_100_reward >= 475 and len(last_100_rewards) >= 100:
                print(f"\n¡Entorno resuelto en el episodio {episode + 1}!")
                break

    return {
        'episode_rewards': episode_rewards,
        'episode_lengths': episode_lengths,
        'losses': losses
    }

# Función principal para ejecutar el experimento
def run_dqn_experiment():
    # Crear entorno
    env = gym.make('CartPole-v1')

    # Obtener dimensiones de estado y acción
    state_dim = env.observation_space.shape[0]  # 4 para CartPole
    action_dim = env.action_space.n  # 2 para CartPole

    # Crear agente
    agent = DQNAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        learning_rate=0.001,
        gamma=0.99,
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.995,
        buffer_size=10000,
        batch_size=64,
        target_update=10
    )

    # Entrenar agente
    print("Iniciando entrenamiento DQN para CartPole-v1...")
    history = train_dqn(env, agent, n_episodes=500, print_interval=20)

    # Visualizar resultados
    plt.figure(figsize=(15, 5))

    # Gráfico de recompensas
    plt.subplot(1, 3, 1)
    plt.plot(history['episode_rewards'])
    plt.axhline(y=475, color='r', linestyle='--', label='Objetivo')
    plt.xlabel('Episodio')
    plt.ylabel('Recompensa')
    plt.title('Recompensa por episodio')
    plt.legend()

    # Gráfico de duración de episodios
    plt.subplot(1, 3, 2)
    plt.plot(history['episode_lengths'])
    plt.xlabel('Episodio')
    plt.ylabel('Pasos')
    plt.title('Duración del episodio')

    # Gráfico de pérdida
    plt.subplot(1, 3, 3)
    plt.plot(history['losses'])
    plt.xlabel('Episodio')
    plt.ylabel('Pérdida')
    plt.title('Pérdida promedio por episodio')

    plt.tight_layout()
    plt.show()

    # Guardar modelo entrenado
    torch.save(agent.q_network.state_dict(), 'dqn_cartpole.pth')
    print("Modelo guardado como 'dqn_cartpole.pth'")

    # Visualizar política aprendida
    visualize_policy(env, agent)

    return agent, history

# Función para visualizar la política aprendida
def visualize_policy(env, agent, n_episodes=3):
    """Visualiza algunos episodios con la política aprendida"""
    print("\nVisualizando política aprendida...")

    for episode in range(n_episodes):
        state, _ = env.reset()
        total_reward = 0
        done = False

        print(f"\nEpisodio de demostración {episode + 1}:")
        step = 0

        while not done and step < 500:
            # Seleccionar acción sin exploración (greedy)
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            with torch.no_grad():
                q_values = agent.q_network(state_tensor)
            action = q_values.argmax().item()

            # Ejecutar acción
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # Imprimir información
            print(f"Paso {step+1}: Estado = {state}, Acción = {action}, Q-valores = {q_values.numpy()[0]}")

            state = next_state
            total_reward += reward
            step += 1

        print(f"Recompensa total: {total_reward}")

# Ejecutar experimento
if __name__ == "__main__":
    agent, history = run_dqn_experiment()

"""**Elementos destacados de esta implementación:**

1. Arquitectura DQN: Utilizamos una red neuronal con dos capas ocultas de 64 neuronas cada una, que resulta suficiente para aprender la dinámica de CartPole.
2. Experience Replay: Almacenamos transiciones (estado, acción, recompensa, siguiente estado, terminado) en un buffer circular, lo que permite:

Romper correlaciones temporales entre muestras

Utilizar cada experiencia múltiples veces para aprendizaje eficiente

Mejorar la estabilidad del entrenamiento

3. Target Network: Mantenemos una copia independiente de la red Q para calcular los valores objetivo, actualizándola periódicamente. Esto previene inestabilidades causadas por la actualización simultánea de los valores Q actuales y objetivo.
4. Exploración Decreciente: Implementamos una estrategia épsilon-greedy con decaimiento, comenzando con exploración alta (ε=1.0) y disminuyendo gradualmente hasta un mínimo (ε=0.01).
5. Monitoreo de Progreso: Verificamos continuamente si el entorno se considera resuelto (recompensa media ≥475 durante 100 episodios), permitiendo detener el entrenamiento temprano si se alcanza el objetivo.

**Ejercicio 2: Adaptar el agente Policy Gradient para resolver LunarLander-v2**

LunarLander-v2 representa un entorno significativamente más complejo que CartPole-v1, con un espacio de observación de 8 dimensiones y 4 acciones posibles. Este ejercicio demuestra cómo adaptar un algoritmo de Policy Gradient para abordar este desafío.
"""

# Primero instalamos las dependencias necesarias para Box2D
!pip install swig
!pip install gymnasium[box2d]

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import matplotlib.pyplot as plt
import time

# Verificamos que podemos importar el entorno correctamente
try:
    test_env = gym.make('LunarLander-v3')
    test_env.close()
    print("LunarLander-v3 importado correctamente!")
except Exception as e:
    print(f"Error al importar LunarLander-v3: {e}")
    print("Intentando alternativas...")

# Configuración de semillas para reproducibilidad
np.random.seed(42)
torch.manual_seed(42)

# Red neuronal para el agente Policy Gradient adaptada para LunarLander
class PolicyNetwork(nn.Module):
    def __init__(self, input_size, output_size, hidden_size=128):
        super(PolicyNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.network(x)

# Agente Policy Gradient adaptado para LunarLander
class LunarLanderPolicyGradient:
    def __init__(self, input_size, output_size, hidden_size=128, learning_rate=0.002):
        self.policy = PolicyNetwork(input_size, output_size, hidden_size)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)

        # Para almacenar la experiencia de cada episodio
        self.log_probs = []
        self.rewards = []

    def choose_action(self, state):
        """Selecciona una acción basada en el estado actual usando la política"""
        state = torch.FloatTensor(state).unsqueeze(0)
        probabilities = self.policy(state)
        distribution = Categorical(probabilities)
        action = distribution.sample()
        self.log_probs.append(distribution.log_prob(action))
        return action.item()

    def store_reward(self, reward):
        """Almacena la recompensa"""
        self.rewards.append(reward)

    def discount_rewards(self, rewards, gamma=0.99):
        """Calcula las recompensas descontadas"""
        discounted_rewards = []
        running_add = 0
        for r in reversed(rewards):
            running_add = running_add * gamma + r
            discounted_rewards.insert(0, running_add)

        # Normalización de las recompensas
        discounted_rewards = torch.FloatTensor(discounted_rewards)
        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)
        return discounted_rewards

    def train(self, gamma=0.99):
        """Entrena el modelo usando el algoritmo de Policy Gradient"""
        if len(self.rewards) == 0:
            return

        # Calculamos las recompensas descontadas
        discounted_rewards = self.discount_rewards(self.rewards, gamma)

        # Calculamos la pérdida
        policy_loss = []
        for log_prob, R in zip(self.log_probs, discounted_rewards):
            policy_loss.append(-log_prob * R)  # Maximizar retorno esperado

        policy_loss = torch.cat(policy_loss).sum()

        # Backpropagation
        self.optimizer.zero_grad()
        policy_loss.backward()

        # Recortar gradientes para evitar explosión
        nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=0.5)

        self.optimizer.step()

        # Limpiamos la experiencia después del entrenamiento
        self.log_probs = []
        self.rewards = []

        return policy_loss.item()

# Función para entrenar el agente (reducimos el número de episodios para demostración)
def train_lunar_lander(env, agent, n_episodes=300, max_steps=1000, gamma=0.99,
                      print_interval=20, solving_threshold=200):
    """Entrena un agente con Policy Gradient en LunarLander"""
    history = {
        'episode_rewards': [],
        'episode_lengths': [],
        'losses': []
    }

    # Para determinar si el entorno está resuelto
    last_100_rewards = []
    start_time = time.time()

    for episode in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        episode_loss = 0

        for step in range(max_steps):
            action = agent.choose_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            agent.store_reward(reward)
            episode_reward += reward
            state = next_state

            if done:
                break

        # Entrenar al final del episodio
        loss = agent.train(gamma)
        if loss is not None:
            episode_loss = loss

        # Registrar resultados del episodio
        history['episode_rewards'].append(episode_reward)
        history['episode_lengths'].append(step + 1)
        history['losses'].append(episode_loss)

        # Calcular media de últimos 100 episodios
        last_100_rewards.append(episode_reward)
        if len(last_100_rewards) > 100:
            last_100_rewards.pop(0)
        mean_100_reward = np.mean(last_100_rewards)

        # Imprimir progreso
        if (episode + 1) % print_interval == 0:
            elapsed = time.time() - start_time
            print(f"Episodio {episode + 1}/{n_episodes}: Recompensa = {episode_reward:.2f}, "
                  f"Media últimos {len(last_100_rewards)} = {mean_100_reward:.2f}, "
                  f"Tiempo transcurrido: {elapsed:.1f}s")

            # Verificar si hemos resuelto el entorno
            if mean_100_reward >= solving_threshold and len(last_100_rewards) >= 100:
                print(f"\n¡Entorno resuelto en el episodio {episode + 1}!")
                break

    total_time = time.time() - start_time
    print(f"Entrenamiento completado en {total_time:.2f} segundos")
    return history

# Función principal para ejecutar el experimento
def run_lunar_lander_experiment():
    try:
        # Crear entorno
        env = gym.make('LunarLander-v3')
        env_name = 'LunarLander-v3'
    except Exception as e:
        print(f"No se pudo crear LunarLander-v3: {e}")
        print("Usando CartPole-v1 como alternativa")
        env = gym.make('CartPole-v1')
        env_name = 'CartPole-v1'

    # Obtener dimensiones de estado y acción
    input_size = env.observation_space.shape[0]
    output_size = env.action_space.n

    print(f"Entorno: {env_name}")
    print(f"Espacio de observación: {env.observation_space}, dimensión: {input_size}")
    print(f"Espacio de acción: {env.action_space}, dimensión: {output_size}")

    # Crear agente con hiperparámetros adaptados
    agent = LunarLanderPolicyGradient(
        input_size=input_size,
        output_size=output_size,
        hidden_size=128,
        learning_rate=0.002
    )

    # Entrenar agente (con menos episodios para demostración)
    print("\nIniciando entrenamiento...")
    history = train_lunar_lander(
        env=env,
        agent=agent,
        n_episodes=300,  # Reducido para demostración
        gamma=0.99,
        print_interval=20,
        solving_threshold=200 if env_name == 'LunarLander-v3' else 475  # Umbral adaptado al entorno
    )

    # Visualizar resultados
    plt.figure(figsize=(15, 5))

    # Gráfico de recompensas
    plt.subplot(1, 2, 1)
    plt.plot(history['episode_rewards'])
    plt.axhline(y=200 if env_name == 'LunarLander-v3' else 475,
                color='r', linestyle='--', label='Objetivo')
    plt.xlabel('Episodio')
    plt.ylabel('Recompensa')
    plt.title(f'Recompensa por episodio - {env_name}')
    plt.legend()

    # Media móvil
    rewards = np.array(history['episode_rewards'])
    window = min(100, len(rewards))
    if window > 0:
        moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')
        plt.plot(range(window-1, len(rewards)), moving_avg, 'g-', label=f'Media móvil ({window})')
        plt.legend()

    # Gráfico de pérdida
    plt.subplot(1, 2, 2)
    plt.plot(history['losses'])
    plt.xlabel('Episodio')
    plt.ylabel('Pérdida')
    plt.title('Pérdida por episodio')

    plt.tight_layout()
    plt.show()

    print("\nDemostración de la política aprendida:")
    demonstrate_policy(env, agent)

    env.close()
    return agent, history

# Función para demostrar la política aprendida
def demonstrate_policy(env, agent, n_episodes=3):
    """Demuestra la política aprendida en varios episodios"""
    for episode in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        steps = 0
        done = False

        while not done and steps < 1000:
            # Seleccionar acción según la política aprendida
            action = agent.choose_action(state)
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            episode_reward += reward
            steps += 1

        print(f"Episodio {episode + 1}: Recompensa = {episode_reward:.2f}, Pasos = {steps}")

# Ejecutar experimento
if __name__ == "__main__":
    agent, history = run_lunar_lander_experiment()

"""**Ejercicio 3: Experimentar con diferentes arquitecturas de redes neuronales y hiperparámetros**

La selección de arquitectura y ajuste de hiperparámetros resulta crucial para el rendimiento de los algoritmos de aprendizaje por refuerzo. Este ejercicio presenta un enfoque sistemático para experimentar con diferentes configuraciones, utilizando CartPole como entorno de prueba.
"""

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import matplotlib.pyplot as plt
from collections import deque
import itertools
import time

# Configuración para reproducibilidad
np.random.seed(42)
torch.manual_seed(42)

# Definición de arquitecturas de red neuronal con diferentes configuraciones
class PolicyNetwork(nn.Module):
    def __init__(self, input_size, output_size, hidden_sizes, activation_fn=nn.ReLU):
        super(PolicyNetwork, self).__init__()

        # Construcción dinámica de capas según la configuración
        layers = []
        prev_size = input_size

        for h_size in hidden_sizes:
            layers.append(nn.Linear(prev_size, h_size))
            layers.append(activation_fn())
            prev_size = h_size

        # Capa de salida con softmax
        layers.append(nn.Linear(prev_size, output_size))
        layers.append(nn.Softmax(dim=1))

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# Agente Policy Gradient configurable
class ConfigurablePolicyGradient:
    def __init__(self, input_size, output_size, config):
        self.config = config

        # Configuración de la red
        self.policy = PolicyNetwork(
            input_size,
            output_size,
            hidden_sizes=config['hidden_sizes'],
            activation_fn=config['activation_fn']
        )

        # Configuración del optimizador
        if config['optimizer'] == 'Adam':
            self.optimizer = optim.Adam(self.policy.parameters(), lr=config['learning_rate'])
        elif config['optimizer'] == 'RMSprop':
            self.optimizer = optim.RMSprop(self.policy.parameters(), lr=config['learning_rate'])
        elif config['optimizer'] == 'SGD':
            self.optimizer = optim.SGD(self.policy.parameters(), lr=config['learning_rate'])

        # Para almacenar la experiencia
        self.log_probs = []
        self.rewards = []

    def choose_action(self, state):
        """Selecciona acción según la política actual"""
        state = torch.FloatTensor(state).unsqueeze(0)
        probabilities = self.policy(state)
        distribution = Categorical(probabilities)
        action = distribution.sample()
        self.log_probs.append(distribution.log_prob(action))
        return action.item()

    def store_reward(self, reward):
        """Almacena recompensa"""
        self.rewards.append(reward)

    def discount_rewards(self, rewards, gamma):
        """Calcula recompensas descontadas"""
        discounted_rewards = []
        running_add = 0
        for r in reversed(rewards):
            running_add = running_add * gamma + r
            discounted_rewards.insert(0, running_add)

        # Normalización
        discounted_rewards = torch.FloatTensor(discounted_rewards)
        if len(discounted_rewards) > 1:  # Evitar división por cero
            discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)
        return discounted_rewards

    def train(self):
        """Entrena la red según algoritmo Policy Gradient"""
        if len(self.rewards) == 0:
            return

        # Calcular recompensas descontadas
        gamma = self.config['gamma']
        discounted_rewards = self.discount_rewards(self.rewards, gamma)

        # Calcular pérdida
        policy_loss = []
        for log_prob, R in zip(self.log_probs, discounted_rewards):
            policy_loss.append(-log_prob * R)

        policy_loss = torch.cat(policy_loss).sum()

        # Actualizar pesos
        self.optimizer.zero_grad()
        policy_loss.backward()

        # Opcional: Recorte de gradientes
        if self.config['clip_gradients']:
            nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=1.0)

        self.optimizer.step()

        # Limpiar experiencia
        self.log_probs = []
        self.rewards = []

        return policy_loss.item()

# Función de evaluación para una configuración específica
def evaluate_configuration(env_name, config, n_episodes=200, print_interval=50):
    """Evalúa una configuración específica de hiperparámetros"""
    # Crear entorno
    env = gym.make(env_name)

    # Dimensiones del problema
    input_size = env.observation_space.shape[0]
    output_size = env.action_space.n

    # Inicializar agente
    agent = ConfigurablePolicyGradient(input_size, output_size, config)

    # Historial de entrenamiento
    history = {
        'episode_rewards': [],
        'episode_lengths': [],
        'solved_episode': None
    }

    # Para determinar solución del entorno
    last_100_rewards = deque(maxlen=100)
    solving_threshold = 475  # Para CartPole-v1

    start_time = time.time()

    for episode in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0

        for step in range(1000):  # Límite alto para evitar truncamiento
            action = agent.choose_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            agent.store_reward(reward)
            episode_reward += reward
            state = next_state

            if done:
                break

        # Entrenar al final del episodio
        agent.train()

        # Registro
        history['episode_rewards'].append(episode_reward)
        history['episode_lengths'].append(step + 1)

        # Verificar solución
        last_100_rewards.append(episode_reward)
        if len(last_100_rewards) >= 100:
            mean_100 = np.mean(last_100_rewards)
            if mean_100 >= solving_threshold and history['solved_episode'] is None:
                history['solved_episode'] = episode
                if print_interval > 0:
                    print(f"¡Configuración resuelta en episodio {episode}!")
                break

        # Mostrar progreso
        if print_interval > 0 and (episode + 1) % print_interval == 0:
            mean_reward = np.mean(history['episode_rewards'][-print_interval:])
            print(f"Episodio {episode + 1}: Recompensa media = {mean_reward:.2f}")

    # Registrar tiempo total
    history['training_time'] = time.time() - start_time

    env.close()
    return history

# Función para explorar espacio de hiperparámetros
def hyperparameter_search(env_name='CartPole-v1', n_episodes=500):
    """Realiza una búsqueda de hiperparámetros y retorna los resultados"""

    # Definir espacio de búsqueda
    param_grid = {
        'hidden_sizes': [
            [16],
            [32],
            [64],
            [128],
            [32, 32],
            [64, 64],
            [128, 64],
            [64, 32, 16]
        ],
        'activation_fn': [nn.ReLU, nn.Tanh, nn.ELU],
        'optimizer': ['Adam', 'RMSprop'],
        'learning_rate': [0.01, 0.001, 0.0005],
        'gamma': [0.99, 0.999],
        'clip_gradients': [True, False]
    }

    # Seleccionar un subconjunto de configuraciones para prueba
    # En una búsqueda completa, se probarían todas las combinaciones
    configurations = [
        {'hidden_sizes': [32], 'activation_fn': nn.ReLU, 'optimizer': 'Adam',
         'learning_rate': 0.01, 'gamma': 0.99, 'clip_gradients': False},

        {'hidden_sizes': [64, 64], 'activation_fn': nn.ReLU, 'optimizer': 'Adam',
         'learning_rate': 0.001, 'gamma': 0.99, 'clip_gradients': True},

        {'hidden_sizes': [128, 64], 'activation_fn': nn.Tanh, 'optimizer': 'RMSprop',
         'learning_rate': 0.001, 'gamma': 0.999, 'clip_gradients': True},

        {'hidden_sizes': [64, 32, 16], 'activation_fn': nn.ELU, 'optimizer': 'Adam',
         'learning_rate': 0.0005, 'gamma': 0.99, 'clip_gradients': True}
    ]

    results = []

    print(f"Iniciando búsqueda de hiperparámetros con {len(configurations)} configuraciones...")

    for i, config in enumerate(configurations):
        print(f"\nEvaluando configuración {i+1}/{len(configurations)}:")
        for key, value in config.items():
            print(f"  {key}: {value}")

        # Evaluar configuración
        history = evaluate_configuration(env_name, config, n_episodes, print_interval=100)

        # Guardar resultados
        results.append({
            'config': config,
            'mean_reward': np.mean(history['episode_rewards'][-100:]),
            'solved_episode': history['solved_episode'],
            'training_time': history['training_time']
        })

        print(f"Resultados:")
        print(f"  Recompensa media (últimos 100): {results[-1]['mean_reward']:.2f}")
        print(f"  Episodio de solución: {results[-1]['solved_episode'] if results[-1]['solved_episode'] is not None else 'No resuelto'}")
        print(f"  Tiempo de entrenamiento: {results[-1]['training_time']:.2f} segundos")

    return results

# Función para visualizar resultados
def visualize_hyperparameter_results(results):
    """Visualiza los resultados de la búsqueda de hiperparámetros"""
    # Ordenar por desempeño
    sorted_results = sorted(results, key=lambda x: (
        -float('inf') if x['solved_episode'] is None else x['solved_episode']
    ))

    # Preparar datos para gráficos
    configs = [str(i+1) for i in range(len(sorted_results))]
    mean_rewards = [r['mean_reward'] for r in sorted_results]
    solved_episodes = []
    for r in sorted_results:
        if r['solved_episode'] is not None:
            solved_episodes.append(r['solved_episode'])
        else:
            solved_episodes.append(None)  # No se resolvió

    training_times = [r['training_time'] for r in sorted_results]

    # Visualización
    plt.figure(figsize=(15, 10))

    # Recompensa media
    plt.subplot(2, 2, 1)
    plt.bar(configs, mean_rewards)
    plt.xlabel('Configuración')
    plt.ylabel('Recompensa media (últimos 100)')
    plt.title('Recompensa media por configuración')

    # Episodio de solución
    plt.subplot(2, 2, 2)
    valid_indices = [i for i, x in enumerate(solved_episodes) if x is not None]
    valid_configs = [configs[i] for i in valid_indices]
    valid_episodes = [solved_episodes[i] for i in valid_indices]

    if valid_episodes:
        plt.bar(valid_configs, valid_episodes)
        plt.xlabel('Configuración')
        plt.ylabel('Episodio de solución')
        plt.title('Episodio en que se resolvió el entorno')
    else:
        plt.text(0.5, 0.5, 'Ninguna configuración resolvió el entorno',
                 horizontalalignment='center', verticalalignment='center')

    # Tiempo de entrenamiento
    plt.subplot(2, 2, 3)
    plt.bar(configs, training_times)
    plt.xlabel('Configuración')
    plt.ylabel('Tiempo (segundos)')
    plt.title('Tiempo de entrenamiento')

    # Tabla de configuraciones
    plt.subplot(2, 2, 4)
    plt.axis('off')
    config_table = "Configuraciones:\n\n"
    for i, result in enumerate(sorted_results):
        config = result['config']
        config_table += f"Config {i+1}:\n"
        config_table += f"  Capas: {config['hidden_sizes']}\n"
        config_table += f"  Activación: {config['activation_fn'].__name__}\n"
        config_table += f"  Optimizador: {config['optimizer']}\n"
        config_table += f"  Tasa aprendizaje: {config['learning_rate']}\n"
        config_table += f"  Gamma: {config['gamma']}\n"
        config_table += f"  Recorte gradientes: {config['clip_gradients']}\n\n"

    plt.text(0, 1, config_table, fontsize=9, verticalalignment='top')

    plt.tight_layout()
    plt.show()

# Función principal
def run_architecture_experiment():
    print("Iniciando experimento de arquitecturas y hiperparámetros...")
    results = hyperparameter_search('CartPole-v1', n_episodes=500)
    visualize_hyperparameter_results(results)

    # Identificar mejor configuración
    best_config = min(
        [r for r in results if r['solved_episode'] is not None],
        key=lambda x: x['solved_episode'],
        default=max(results, key=lambda x: x['mean_reward'])
    )

    print("\nMejor configuración encontrada:")
    for key, value in best_config['config'].items():
        print(f"  {key}: {value}")

    print(f"Recompensa media: {best_config['mean_reward']:.2f}")
    print(f"Episodio de solución: {best_config['solved_episode']}")
    print(f"Tiempo de entrenamiento: {best_config['training_time']:.2f} segundos")

    return best_config

# Ejecutar experimento
if __name__ == "__main__":
    best_config = run_architecture_experiment()

"""**Aspectos destacados del enfoque de experimentación:**

**Arquitectura configurable:** Implementamos una clase PolicyNetwork que permite definir redes con diferentes profundidades, anchuras y funciones de activación de forma dinámica.

**Espacio de hiperparámetros: Exploramos variaciones en:**

Profundidad y anchura de la red (desde una capa oculta con 16 neuronas hasta tres capas: 64→32→16)

Funciones de activación (ReLU, Tanh, ELU)

Optimizadores (Adam, RMSprop)

Tasas de aprendizaje (0.01, 0.001, 0.0005)

Factores de descuento (0.99, 0.999)

Recorte de gradientes (activado/desactivado)


**Métricas de evaluación:**

Recompensa media en los últimos 100 episodios

Episodio en que se resuelve el entorno (si aplica)

Tiempo de entrenamiento


Visualización comparativa: Gráficos que ilustran el rendimiento de cada configuración para facilitar la identificación de patrones y tendencias.

Optimización de recursos: En lugar de una búsqueda en cuadrícula exhaustiva (que sería computacionalmente prohibitiva), seleccionamos manualmente configuraciones representativas para explorar el espacio de hiperparámetros de manera eficiente.


Este enfoque sistemático permite identificar configuraciones óptimas mientras se comprenden las relaciones entre los diferentes hiperparámetros y su impacto en el rendimiento del agente.

**Ejercicio 4: Implementar una política que combine conocimiento experto con aprendizaje automático**

Este ejercicio integra reglas heurísticas con aprendizaje por refuerzo, creando un agente híbrido que aprovecha tanto el conocimiento humano como la capacidad de aprendizaje automático.
"""

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import matplotlib.pyplot as plt
from collections import deque
import random

# Configuración de semillas
np.random.seed(42)
torch.manual_seed(42)
random.seed(42)

# Clase para el agente híbrido
class HybridAgent:
    def __init__(self, state_dim, action_dim, expert_policy_fn,
                 learning_rate=0.001, hidden_size=64,
                 initial_expert_weight=0.8, expert_decay=0.995,
                 min_expert_weight=0.1):
        """
        Inicializa un agente que combina política experta con aprendizaje

        Parámetros:
        - state_dim: Dimensión del espacio de estados
        - action_dim: Dimensión del espacio de acciones
        - expert_policy_fn: Función que define la política experta
        - learning_rate: Tasa de aprendizaje para la red neuronal
        - hidden_size: Tamaño de las capas ocultas
        - initial_expert_weight: Peso inicial de la política experta
        - expert_decay: Tasa de decaimiento para el peso experto
        - min_expert_weight: Peso mínimo para la política experta
        """
        self.expert_policy_fn = expert_policy_fn
        self.expert_weight = initial_expert_weight
        self.expert_decay = expert_decay
        self.min_expert_weight = min_expert_weight

        # Red para la política aprendida
        self.policy_network = nn.Sequential(
            nn.Linear(state_dim, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, action_dim),
            nn.Softmax(dim=1)
        )

        # Optimizador
        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)

        # Almacenamiento para experiencias
        self.states = []
        self.actions = []
        self.rewards = []
        self.log_probs = []

        # Estadísticas
        self.expert_actions_taken = 0
        self.learned_actions_taken = 0

    def decrease_expert_weight(self):
        """Reduce gradualmente la influencia del experto"""
        self.expert_weight = max(self.min_expert_weight,
                                 self.expert_weight * self.expert_decay)

    def choose_action(self, state, training=True):
        """
        Selecciona una acción combinando política experta y aprendida

        Durante el entrenamiento, mezcla las políticas según expert_weight.
        Durante la evaluación, utiliza solo la política aprendida.
        """
        if not training:
            # Modo de evaluación: usar solo la política aprendida
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                action_probs = self.policy_network(state_tensor)[0]
                return action_probs.argmax().item()

        # Decidir si usar política experta o aprendida
        if random.random() < self.expert_weight:
            # Usar política experta
            action = self.expert_policy_fn(state)
            self.expert_actions_taken += 1
        else:
            # Usar política aprendida
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            action_probs = self.policy_network(state_tensor)
            dist = Categorical(action_probs)
            action = dist.sample().item()
            self.learned_actions_taken += 1

        # Calcular log-probabilidad para el entrenamiento
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        action_probs = self.policy_network(state_tensor)
        dist = Categorical(action_probs)
        self.log_probs.append(dist.log_prob(torch.tensor(action)))

        # Almacenar estado y acción para entrenamiento
        self.states.append(state)
        self.actions.append(action)

        return action

    def store_reward(self, reward):
        """Almacena la recompensa recibida"""
        self.rewards.append(reward)

    def discount_rewards(self, gamma=0.99):
        """Calcula recompensas descontadas"""
        discounted_rewards = []
        running_add = 0
        for r in reversed(self.rewards):
            running_add = running_add * gamma + r
            discounted_rewards.insert(0, running_add)

        discounted_rewards = torch.FloatTensor(discounted_rewards)
        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)
        return discounted_rewards

    def train(self, gamma=0.99):
        """Entrena la red de política"""
        if len(self.rewards) == 0:
            return 0

        # Calcular recompensas descontadas
        discounted_rewards = self.discount_rewards(gamma)

        # Calcular pérdida de policy gradient
        policy_loss = []
        for log_prob, R in zip(self.log_probs, discounted_rewards):
            policy_loss.append(-log_prob * R)

        policy_loss = torch.cat(policy_loss).sum()

        # Actualizar pesos
        self.optimizer.zero_grad()
        policy_loss.backward()
        self.optimizer.step()

        # Limpiar memoria de episodio
        self.states = []
        self.actions = []
        self.rewards = []
        self.log_probs = []

        # Reducir peso del experto
        self.decrease_expert_weight()

        return policy_loss.item()

    def get_expert_percentage(self):
        """Calcula el porcentaje de acciones tomadas por el experto"""
        total = self.expert_actions_taken + self.learned_actions_taken
        if total == 0:
            return 0
        return 100 * self.expert_actions_taken / total

# Definición de la política experta para CartPole
def expert_cartpole_policy(state):
    """
    Política experta avanzada para CartPole

    Considera tanto el ángulo como la velocidad angular
    """
    x, x_dot, theta, theta_dot = state

    # Umbrales para determinar si necesitamos intervenir
    theta_threshold = 0.03  # Ángulo máximo permitido (en radianes)
    theta_dot_threshold = 0.5  # Velocidad angular máxima permitida

    # Si el poste está casi vertical y con poca velocidad angular, mantener movimiento
    if abs(theta) < 0.01 and abs(theta_dot) < 0.1:
        # Si estamos alejándonos del centro, intentar volver
        if abs(x) > 1.0:
            return 0 if x > 0 else 1
        # Si no, mantener el movimiento actual para conservar momentum
        return 0 if theta_dot < 0 else 1

    # Si el ángulo es significativo, actuar decisivamente
    if abs(theta) > theta_threshold:
        # Caso 1: Ángulo positivo (inclinado a la derecha)
        if theta > 0:
            # Si está cayendo rápido, intervenir con fuerza
            if theta_dot > theta_dot_threshold:
                return 1  # Empujar a la derecha
            else:
                # Decisión basada en posición para evitar salir de la pista
                return 0 if x > 2.0 else 1
        # Caso 2: Ángulo negativo (inclinado a la izquierda)
        else:
            # Si está cayendo rápido, intervenir con fuerza
            if theta_dot < -theta_dot_threshold:
                return 0  # Empujar a la izquierda
            else:
                # Decisión basada en posición para evitar salir de la pista
                return 1 if x < -2.0 else 0

    # En casos intermedios, decidir principalmente por el ángulo
    # pero considerando también la velocidad angular para anticipar
    if theta_dot > 0:  # Rotando en sentido horario
        return 1 if theta > 0 else 0
    else:  # Rotando en sentido antihorario
        return 0 if theta > 0 else 1

# Función para entrenar el agente híbrido
def train_hybrid_agent(env, agent, n_episodes=1000, max_steps=500,
                       gamma=0.99, print_interval=50):
    """Entrena al agente híbrido en el entorno"""
    # Historial de entrenamiento
    history = {
        'episode_rewards': [],
        'episode_lengths': [],
        'expert_weights': [],
        'losses': []
    }

    # Para determinar solución del entorno
    last_100_rewards = deque(maxlen=100)
    solving_threshold = 475  # Para CartPole-v1
    solved_episode = None

    for episode in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0

        for step in range(max_steps):
            # Seleccionar acción
            action = agent.choose_action(state)

            # Ejecutar acción
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            # Almacenar recompensa
            agent.store_reward(reward)

            # Actualizar estado y recompensa
            state = next_state
            episode_reward += reward

            if done:
                break

        # Entrenar al final del episodio
        loss = agent.train(gamma)

        # Registrar resultados
        history['episode_rewards'].append(episode_reward)
        history['episode_lengths'].append(step + 1)
        history['expert_weights'].append(agent.expert_weight)
        history['losses'].append(loss)

        # Comprobar solución
        last_100_rewards.append(episode_reward)
        if len(last_100_rewards) >= 100:
            mean_100 = np.mean(last_100_rewards)
            if mean_100 >= solving_threshold and solved_episode is None:
                solved_episode = episode
                print(f"¡Entorno resuelto en episodio {episode}!")

        # Mostrar progreso
        if (episode + 1) % print_interval == 0:
            mean_reward = np.mean(history['episode_rewards'][-print_interval:])
            expert_percentage = agent.get_expert_percentage()
            print(f"Episodio {episode + 1}: Recompensa = {episode_reward:.2f}, "
                  f"Media = {mean_reward:.2f}, Peso experto = {agent.expert_weight:.3f}, "
                  f"Acciones experto = {expert_percentage:.1f}%")

    return history, solved_episode

# Función para evaluar agente entrenado vs versiones puras
def compare_agents(env_name, hybrid_agent, n_episodes=100):
    """Compara el agente híbrido con las versiones de experto puro y aprendida pura"""
    env = gym.make(env_name)

    # 1. Política experta pura
    expert_rewards = []
    for _ in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        done = False
        while not done:
            action = expert_cartpole_policy(state)
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            episode_reward += reward
        expert_rewards.append(episode_reward)

    # 2. Política aprendida pura (desde el agente híbrido)
    learned_rewards = []
    for _ in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        done = False
        while not done:
            action = hybrid_agent.choose_action(state, training=False)
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            episode_reward += reward
        learned_rewards.append(episode_reward)

    # 3. Política híbrida (combinación fija 50/50)
    hybrid_rewards = []
    for _ in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        done = False
        while not done:
            # Emular una combinación 50/50
            if random.random() < 0.5:
                action = expert_cartpole_policy(state)
            else:
                action = hybrid_agent.choose_action(state, training=False)
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            episode_reward += reward
        hybrid_rewards.append(episode_reward)

    return {
        'expert': expert_rewards,
        'learned': learned_rewards,
        'hybrid': hybrid_rewards
    }

# Función para visualizar resultados
def visualize_hybrid_results(history, comparison_results, solved_episode):
    """Visualiza resultados del entrenamiento y comparación"""
    plt.figure(figsize=(15, 10))

    # 1. Recompensa por episodio durante entrenamiento
    plt.subplot(2, 2, 1)
    plt.plot(history['episode_rewards'])
    if solved_episode is not None:
        plt.axvline(x=solved_episode, color='r', linestyle='--',
                    label=f'Resuelto en episodio {solved_episode}')
    plt.axhline(y=475, color='g', linestyle='--', label='Objetivo')
    plt.xlabel('Episodio')
    plt.ylabel('Recompensa')
    plt.title('Recompensa por episodio durante entrenamiento')
    plt.legend()

    # 2. Media móvil de recompensa
    plt.subplot(2, 2, 2)
    rewards = np.array(history['episode_rewards'])
    window = 100
    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')
    plt.plot(range(window-1, len(rewards)), moving_avg)
    plt.axhline(y=475, color='g', linestyle='--', label='Objetivo')
    plt.xlabel('Episodio')
    plt.ylabel('Recompensa media (100 episodios)')
    plt.title('Media móvil de recompensa')
    plt.legend()

    # 3. Evolución del peso del experto
    plt.subplot(2, 2, 3)
    plt.plot(history['expert_weights'])
    plt.xlabel('Episodio')
    plt.ylabel('Peso del experto')
    plt.title('Evolución del peso del experto')

    # 4. Comparación de políticas
    plt.subplot(2, 2, 4)
    plt.boxplot([comparison_results['expert'],
                 comparison_results['learned'],
                 comparison_results['hybrid']],
                labels=['Experto', 'Aprendida', 'Híbrida 50/50'])
    plt.ylabel('Recompensa')
    plt.title('Comparación de políticas')

    plt.tight_layout()
    plt.show()

    # Estadísticas de comparación
    print("\nEstadísticas de comparación:")
    for policy_name, rewards in comparison_results.items():
        print(f"{policy_name.capitalize()}:")
        print(f"  Media: {np.mean(rewards):.2f}")
        print(f"  Mediana: {np.median(rewards):.2f}")
        print(f"  Desv. estándar: {np.std(rewards):.2f}")
        print(f"  Mínimo: {np.min(rewards):.2f}")
        print(f"  Máximo: {np.max(rewards):.2f}")
        print(f"  % Resueltos (≥475): {100 * np.mean([r >= 475 for r in rewards]):.1f}%")

# Función principal para ejecutar experimento híbrido
def run_hybrid_experiment():
    # Crear entorno
    env_name = 'CartPole-v1'
    env = gym.make(env_name)

    # Dimensiones del problema
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    print(f"Entorno: {env_name}")
    print(f"Dimensión de estados: {state_dim}")
    print(f"Dimensión de acciones: {action_dim}")

    # Crear agente
    agent = HybridAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        expert_policy_fn=expert_cartpole_policy,
        learning_rate=0.001,
        hidden_size=64,
        initial_expert_weight=0.9,  # Alta dependencia inicial del experto
        expert_decay=0.995,         # Decaimiento gradual
        min_expert_weight=0.05      # Pequeña influencia residual
    )

    # Entrenar agente
    print("\nIniciando entrenamiento del agente híbrido...")
    history, solved_episode = train_hybrid_agent(
        env=env,
        agent=agent,
        n_episodes=500,
        gamma=0.99,
        print_interval=50
    )

    # Comparar con otras políticas
    print("\nComparando diferentes políticas...")
    comparison_results = compare_agents(env_name, agent)

    # Visualizar resultados
    visualize_hybrid_results(history, comparison_results, solved_episode)

    return agent, history, comparison_results

# Ejecutar experimento
if __name__ == "__main__":
    agent, history, comparison = run_hybrid_experiment()

"""**Características clave del enfoque híbrido:**

**Política experta avanzada:** Implementamos una heurística sofisticada para
CartPole que considera múltiples factores:


Ángulo del poste y su velocidad angular

Posición del carro para evitar salirse de la pista

Diferentes umbrales para tomar decisiones apropiadas en cada situación


**Mecanismo de combinación adaptativa:**

Inicialmente, el agente confía principalmente en el experto (90% de las decisiones)

Gradualmente, disminuye esta dependencia mediante un factor de decaimiento (0.995)

Mantiene una influencia mínima del experto (5%) para situaciones críticas


**Entrenamiento guiado:** El conocimiento experto acelera el aprendizaje inicial, concentrando la exploración en regiones prometedoras del espacio de estados-acciones.

**Transición suave:** El decaimiento gradual permite una transición fluida desde decisiones basadas en reglas hacia decisiones aprendidas.

**Análisis comparativo:** Al final, evaluamos tres enfoques:

Política puramente experta (basada en reglas)

Política puramente aprendida (red neuronal entrenada)

Política híbrida fija (combinación 50/50)


**Este enfoque híbrido ofrece varias ventajas:**

Aprendizaje más rápido que los métodos puramente basados en datos

Mayor robustez que las heurísticas puras en situaciones no contempladas

Capacidad para desarrollar comportamientos emergentes no codificados explícitamente

**Ejercicio 5: Desarrollo de un agente que pueda transferir conocimiento entre entornos**

La transferencia de conocimiento constituye un área fascinante del aprendizaje por refuerzo, permitiendo que agentes aprovechen lo aprendido en un entorno para desempeñarse mejor en otro. Implementaremos un enfoque de transferencia entre versiones del entorno CartPole con diferentes características.
"""

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import matplotlib.pyplot as plt
from collections import deque
import copy
import time

# Configuración para reproducibilidad
np.random.seed(42)
torch.manual_seed(42)

# Definir redes neuronales para agentes
class PolicyNetwork(nn.Module):
    def __init__(self, input_size, output_size, hidden_size=64):
        super(PolicyNetwork, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.network(x)

# Agente Policy Gradient con capacidad de transferencia
class TransferablePolicyGradient:
    def __init__(self, input_size, output_size, hidden_size=64, learning_rate=0.001):
        self.policy = PolicyNetwork(input_size, output_size, hidden_size)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)
        self.transfer_mode = None

        # Almacenamiento para experiencias
        self.log_probs = []
        self.rewards = []

        # Contadores y métricas
        self.total_steps = 0
        self.total_episodes = 0

    def choose_action(self, state):
        """Selecciona una acción según la política"""
        state = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():  # No necesitamos gradientes para seleccionar acción
            probabilities = self.policy(state)
        distribution = Categorical(probabilities)
        action = distribution.sample()

        # Solo guardamos log_probs si vamos a entrenar (requiere gradientes)
        if self.transfer_mode != 'full':
            # Necesitamos calcular log_probs con gradientes habilitados
            probabilities = self.policy(state)
            distribution = Categorical(probabilities)
            self.log_probs.append(distribution.log_prob(action))

        return action.item()

    def store_reward(self, reward):
        """Almacena recompensa recibida"""
        self.rewards.append(reward)

    def discount_rewards(self, rewards, gamma=0.99):
        """Calcula recompensas descontadas"""
        discounted_rewards = []
        running_add = 0
        for r in reversed(rewards):
            running_add = running_add * gamma + r
            discounted_rewards.insert(0, running_add)

        # Normalización
        discounted_rewards = torch.FloatTensor(discounted_rewards)
        if len(discounted_rewards) > 1:  # Evitar división por cero
            discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)
        return discounted_rewards

    def train(self, gamma=0.99):
        """Entrena la política según algoritmo Policy Gradient"""
        if len(self.rewards) == 0 or self.transfer_mode == 'full':
            # No entrenar si no hay recompensas o en modo transferencia completa
            return 0

        # Calcular recompensas descontadas
        discounted_rewards = self.discount_rewards(self.rewards, gamma)

        # Calcular pérdida
        policy_loss = []
        for log_prob, R in zip(self.log_probs, discounted_rewards):
            policy_loss.append(-log_prob * R)

        # Comprobamos que tenemos elementos en policy_loss
        if len(policy_loss) == 0:
            self.log_probs = []  # Limpiamos por si acaso
            self.rewards = []
            return 0

        policy_loss = torch.cat(policy_loss).sum()

        # Actualizar pesos
        self.optimizer.zero_grad()
        policy_loss.backward()
        self.optimizer.step()

        # Actualizar contadores
        self.total_steps += len(self.rewards)
        self.total_episodes += 1

        # Limpiar experiencia
        self.log_probs = []
        self.rewards = []

        return policy_loss.item()

    def get_policy_state_dict(self):
        """Obtiene los parámetros del modelo para transferencia"""
        return copy.deepcopy(self.policy.state_dict())

    def load_policy_state_dict(self, state_dict, transfer_mode='full'):
        """
        Carga parámetros de política para transferencia de conocimiento

        Modos de transferencia:
        - 'full': Transfiere todos los parámetros
        - 'features_only': Transfiere solo las capas ocultas (extracción de características)
        - 'fine_tune': Carga todos los parámetros pero permite ajuste fino posterior
        """
        self.transfer_mode = transfer_mode

        if transfer_mode == 'full':
            # Transferencia completa de todos los parámetros
            self.policy.load_state_dict(state_dict)

            # Congelar todos los parámetros (no se actualizarán durante el entrenamiento)
            for param in self.policy.parameters():
                param.requires_grad = False

        elif transfer_mode == 'features_only':
            # Transferir solo capas ocultas (extracción de características)
            # Primero cargamos todos los parámetros
            model_dict = self.policy.state_dict()

            # Filtramos para mantener solo las capas ocultas
            filtered_dict = {k: v for k, v in state_dict.items()
                            if 'network.0' in k or 'network.2' in k}

            # Actualizamos el diccionario de parámetros
            model_dict.update(filtered_dict)
            self.policy.load_state_dict(model_dict)

            # Congelamos las capas transferidas
            for name, param in self.policy.named_parameters():
                if 'network.0' in name or 'network.2' in name:
                    param.requires_grad = False
                else:
                    param.requires_grad = True

        elif transfer_mode == 'fine_tune':
            # Transferencia completa pero permitiendo ajuste fino
            self.policy.load_state_dict(state_dict)

            # Todos los parámetros se mantienen entrenables
            for param in self.policy.parameters():
                param.requires_grad = True

            # Reducimos la tasa de aprendizaje para el ajuste fino
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = param_group['lr'] * 0.1

        else:
            self.transfer_mode = None
            raise ValueError(f"Modo de transferencia desconocido: {transfer_mode}")

# Función para entrenar agente
def train_agent(env, agent, n_episodes=200, max_steps=500, gamma=0.99,
                print_interval=50, early_stopping=True, solving_threshold=475):
    """Entrena agente en entorno especificado"""
    # Historial de entrenamiento
    history = {
        'episode_rewards': [],
        'episode_lengths': [],
        'losses': []
    }

    # Para determinar solución del entorno
    last_100_rewards = deque(maxlen=100)
    solved_episode = None

    start_time = time.time()

    for episode in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0

        for step in range(max_steps):
            action = agent.choose_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            agent.store_reward(reward)
            episode_reward += reward
            state = next_state

            if done:
                break

        # Entrenar solo si no estamos en modo de transferencia completa
        if agent.transfer_mode != 'full':
            loss = agent.train(gamma)
            history['losses'].append(loss)
        else:
            # En modo 'full' no entrenamos, así que registramos pérdida 0
            history['losses'].append(0)

        # Registrar resultados
        history['episode_rewards'].append(episode_reward)
        history['episode_lengths'].append(step + 1)

        # Comprobar solución
        last_100_rewards.append(episode_reward)
        if len(last_100_rewards) >= 100:
            mean_100 = np.mean(last_100_rewards)
            if mean_100 >= solving_threshold and solved_episode is None:
                solved_episode = episode
                if early_stopping:
                    print(f"¡Entorno resuelto en episodio {episode}! Deteniendo entrenamiento.")
                    break

        # Mostrar progreso
        if (episode + 1) % print_interval == 0:
            mean_reward = np.mean(history['episode_rewards'][-print_interval:])
            print(f"Episodio {episode + 1}: Recompensa = {episode_reward:.2f}, Media = {mean_reward:.2f}")

    # Calcular tiempo total
    training_time = time.time() - start_time

    # Agregar métricas adicionales
    history['training_time'] = training_time
    history['solved_episode'] = solved_episode

    return history

# Función para evaluar agente
def evaluate_agent(env, agent, n_episodes=20):
    """Evalúa agente entrenado en entorno especificado"""
    rewards = []

    for _ in range(n_episodes):
        state, _ = env.reset()
        episode_reward = 0
        done = False

        while not done:
            # En evaluación, usar política sin exploración
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            with torch.no_grad():
                action_probs = agent.policy(state_tensor)
                action = action_probs.argmax().item()

            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            episode_reward += reward

        rewards.append(episode_reward)

    return {
        'mean': np.mean(rewards),
        'median': np.median(rewards),
        'std': np.std(rewards),
        'min': np.min(rewards),
        'max': np.max(rewards),
        'success_rate': np.mean([r >= 475 for r in rewards]) * 100
    }

# Función para crear entorno personalizado (variante de CartPole)
def create_custom_cartpole(gravity=9.8, masscart=1.0, masspole=0.1, pole_length=0.5):
    """Crea un entorno CartPole con parámetros físicos personalizados"""
    env = gym.make('CartPole-v1')

    # Modificar parámetros físicos
    env.unwrapped.gravity = gravity
    env.unwrapped.masscart = masscart
    env.unwrapped.masspole = masspole
    env.unwrapped.length = pole_length  # medio largo del palo

    # Recalcular parámetros derivados
    env.unwrapped.polemass_length = env.unwrapped.masspole * env.unwrapped.length

    return env

# Función principal para ejecutar experimento de transferencia
def run_transfer_experiment():
    print("Iniciando experimento de transferencia de conocimiento...")

    # Definir variantes del entorno CartPole
    env_configs = [
        # Entorno fuente (CartPole estándar)
        {
            'name': 'CartPole-Estándar',
            'gravity': 9.8,
            'masscart': 1.0,
            'masspole': 0.1,
            'pole_length': 0.5
        },
        # Entorno objetivo 1 (poste más pesado)
        {
            'name': 'CartPole-PesoPalo',
            'gravity': 9.8,
            'masscart': 1.0,
            'masspole': 0.3,  # 3x más pesado
            'pole_length': 0.5
        },
        # Entorno objetivo 2 (poste más largo)
        {
            'name': 'CartPole-PaloLargo',
            'gravity': 9.8,
            'masscart': 1.0,
            'masspole': 0.1,
            'pole_length': 1.0  # 2x más largo
        },
        # Entorno objetivo 3 (gravedad reducida - similar a Luna)
        {
            'name': 'CartPole-GravedadBaja',
            'gravity': 1.62,  # Gravedad lunar
            'masscart': 1.0,
            'masspole': 0.1,
            'pole_length': 0.5
        }
    ]

    # Parámetros compartidos
    input_size = 4  # Dimensión del espacio de estados en CartPole
    output_size = 2  # Número de acciones en CartPole

    # 1. Entrenar agente en entorno fuente (CartPole estándar)
    print("\n1. Entrenando agente en entorno fuente...")
    source_env = create_custom_cartpole(
        gravity=env_configs[0]['gravity'],
        masscart=env_configs[0]['masscart'],
        masspole=env_configs[0]['masspole'],
        pole_length=env_configs[0]['pole_length']
    )
    source_agent = TransferablePolicyGradient(input_size, output_size)

    # Asegurarnos de que el modo de transferencia sea None para entrenamiento normal
    source_agent.transfer_mode = None

    source_history = train_agent(
        env=source_env,
        agent=source_agent,
        n_episodes=500,
        gamma=0.99,
        print_interval=50,
        early_stopping=True
    )

    # Guardar parámetros del modelo entrenado en fuente
    source_params = source_agent.get_policy_state_dict()

    # Evaluar agente fuente
    source_metrics = evaluate_agent(source_env, source_agent)
    print("\nMétricas del agente en entorno fuente:")
    for key, value in source_metrics.items():
        print(f"  {key}: {value:.2f}")

    # 2. Experimentos de transferencia
    transfer_modes = ['no_transfer', 'full', 'features_only', 'fine_tune']
    transfer_results = {}

    for target_idx in range(1, len(env_configs)):
        target_config = env_configs[target_idx]
        target_env_name = target_config['name']
        print(f"\n2.{target_idx}. Experimentos de transferencia para {target_env_name}...")

        # Crear entorno objetivo
        target_env = create_custom_cartpole(
            gravity=target_config['gravity'],
            masscart=target_config['masscart'],
            masspole=target_config['masspole'],
            pole_length=target_config['pole_length']
        )

        # Ejecutar cada modo de transferencia
        mode_results = {}

        for mode in transfer_modes:
            print(f"  Modo de transferencia: {mode}")

            # Crear nuevo agente para entorno objetivo
            target_agent = TransferablePolicyGradient(input_size, output_size)

            # Aplicar transferencia si corresponde
            if mode != 'no_transfer':
                target_agent.load_policy_state_dict(source_params, transfer_mode=mode)
            else:
                target_agent.transfer_mode = None  # Asegurarnos que sea None para entrenamiento normal

            # Entrenar agente en entorno objetivo
            target_history = train_agent(
                env=target_env,
                agent=target_agent,
                n_episodes=150,  # Reducido para demostración
                gamma=0.99,
                print_interval=50,
                early_stopping=True
            )

            # Evaluar agente final
            target_metrics = evaluate_agent(target_env, target_agent, n_episodes=20)

            # Guardar resultados
            mode_results[mode] = {
                'history': target_history,
                'metrics': target_metrics
            }

            solved_ep = target_history['solved_episode']
            solved_text = f"ep. {solved_ep}" if solved_ep is not None else "No resuelto"
            print(f"  Resultados: {solved_text}, "
                  f"Tasa éxito: {target_metrics['success_rate']:.2f}%")

        transfer_results[target_env_name] = mode_results

    # 3. Visualizar resultados
    visualize_transfer_results(source_history, transfer_results, env_configs)

    return source_agent, transfer_results

# Función para visualizar resultados de transferencia
def visualize_transfer_results(source_history, transfer_results, env_configs):
    """Visualiza los resultados del experimento de transferencia"""
    # Colores para los diferentes modos de transferencia
    colors = {
        'no_transfer': 'red',
        'full': 'green',
        'features_only': 'blue',
        'fine_tune': 'purple'
    }

    # Crear una figura para cada entorno objetivo
    for i, (env_name, modes) in enumerate(transfer_results.items(), 1):
        plt.figure(figsize=(15, 10))

        # 1. Curvas de aprendizaje (recompensa por episodio)
        plt.subplot(2, 2, 1)

        # Curva del entorno fuente como referencia
        source_rewards = source_history['episode_rewards']
        plt.plot(source_rewards, color='black', linestyle='--', label='Fuente (original)')

        # Curvas para cada modo de transferencia
        for mode, results in modes.items():
            rewards = results['history']['episode_rewards']
            plt.plot(rewards, color=colors[mode], label=f'Modo: {mode}')

            # Marcar episodio de solución
            solved_episode = results['history']['solved_episode']
            if solved_episode is not None:
                plt.scatter(solved_episode, rewards[solved_episode],
                           color=colors[mode], marker='o')

        plt.axhline(y=475, color='gray', linestyle='--', label='Objetivo')
        plt.xlabel('Episodio')
        plt.ylabel('Recompensa')
        plt.title(f'Curvas de aprendizaje para {env_name}')
        plt.legend()

        # 2. Media móvil de recompensa
        plt.subplot(2, 2, 2)
        window = 20  # Menor ventana para mejor visualización

        # Media móvil para entorno fuente
        if len(source_rewards) >= window:
            source_ma = np.convolve(source_rewards, np.ones(window)/window, mode='valid')
            plt.plot(range(window-1, len(source_rewards)), source_ma,
                    color='black', linestyle='--', label='Fuente')

        # Media móvil para cada modo
        for mode, results in modes.items():
            rewards = results['history']['episode_rewards']
            if len(rewards) >= window:  # Verificación para evitar errores
                ma = np.convolve(rewards, np.ones(window)/window, mode='valid')
                plt.plot(range(window-1, len(rewards)), ma,
                        color=colors[mode], label=f'Modo: {mode}')

        plt.axhline(y=475, color='gray', linestyle='--')
        plt.xlabel('Episodio')
        plt.ylabel('Recompensa media móvil')
        plt.title(f'Media móvil de recompensa (ventana: {window})')
        plt.legend()

        # 3. Métricas de evaluación
        plt.subplot(2, 2, 3)
        metrics = ['mean', 'median', 'std', 'success_rate']
        x = np.arange(len(metrics))
        width = 0.2
        offsets = [-0.3, -0.1, 0.1, 0.3]

        for (mode, results), offset in zip(modes.items(), offsets):
            values = [results['metrics'][m] for m in metrics]
            plt.bar(x + offset, values, width, label=f'Modo: {mode}', color=colors[mode])

        plt.xticks(x, metrics)
        plt.ylabel('Valor')
        plt.title('Métricas de evaluación')
        plt.legend()

        # 4. Episodio de solución y tiempo de entrenamiento
        plt.subplot(2, 2, 4)
        solve_episodes = []
        train_times = []
        mode_names = []

        for mode, results in modes.items():
            solve_ep = results['history']['solved_episode']
            if solve_ep is None:
                solve_ep = 150  # Usar máximo episodios como valor para visualización

            solve_episodes.append(solve_ep)
            train_times.append(results['history'].get('training_time', 0))
            mode_names.append(mode)

        # Normalizar tiempo para visualización conjunta
        max_time = max(train_times) if train_times and max(train_times) > 0 else 1
        norm_times = [t/max_time * 300 if max_time > 0 else 0 for t in train_times]  # Escalar para visualización

        # Gráfico de barras para episodio de solución
        ax1 = plt.gca()
        bars = ax1.bar(mode_names, solve_episodes, color=[colors[m] for m in mode_names])
        ax1.set_ylabel('Episodio de solución')
        ax1.set_title('Episodio de solución y tiempo de entrenamiento')

        # Agregar tiempo como línea en eje secundario
        ax2 = ax1.twinx()
        ax2.plot(mode_names, train_times, 'o-', color='orange')
        ax2.set_ylabel('Tiempo (segundos)', color='orange')
        ax2.tick_params(axis='y', labelcolor='orange')

        # Anotar valores
        for i, (ep, time) in enumerate(zip(solve_episodes, train_times)):
            if ep < 150:  # Solo etiquetar los realmente resueltos
                ax1.text(i, ep, f"{ep}", ha='center', va='bottom')
            ax2.text(i, time, f"{time:.1f}s", ha='center', va='bottom', color='orange')

        plt.tight_layout()
        plt.show()

    # Gráfico comparativo final entre entornos
    plt.figure(figsize=(12, 6))

    # Preparar datos
    env_names = list(transfer_results.keys())
    if env_names:  # Verificar que hay resultados para mostrar
        mode_names = list(transfer_results[env_names[0]].keys())

        # Para cada modo, mostrar tasa de éxito y episodio de solución por entorno
        width = 0.2
        x = np.arange(len(env_names))

        # 1. Tasa de éxito
        plt.subplot(1, 2, 1)
        for i, mode in enumerate(mode_names):
            success_rates = [transfer_results[env][mode]['metrics']['success_rate']
                            for env in env_names]
            plt.bar(x + (i - 1.5) * width, success_rates, width,
                   label=f'Modo: {mode}', color=colors[mode])

        plt.xticks(x, env_names, rotation=45, ha='right')
        plt.ylabel('Tasa de éxito (%)')
        plt.title('Tasa de éxito por entorno y modo de transferencia')
        plt.legend()

        # 2. Episodio de solución
        plt.subplot(1, 2, 2)
        for i, mode in enumerate(mode_names):
            solved_episodes = []
            for env in env_names:
                ep = transfer_results[env][mode]['history']['solved_episode']
                solved_episodes.append(ep if ep is not None else 150)  # Max episodios

            plt.bar(x + (i - 1.5) * width, solved_episodes, width,
                   label=f'Modo: {mode}', color=colors[mode])

        plt.xticks(x, env_names, rotation=45, ha='right')
        plt.ylabel('Episodio de solución')
        plt.title('Episodio de solución por entorno y modo de transferencia')
        plt.legend()

        plt.tight_layout()
        plt.show()

# Ejecutar experimento
if __name__ == "__main__":
    source_agent, transfer_results = run_transfer_experiment()