# -*- coding: utf-8 -*-
"""Ejercicios_modulo2_IPAR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bhmCrsN4FZfCOGKeiGrHC0k6AAomgsMr

# 🤖 Módulo 2: Programación Funcional y Orientada a Objetos en Python
## Guía de Ejercicios para Aprendizaje por Refuerzo

![Python & RL](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![OpenAI Gym](https://img.shields.io/badge/OpenAI_Gym-0081A5?style=for-the-badge&logo=OpenAI&logoColor=white)

### 📋 Sociedad Argentina de Estadística
**Curso:** Introducción a la Programación en Python para Aprendizaje por Refuerzo  
**Docente:** Dr. Darío Ezequiel Díaz  
**Marzo-Abril 2025**

---

> Esta guía de ejercicios está diseñada para desarrollar habilidades en programación funcional y orientada a objetos en Python, con énfasis en su aplicación al campo del Aprendizaje por Refuerzo. Los ejercicios avanzan progresivamente en complejidad, desde conceptos básicos hasta implementaciones avanzadas.

### 📝 Instrucciones:
- Complete cada ejercicio en las celdas correspondientes
- Ejecute el código para verificar su funcionamiento
- Los ejercicios están diseñados para reforzar conceptos teóricos vistos en clase
- Consulte la documentación oficial de Python cuando sea necesario

### 📚 Contenidos:
- Funciones y parámetros
- Funciones Lambda
- Programación orientada a objetos
- Listas por comprensión
- Decoradores
- Manejo de excepciones
- Patrones de diseño

---

*"En el aprendizaje por refuerzo, como en la programación, la exploración y la explotación deben estar en perfecto equilibrio."*

---

# Ejercicio 1: Funciones con Parámetros por Defecto y Nombrados
**Dificultad: Básica**

**Objetivo**: Practicar la creación de funciones con parámetros opcionales y valores por defecto.

**Descripción**: Crea una función llamada `calcular_precio_final` que calcule el precio de un producto después de aplicar impuestos y descuentos.

La función debe aceptar:
- `precio_base` (obligatorio)
- `impuesto` (por defecto 0.21 - equivale a 21%)
- `descuento` (por defecto 0 - sin descuento)

Primero se debe aplicar el descuento al precio base, y luego calcular el impuesto sobre ese valor con descuento.
"""

# Completa la función calcular_precio_final
def calcular_precio_final(precio_base, impuesto=0.21, descuento=0):
    # Aplica primero el descuento y luego el impuesto
    # Tu código aquí

    pass  # Elimina esta línea al implementar tu solución

# Pruebas:
print(calcular_precio_final(100))  # Debería ser 121.0
print(calcular_precio_final(100, descuento=0.1))  # Debería ser 108.9
print(calcular_precio_final(100, 0.05, 0.2))  # Debería ser 84.0

"""# Ejercicio 2: Funciones Lambda para Ordenamiento
**Dificultad: Básica**

**Objetivo**: Practicar el uso de funciones lambda con la función `sorted()` para ordenar colecciones complejas.

**Descripción**: Utiliza funciones lambda con la función `sorted()` para ordenar una lista de tuplas que representan episodios de entrenamiento de un agente.

Cada tupla contiene:
- (número_episodio, pasos, recompensa_total)

Debes ordenar la lista de 3 formas diferentes:
1. Por recompensa (de mayor a menor)
2. Por número de pasos (de menor a mayor)
3. Por eficiencia (recompensa/pasos, de mayor a menor)
"""

# Lista de episodios: (número_episodio, pasos, recompensa_total)
episodios = [
    (1, 145, 24),
    (2, 97, 31),
    (3, 156, 18),
    (4, 82, 42),
    (5, 113, 37)
]

# Ordena los episodios por recompensa (mayor a menor)
episodios_por_recompensa = None  # Tu código aquí

# Ordena los episodios por número de pasos (menor a mayor)
episodios_por_pasos = None  # Tu código aquí

# Ordena los episodios por eficiencia (recompensa/pasos, mayor a menor)
episodios_por_eficiencia = None  # Tu código aquí

# Imprime los resultados
print("Por recompensa:", episodios_por_recompensa)
print("Por pasos:", episodios_por_pasos)
print("Por eficiencia:", episodios_por_eficiencia)

"""# Ejercicio 3: Clase Básica para Agente de Aprendizaje por Refuerzo
**Dificultad: Básica**

**Objetivo**: Practicar la creación de clases, atributos y métodos en POO.

**Descripción**: Crea una clase `AgenteRL` que represente un agente básico de aprendizaje por refuerzo con:
- Atributos para almacenar parámetros y valores Q
- Un método para seleccionar acciones con política epsilon-greedy
- Un método para aprender de experiencias
- Un método `__str__` para visualizar información del agente

La política epsilon-greedy consiste en:
- Con probabilidad epsilon: seleccionar una acción aleatoria (exploración)
- Con probabilidad 1-epsilon: seleccionar la mejor acción conocida (explotación)
"""

import random

# Completa la clase AgenteRL
class AgenteRL:
    def __init__(self, num_acciones, epsilon=0.1):
        # Inicializa atributos: tabla de valores Q, epsilon, etc.
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def seleccionar_accion(self, estado):
        # Implementa la política epsilon-greedy:
        # - Con probabilidad epsilon: acción aleatoria (exploración)
        # - Con probabilidad 1-epsilon: mejor acción conocida (explotación)
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def aprender(self, estado, accion, recompensa, siguiente_estado):
        # Implementa actualización simple de valor Q
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def __str__(self):
        # Representación en string del agente
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

# Prueba la clase
agente = AgenteRL(num_acciones=4)
print(agente)

# Prueba selección de acciones
estado1 = "s1"
accion1 = agente.seleccionar_accion(estado1)
print(f"Acción seleccionada para {estado1}: {accion1}")

# Prueba aprendizaje
agente.aprender(estado1, accion1, 5, "s2")
print(f"Después de aprender una vez: {agente}")

"""# Ejercicio 4: Listas por Comprensión para Transformación de Datos
**Dificultad: Intermedia**

**Objetivo**: Practicar el uso de listas por comprensión para transformar y filtrar datos.

**Descripción**: Utiliza listas por comprensión para procesar un conjunto de observaciones de un entorno de aprendizaje por refuerzo.

Cada observación es una tupla con formato:
- (estado, acción, recompensa, siguiente_estado, terminado)

Debes crear:
1. Una lista de todas las recompensas
2. Una lista de transiciones (estado, acción) que obtuvieron recompensa positiva
3. Un diccionario que mapee cada estado a la recompensa promedio obtenida desde él
4. Una lista de estados terminales (donde terminado=True)
"""

# Datos de observaciones de un entorno - cada tupla contiene:
# (estado, acción, recompensa, siguiente_estado, terminado)
observaciones = [
    (0, 1, 5, 1, False),
    (1, 0, -1, 2, False),
    (2, 1, 0, 3, False),
    (3, 2, 10, 4, True),
    (0, 2, 3, 2, False),
    (2, 0, 7, 3, False),
    (3, 1, -2, 0, False),
    (0, 0, 4, 4, True)
]

# 1. Crea una lista de todas las recompensas
recompensas = None  # Tu código aquí

# 2. Crea una lista de transiciones (estado, acción) que obtuvieron recompensa positiva
transiciones_positivas = None  # Tu código aquí

# 3. Crea un diccionario que mapee cada estado a la recompensa promedio obtenida desde él
# Pista: Necesitarás contar recompensas y acciones por estado
recompensa_por_estado = None  # Tu código aquí

# 4. Crea una lista de estados terminales
estados_terminales = None  # Tu código aquí

# Imprime los resultados
print("Recompensas:", recompensas)
print("Transiciones con recompensa positiva:", transiciones_positivas)
print("Recompensa promedio por estado:", recompensa_por_estado)
print("Estados terminales:", estados_terminales)

"""# Ejercicio 5: Argumentos Variables con *args y **kwargs
**Dificultad: Intermedia**

**Objetivo**: Practicar el uso de argumentos variables en funciones.

**Descripción**: Crea una función llamada `crear_informe_agente` que genere un informe para agentes de aprendizaje por refuerzo.

La función debe aceptar:
- `nombre_agente` (obligatorio)
- Cualquier número de episodios con sus recompensas (`*args`) - cada episodio es una tupla (num_episodio, recompensa)
- Cualquier número de parámetros de configuración (`**kwargs`) - pueden ser alpha, gamma, epsilon, etc.

La función debe calcular la recompensa total, promedio, máxima, mínima, y retornar un diccionario con toda esta información junto con los parámetros recibidos.
"""

# Completa la función crear_informe_agente
def crear_informe_agente(nombre_agente, *args, **kwargs):
    # args contiene tuplas (episodio, recompensa)
    # kwargs contiene parámetros de configuración

    # Calcula la recompensa total y promedio
    # Tu código aquí

    # Genera y retorna un diccionario con el informe
    # Tu código aquí

    pass  # Elimina esta línea al implementar tu solución

# Pruebas:
informe = crear_informe_agente("AgentQ",
                             (1, 10), (2, 15), (3, 5),
                             alpha=0.1, gamma=0.99, epsilon=0.2)
print(informe)
# Debería mostrar un diccionario con nombre_agente, episodios, recompensa_total,
# recompensa_promedio, recompensa_máxima, recompensa_mínima y un diccionario con los parámetros

"""# Ejercicio 6: Decorador para Medir Tiempo de Ejecución
**Dificultad: Intermedia**

**Objetivo**: Entender y crear decoradores para modificar el comportamiento de funciones.

**Descripción**: Crea un decorador `medir_tiempo` que mida y muestre el tiempo de ejecución de una función. El decorador debe:

1. Registrar el tiempo antes de llamar a la función
2. Ejecutar la función y almacenar su resultado
3. Calcular el tiempo transcurrido
4. Imprimir información sobre el tiempo de ejecución
5. Retornar el resultado original de la función

Prueba el decorador con la función recursiva fibonacci, que suele tener tiempos de ejecución crecientes.
"""

import time

# Completa el decorador medir_tiempo
def medir_tiempo(funcion):
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

# Aplica el decorador a la función fibonacci
@medir_tiempo
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# Prueba la función decorada
resultado = fibonacci(30)
print(f"Resultado: {resultado}")

"""# Ejercicio 7: Procesamiento de Datos con Map/Filter/Reduce
**Dificultad: Intermedia**

**Objetivo**: Practicar programación funcional aplicando funciones de orden superior.

**Descripción**: Procesa un conjunto de datos de recompensas obtenidas por un agente utilizando las funciones `map()`, `filter()` y `reduce()`.

En este ejercicio debes:
1. Usar `map()` para calcular la recompensa total de cada episodio
2. Usar `filter()` para encontrar episodios con recompensa total positiva
3. Usar `reduce()` para encontrar el episodio con la mayor recompensa

Cada episodio contiene una lista de valores de recompensa recibidos.
"""

from functools import reduce

# Datos de recompensas por episodio
recompensas = [
    {"episodio": 1, "valores": [5, -1, 0, 10, -2]},
    {"episodio": 2, "valores": [7, 8, -3, 4, 2]},
    {"episodio": 3, "valores": [-4, 2, 0, 9, 1]},
    {"episodio": 4, "valores": [6, -2, 3, -1, 8]},
    {"episodio": 5, "valores": [1, 5, 3, 7, -6]}
]

# 1. Usa map para calcular la recompensa total de cada episodio
# Tu código aquí

# 2. Usa filter para encontrar episodios con recompensa total positiva
# Tu código aquí

# 3. Usa reduce para encontrar el episodio con la mayor recompensa
# Tu código aquí

# Imprime los resultados
print("Recompensas totales:", recompensas_totales)
print("Episodios positivos:", episodios_positivos)
print("Mejor episodio:", mejor_episodio)

"""# Ejercicio 8: Encapsulación con Propiedades y Validación
**Dificultad: Intermedia**

**Objetivo**: Implementar encapsulación y validación de datos utilizando propiedades en Python.

**Descripción**: Mejora una clase `AgenteRL` añadiendo propiedades con validación para parámetros críticos.

Las propiedades deben incluir:
- `epsilon`: Valor entre 0 y 1 (probabilidad de exploración)
- `alpha`: Valor entre 0 y 1 (tasa de aprendizaje)
- `gamma`: Valor entre 0 y 1 (factor de descuento)

Cada propiedad debe incluir validación para asegurar que los valores estén en el rango correcto,
lanzando `ValueError` con un mensaje descriptivo cuando se intente asignar un valor inválido.
"""

# Completa la clase AgenteRL mejorada con propiedades
class AgenteRL:
    def __init__(self, num_acciones, epsilon=0.1, alpha=0.1, gamma=0.9):
        self._num_acciones = num_acciones
        self._q_values = {}  # Diccionario para almacenar valores Q

        # Usa propiedades para estos parámetros
        self._epsilon = None
        self._alpha = None
        self._gamma = None

        # Asigna valores a través de propiedades para validación
        self.epsilon = epsilon
        self.alpha = alpha
        self.gamma = gamma

    # Implementa la propiedad epsilon con validación
    @property
    def epsilon(self):
        # Tu código aquí
        pass

    @epsilon.setter
    def epsilon(self, valor):
        # Valida que epsilon esté entre 0 y 1
        # Tu código aquí
        pass

    # Implementa propiedades alpha y gamma con validación similar
    # Tu código aquí

    def seleccionar_accion(self, estado):
        """Selecciona una acción usando la política epsilon-greedy."""
        estado_str = str(estado)

        # Inicializa valores Q si es necesario
        if estado_str not in self._q_values:
            self._q_values[estado_str] = [0.0] * self._num_acciones

        # Política epsilon-greedy
        if random.random() < self.epsilon:
            return random.randint(0, self._num_acciones - 1)  # Exploración
        else:
            return self._q_values[estado_str].index(max(self._q_values[estado_str]))  # Explotación

# Prueba la clase con propiedades
import random

agente = AgenteRL(num_acciones=4, epsilon=0.2)
print(f"Epsilon: {agente.epsilon}")

# Intenta asignar valores válidos e inválidos
try:
    agente.epsilon = 0.5
    print(f"Nuevo epsilon: {agente.epsilon}")

    agente.epsilon = 1.5  # Debería lanzar ValueError
except ValueError as e:
    print(f"Error capturado: {e}")

"""# Ejercicio 9: Herencia para Diferentes Tipos de Agentes
**Dificultad: Intermedia**

**Objetivo**: Practicar la herencia y la sobreescritura de métodos en POO.

**Descripción**: Crea una jerarquía de clases para diferentes tipos de agentes de aprendizaje por refuerzo,
heredando de una clase base común llamada `AgenteBase`.

Implementa:
1. `AgenteBase`: Clase abstracta con métodos `seleccionar_accion` y `aprender`
2. `AgenteAleatorio`: Un agente que selecciona acciones aleatorias y no aprende
3. `AgenteQLearning`: Un agente que implementa el algoritmo Q-Learning

Cada agente debe tener su propia implementación específica de los métodos heredados.
"""

import random

# Completa la jerarquía de clases de agentes

# Clase base
class AgenteBase:
    def __init__(self, num_acciones, nombre="AgenteGenérico"):
        self.num_acciones = num_acciones
        self.nombre = nombre
        self.experiencia_total = 0

    def seleccionar_accion(self, estado):
        # Método abstracto que las subclases deben implementar
        raise NotImplementedError("Las subclases deben implementar seleccionar_accion")

    def aprender(self, estado, accion, recompensa, siguiente_estado):
        # Método abstracto que las subclases deben implementar
        raise NotImplementedError("Las subclases deben implementar aprender")

    def __str__(self):
        return f"{self.nombre} (acciones: {self.num_acciones}, exp: {self.experiencia_total})"

# Completa la clase AgenteAleatorio
class AgenteAleatorio(AgenteBase):
    # Un agente que selecciona acciones aleatorias
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

# Completa la clase AgenteQLearning
class AgenteQLearning(AgenteBase):
    # Un agente que implementa Q-Learning
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

# Prueba las clases
agente_aleatorio = AgenteAleatorio(4, "Explorador")
agente_q = AgenteQLearning(4, "Aprendiz", alpha=0.2, gamma=0.9, epsilon=0.1)

print(agente_aleatorio)
print(agente_q)

# Prueba métodos
estado = "estado_prueba"
accion_aleatoria = agente_aleatorio.seleccionar_accion(estado)
accion_q = agente_q.seleccionar_accion(estado)

print(f"Acción aleatoria: {accion_aleatoria}")
print(f"Acción Q-Learning: {accion_q}")

# Prueba aprendizaje
agente_aleatorio.aprender(estado, accion_aleatoria, 1, "siguiente_estado")
agente_q.aprender(estado, accion_q, 1, "siguiente_estado")

"""# Ejercicio 10: Métodos de Instancia, de Clase y Estáticos
**Dificultad: Intermedia**

**Objetivo**: Entender y practicar los diferentes tipos de métodos en clases Python.

**Descripción**: Crea una clase `Experimento` que utilice los tres tipos de métodos para
gestionar experimentos de aprendizaje por refuerzo:

1. **Método de instancia**: `ejecutar()` - Ejecuta el experimento y almacena resultados
2. **Método de clase**: `crear_desde_configuracion()` - Crea un experimento a partir de un diccionario de configuración
3. **Método estático**: `calcular_metricas()` - Calcula métricas a partir de un histórico de recompensas

El método de clase debe usar el decorador `@classmethod` y el método estático debe usar `@staticmethod`.
"""

import time
import random
import numpy as np

# Completa la clase Experimento con los tres tipos de métodos
class Experimento:
    # Atributo de clase
    contador_experimentos = 0

    def __init__(self, nombre, agente, entorno):
        # Inicializa atributos de instancia
        self.nombre = nombre
        self.agente = agente
        self.entorno = entorno
        self.fecha_inicio = None
        self.fecha_fin = None
        self.resultados = None

        # Incrementa contador de clase
        Experimento.contador_experimentos += 1
        self.id = Experimento.contador_experimentos

    # Método de instancia
    def ejecutar(self, episodios=100, max_pasos=1000):
        """Ejecuta el experimento y almacena resultados."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    # Método de clase
    @classmethod
    def crear_desde_configuracion(cls, config_dict):
        """
        Método de clase que crea un experimento a partir de un diccionario de configuración.
        """
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    # Método estático
    @staticmethod
    def calcular_metricas(historico_recompensas):
        """
        Método estático que calcula métricas a partir de un histórico de recompensas.
        No depende del estado del objeto.
        """
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

# Prueba los diferentes tipos de métodos
# Simula agente y entorno para pruebas
class AgenteSimulado:
    def seleccionar_accion(self, estado):
        return 0

class EntornoSimulado:
    def reset(self):
        return 0

    def paso(self, accion):
        return 0, random.randint(1, 10), random.random() < 0.1, {}

# Prueba método de instancia
agente = AgenteSimulado()
entorno = EntornoSimulado()
experimento = Experimento("Prueba1", agente, entorno)
experimento.ejecutar(episodios=5)
print(f"Resultados: {experimento.resultados}")

# Prueba método de clase
config = {
    "nombre": "ExperimentoConfig",
    "agente": agente,
    "entorno": entorno
}
experimento2 = Experimento.crear_desde_configuracion(config)
print(f"Experimento desde config: {experimento2.nombre} (ID: {experimento2.id})")

# Prueba método estático
historico = [[5, 8, 10], [6, 7, 9, 12], [3, 5, 8]]
metricas = Experimento.calcular_metricas(historico)
print(f"Métricas calculadas: {metricas}")

"""# Ejercicio 11: Polimorfismo para Entornos de Entrenamiento
**Dificultad: Intermedia**

**Objetivo**: Implementar polimorfismo con clases de entornos que comparten una interfaz común.

**Descripción**: Crea diferentes clases de entornos que comparten una interfaz común, y una función
de entrenamiento que funcione con cualquiera de ellos.

Implementa:
1. `Entorno`: Clase base abstracta con métodos `reset()`, `paso()` y `obtener_num_acciones()`
2. `EntornoSimple`: Entorno con estados lineales (0-9) y dos acciones (izquierda/derecha)
3. `EntornoRejilla`: Entorno tipo rejilla 2D con 4 acciones (arriba/derecha/abajo/izquierda)
4. Función `entrenar()`: Entrena un agente en cualquier entorno que implemente la interfaz

El polimorfismo permite que la función de entrenamiento funcione con diferentes tipos de entornos.
"""

import random
from abc import ABC, abstractmethod

# Completa la jerarquía de clases para entornos

# Clase base abstracta
class Entorno(ABC):
    @abstractmethod
    def reset(self):
        """Reinicia el entorno y retorna el estado inicial."""
        pass

    @abstractmethod
    def paso(self, accion):
        """
        Ejecuta una acción y retorna (siguiente_estado, recompensa, terminado, info).
        """
        pass

    @abstractmethod
    def obtener_num_acciones(self):
        """Retorna el número de acciones posibles."""
        pass

# Completa la clase EntornoSimple
class EntornoSimple(Entorno):
    # Un entorno con estados del 0 al 9 y 2 acciones (izquierda/derecha)
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

# Completa la clase EntornoRejilla
class EntornoRejilla(Entorno):
    # Un entorno tipo rejilla 2D con 4 acciones (arriba/derecha/abajo/izquierda)
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

# Función polimórfica para entrenar un agente en cualquier entorno
def entrenar(agente, entorno, episodios=100, max_pasos=1000):
    """
    Entrena un agente en un entorno.
    Funciona con cualquier agente y entorno que implementen las interfaces requeridas.
    """
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

# Prueba de polimorfismo
from random import randint

# Creamos un agente aleatorio simple
class AgenteAleatorio:
    def __init__(self, num_acciones):
        self.num_acciones = num_acciones

    def seleccionar_accion(self, estado):
        return randint(0, self.num_acciones - 1)

    def aprender(self, estado, accion, recompensa, siguiente_estado):
        pass  # Este agente no aprende

# Entrenamos al agente en diferentes entornos
entorno1 = EntornoSimple()
entorno2 = EntornoRejilla(ancho=5, alto=5)

agente1 = AgenteAleatorio(entorno1.obtener_num_acciones())
agente2 = AgenteAleatorio(entorno2.obtener_num_acciones())

resultado1 = entrenar(agente1, entorno1, episodios=10)
resultado2 = entrenar(agente2, entorno2, episodios=10)

print("Resultados en EntornoSimple:", resultado1)
print("Resultados en EntornoRejilla:", resultado2)

"""# Ejercicio 12: Manejo de Excepciones en Entorno de RL
**Dificultad: Intermedia**

**Objetivo**: Implementar manejo de excepciones robusto para un entorno de aprendizaje por refuerzo.

**Descripción**: Crea excepciones personalizadas y un sistema de manejo de errores para un entorno de RL.

Debes implementar:
1. Excepciones personalizadas:
   - `EntornoError`: Clase base para excepciones del entorno
   - `AccionInvalidaError`: Cuando se intenta una acción no válida
   - `EstadoInvalidoError`: Cuando se intenta acceder a un estado no válido

2. Un entorno robusto `EntornoRobusto` con manejo de excepciones para:
   - Validar acciones
   - Validar estados
   - Manejar errores durante la ejecución de acciones

Usa bloques try/except en los métodos relevantes y proporciona mensajes de error descriptivos.
"""

# Excepciones personalizadas para entorno de RL
class EntornoError(Exception):
    """Clase base para excepciones relacionadas con el entorno."""
    pass

class AccionInvalidaError(EntornoError):
    """Se lanza cuando se intenta realizar una acción no válida."""
    pass

class EstadoInvalidoError(EntornoError):
    """Se lanza cuando se intenta acceder a un estado no válido."""
    pass

# Completa la clase EntornoRobusto con manejo de excepciones
class EntornoRobusto:
    def __init__(self, num_estados=10, num_acciones=4):
        self.num_estados = num_estados
        self.num_acciones = num_acciones
        self.estado_actual = 0
        self.terminado = False

    def reset(self):
        """Reinicia el entorno al estado inicial."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def validar_accion(self, accion):
        """Valida que una acción sea válida, lanzando excepción si no lo es."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def validar_estado(self, estado):
        """Valida que un estado sea válido, lanzando excepción si no lo es."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def paso(self, accion):
        """
        Ejecuta una acción y retorna (siguiente_estado, recompensa, terminado, info).
        Incluye manejo de excepciones.
        """
        try:
            # Validar acción
            # Tu código aquí

            # Lógica del entorno
            # Tu código aquí

            # Retornar resultado
            # Tu código aquí
            pass  # Elimina esta línea al implementar tu solución

        except AccionInvalidaError as e:
            # Manejo del error
            # Tu código aquí
            pass  # Elimina esta línea al implementar tu solución

        except Exception as e:
            # Manejo de otros errores inesperados
            # Tu código aquí
            pass  # Elimina esta línea al implementar tu solución

# Prueba el manejo de excepciones
entorno = EntornoRobusto(num_estados=5, num_acciones=2)
estado = entorno.reset()
print(f"Estado inicial: {estado}")

# Prueba con acción válida
try:
    resultado = entorno.paso(1)
    print(f"Paso exitoso: {resultado}")
except EntornoError as e:
    print(f"Error controlado: {e}")

# Prueba con acción inválida
try:
    resultado = entorno.paso(5)  # Acción fuera de rango
    print(f"Paso exitoso: {resultado}")
except EntornoError as e:
    print(f"Error controlado: {e}")

"""# Ejercicio 13: Decorador Parametrizado para Control de Intentos
**Dificultad: Avanzada**

**Objetivo**: Crear decoradores parametrizados que añadan funcionalidad específica a funciones.

**Descripción**: Implementa un decorador `reintentar` que intentará ejecutar una función varias veces si ocurre una excepción.

El decorador debe:
1. Aceptar parámetros para configurar el comportamiento:
   - `max_intentos`: Número máximo de intentos (por defecto 3)
   - `excepciones`: Tupla de tipos de excepciones a capturar (por defecto todas)
   - `espera`: Tiempo de espera entre intentos (por defecto 0)
2. Aplicar la lógica de reintentos solo si ocurre alguna de las excepciones especificadas
3. Registrar cada intento fallido y el error
4. Tras alcanzar el número máximo de intentos, relanzar la última excepción

Este tipo de decorador es útil en entornos donde pueden ocurrir errores transitorios.
"""

import random
import time

# Completa el decorador parametrizado reintentar
def reintentar(max_intentos=3, excepciones=(Exception,), espera=0):
    # Tu código aquí - recuerda que es un decorador con argumentos
    pass  # Elimina esta línea al implementar tu solución

# Función que simula un servicio inestable
@reintentar(max_intentos=5, excepciones=(ValueError, RuntimeError), espera=0.5)
def servicio_inestable(probabilidad_error=0.7):
    """
    Simula un servicio que falla con cierta probabilidad.
    """
    # Simula un servicio que falla con cierta probabilidad
    if random.random() < probabilidad_error:
        if random.random() < 0.5:
            raise ValueError("Error de valor simulado")
        else:
            raise RuntimeError("Error de ejecución simulado")
    return "¡Operación exitosa!"

# Prueba el servicio con reintento
for i in range(3):  # Prueba 3 veces
    try:
        resultado = servicio_inestable()
        print(f"Intento {i+1}: {resultado}")
    except Exception as e:
        print(f"Intento {i+1}: Falló después de múltiples intentos: {e}")

"""# Ejercicio 14: Jerarquía de Excepciones Personalizadas
**Dificultad: Avanzada**

**Objetivo**: Diseñar una jerarquía completa de excepciones personalizadas para un framework de aprendizaje por refuerzo.

**Descripción**: Crea una estructura jerárquica de excepciones para diferentes componentes del framework:

1. Estructura básica:
   - `RLError`: Clase base para todas las excepciones del framework
   - `AgentError`: Errores relacionados con agentes
   - `EntornoError`: Errores relacionados con entornos
   - `PoliticaError`: Errores relacionados con políticas
   - `EntrenamientoError`: Errores relacionados con el proceso de entrenamiento

2. Excepciones específicas para cada categoría

3. Función `entrenar_agente_robusto` que utiliza diferentes tipos de excepciones para mostrar
   cómo pueden manejarse de manera diferenciada.

Este enfoque permite un manejo de errores más granular y específico.
"""

# Completa la jerarquía de excepciones para un framework RL

# Excepción base para todo el framework
class RLError(Exception):
    """Clase base para todas las excepciones del framework RL."""
    pass

# ---- Completa las excepciones relacionadas con agentes ----
class AgentError(RLError):
    """Error relacionado con agentes."""
    pass

class AgenteFuncionFaltanteError(AgentError):
    """Error cuando un agente no implementa una función requerida."""
    def __init__(self, funcion):
        self.funcion = funcion
        super().__init__(f"El agente debe implementar la función '{funcion}'")

# ---- Completa las excepciones relacionadas con entornos ----
class EntornoError(RLError):
    """Error relacionado con entornos."""
    pass

class EntornoInterfazError(EntornoError):
    """Error cuando un entorno no implementa la interfaz requerida."""
    pass

# ---- Completa las excepciones relacionadas con políticas ----
class PoliticaError(RLError):
    """Error relacionado con políticas."""
    pass

# ---- Completa las excepciones relacionadas con entrenamiento ----
class EntrenamientoError(RLError):
    """Error relacionado con entrenamiento."""
    pass

class EpisodioError(EntrenamientoError):
    """Error durante un episodio de entrenamiento."""
    pass

# Función de ejemplo que utiliza las excepciones
def entrenar_agente_robusto(agente, entorno, num_episodios):
    """
    Función de ejemplo que usa el sistema de excepciones para un entrenamiento robusto.
    """
    try:
        if not hasattr(agente, 'seleccionar_accion'):
            raise AgenteFuncionFaltanteError("seleccionar_accion")

        if not hasattr(entorno, 'reset') or not hasattr(entorno, 'paso'):
            raise EntornoInterfazError("El entorno debe implementar reset() y paso()")

        resultados = []
        for episodio in range(num_episodios):
            try:
                recompensa_total = ejecutar_episodio(agente, entorno)
                resultados.append(recompensa_total)
            except EpisodioError as e:
                print(f"Error en episodio {episodio}: {e}")
                # Continúa con el siguiente episodio

        return resultados

    except AgentError as e:
        print(f"Error crítico en el agente: {e}")
        return None
    except EntornoError as e:
        print(f"Error crítico en el entorno: {e}")
        return None
    except RLError as e:
        print(f"Error general en el framework: {e}")
        return None
    except Exception as e:
        print(f"Error inesperado: {e}")
        raise  # Re-lanza excepciones no controladas

# Función auxiliar para el ejemplo
def ejecutar_episodio(agente, entorno):
    """Ejecuta un episodio de entrenamiento."""
    estado = entorno.reset()
    recompensa_total = 0
    terminado = False

    while not terminado:
        try:
            accion = agente.seleccionar_accion(estado)
            siguiente_estado, recompensa, terminado, _ = entorno.paso(accion)
            agente.aprender(estado, accion, recompensa, siguiente_estado)
            estado = siguiente_estado
            recompensa_total += recompensa
        except Exception as e:
            # Convertimos la excepción a un tipo específico de nuestro framework
            raise EpisodioError(f"Error durante el episodio: {e}")

    return recompensa_total

# Prueba con algunas excepciones
class AgenteIncompleto:
    # Falta el método seleccionar_accion
    pass

class EntornoIncompleto:
    def reset(self):
        return 0
    # Falta el método paso

# Escenarios de prueba
try:
    entrenar_agente_robusto(AgenteIncompleto(), EntornoIncompleto(), 10)
except Exception as e:
    print(f"Error de prueba: {e}")

"""# Ejercicio 15: Uso Avanzado de Try-Except-Finally
**Dificultad: Avanzada**

**Objetivo**: Implementar un sistema robusto de manejo de errores con try-except-finally para garantizar limpieza de recursos.

**Descripción**: Crea un sistema de guardado de modelos para agentes de RL que use bloques `try-except-finally` para garantizar la persistencia de datos incluso en caso de errores.

La clase `ModelManager` debe:
1. Manejar el guardado seguro de modelos usando archivos temporales
2. Garantizar limpieza de recursos incluso en caso de fallo
3. Implementar manejo específico para diferentes tipos de errores
4. Proporcionar funciones para cargar, listar y eliminar modelos

Este enfoque garantiza que no se pierdan datos ni queden recursos sin liberar, incluso cuando ocurren errores.
"""

import os
import pickle
import time
import random

# Completa la clase ModelManager para gestión robusta de modelos
class ModelManager:
    def __init__(self, directorio="modelos"):
        self.directorio = directorio
        self.archivo_temporal = None

        # Crea el directorio si no existe
        if not os.path.exists(directorio):
            os.makedirs(directorio)

    def guardar_modelo(self, agente, nombre_archivo=None):
        """
        Guarda un modelo de agente de forma segura, con protección contra corrupción.
        Usa try-except-finally para garantizar limpieza de recursos.
        """
        # Tu código aquí: implementa guardado seguro usando try-except-finally
        # Recuerda utilizar un archivo temporal primero, y renombrarlo solo si todo va bien
        pass  # Elimina esta línea al implementar tu solución

    def cargar_modelo(self, nombre_archivo):
        """
        Carga un modelo de agente con manejo de errores.
        Incluye verificación de integridad básica.
        """
        # Tu código aquí: implementa carga segura usando try-except
        pass  # Elimina esta línea al implementar tu solución

    def listar_modelos(self):
        """Lista todos los modelos guardados en el directorio."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def eliminar_modelo(self, nombre_archivo):
        """Elimina un modelo con confirmación de éxito."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

# Agente simple para pruebas
class AgenteDemo:
    def __init__(self, nombre="AgentePrueba", parametros=None):
        self.nombre = nombre
        self.parametros = parametros or {"alpha": 0.1, "gamma": 0.9}
        self.q_table = {str(i): [random.random() for _ in range(4)] for i in range(5)}
        self.timestamp = time.time()

    def __str__(self):
        return f"Agente '{self.nombre}' (timestamp: {self.timestamp})"

# Prueba el sistema de guardado y carga
manager = ModelManager()

# Crea y guarda un agente
agente_original = AgenteDemo("Explorador")
print(f"Agente original: {agente_original}")

# Guarda el modelo
try:
    ruta = manager.guardar_modelo(agente_original, "explorador_v1")
    print(f"Modelo guardado en: {ruta}")
except Exception as e:
    print(f"Error al guardar: {e}")

# Lista modelos disponibles
modelos = manager.listar_modelos()
print(f"Modelos disponibles: {modelos}")

# Carga el modelo
try:
    agente_cargado = manager.cargar_modelo("explorador_v1")
    print(f"Modelo cargado: {agente_cargado}")
except Exception as e:
    print(f"Error al cargar: {e}")

# Simula una corrupción o error durante guardado
try:
    # Forzamos un error durante guardado
    class AgenteProblematico(AgenteDemo):
        def __getstate__(self):
            # Este método se llama durante la serialización con pickle
            raise RuntimeError("¡Error simulado durante serialización!")

    agente_malo = AgenteProblematico("AgenteMalo")
    manager.guardar_modelo(agente_malo, "no_deberia_existir")
except Exception as e:
    print(f"Error esperado capturado: {e}")

# Verificamos que los archivos temporales se hayan limpiado
archivos = os.listdir(manager.directorio)
print(f"Archivos en directorio: {archivos}")

"""# Ejercicio 16: Patrón Singleton para Configuración de Experimentos
**Dificultad: Avanzada**

**Objetivo**: Implementar el patrón de diseño Singleton para mantener una configuración global.

**Descripción**: Crea una clase `ConfigExperimentos` que use el patrón Singleton para mantener una configuración global de experimentos de aprendizaje por refuerzo.

La clase debe:
1. Implementar el patrón Singleton (solo puede existir una instancia)
2. Almacenar parámetros de configuración globales como seed, número de episodios, etc.
3. Permitir registrar y recuperar configuraciones de entornos
4. Proporcionar método para reiniciar la configuración a valores por defecto

Este patrón evita la duplicación de configuraciones y garantiza que todos los componentes
del sistema accedan a los mismos parámetros.
"""

# Completa el Singleton para configuración de experimentos

class ConfigExperimentos:
    """
    Singleton para gestionar la configuración global de experimentos.
    """
    _instancia = None

    def __new__(cls):
        # Implementa el patrón Singleton
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def __init__(self):
        # Solo se ejecuta una vez para la única instancia
        # Valores por defecto
        if not hasattr(self, 'inicializado'):
            self.seed = 42
            self.num_episodios = 1000
            self.max_pasos = 500
            self.directorio_resultados = "./resultados"
            self.hiperparametros = {
                "alpha": 0.1,
                "gamma": 0.99,
                "epsilon": 0.1
            }
            self.entornos_registrados = []
            self.inicializado = True

    def registrar_entorno(self, nombre_entorno, configuracion):
        """Registra un nuevo entorno con su configuración."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def obtener_entorno(self, nombre_entorno):
        """Obtiene la configuración de un entorno registrado."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def reiniciar_configuracion(self):
        """Reinicia la configuración a valores por defecto."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def __str__(self):
        """Representación en texto de la configuración actual."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

# Prueba el Singleton
# 1. Obtenemos la instancia y modificamos valores
config1 = ConfigExperimentos()
config1.seed = 123
config1.hiperparametros["alpha"] = 0.05
config1.registrar_entorno("CartPole", {"version": "v1", "render_mode": "rgb_array"})

# 2. Obtenemos otra "instancia" (que debe ser la misma)
config2 = ConfigExperimentos()
print(f"¿Son el mismo objeto? {config1 is config2}")  # Debe ser True
print(f"Seed en config2: {config2.seed}")  # Debe mostrar 123
print(f"Alpha en config2: {config2.hiperparametros['alpha']}")  # Debe mostrar 0.05

# 3. Verificamos que los entornos registrados son accesibles
entorno = config2.obtener_entorno("CartPole")
print(f"Entorno CartPole: {entorno}")

"""# Ejercicio 17: Patrón Factory para Crear Diferentes Tipos de Entornos
**Dificultad: Avanzada**

**Objetivo**: Implementar el patrón de diseño Factory para crear diferentes tipos de objetos.

**Descripción**: Crea una Factory para generar diferentes tipos de entornos de aprendizaje por refuerzo.

El sistema debe incluir:
1. Clase base abstracta `Entorno` definiendo la interfaz común
2. Implementaciones de diferentes tipos de entornos:
   - `EntornoDiscreto`: Con espacio de estados discreto y finito
   - `EntornoRejilla`: Entorno tipo rejilla 2D
   - `EntornoDinamico`: Entorno con dinámica cambiante
3. `EntornoFactory`: Factory básica para crear entornos a partir de su tipo
4. `RegistroEntornos`: Factory avanzada con registro dinámico de tipos de entornos

Este patrón facilita la creación de diferentes tipos de objetos sin exponer la lógica
de instanciación al cliente.
"""

from abc import ABC, abstractmethod
import random

# Completa la jerarquía de clases de entornos y su factory

# Clase base abstracta para entornos
class Entorno(ABC):
    @abstractmethod
    def reset(self):
        """Reinicia el entorno y retorna el estado inicial."""
        pass

    @abstractmethod
    def paso(self, accion):
        """Ejecuta acción y retorna (siguiente_estado, recompensa, terminado, info)."""
        pass

    @abstractmethod
    def obtener_num_acciones(self):
        """Retorna el número de acciones posibles."""
        pass

    @abstractmethod
    def obtener_nombre(self):
        """Retorna el nombre del entorno."""
        pass

# Implementa diferentes tipos de entornos

class EntornoDiscreto(Entorno):
    """Entorno con espacio de estados discreto y finito."""
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

class EntornoRejilla(Entorno):
    """Entorno tipo rejilla 2D."""
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

class EntornoDinamico(Entorno):
    """Entorno con dinámica cambiante."""
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

# Implementa la factory de entornos

class EntornoFactory:
    """Factory para crear diferentes tipos de entornos."""

    @staticmethod
    def crear_entorno(tipo, **kwargs):
        """
        Crea un entorno del tipo especificado con los parámetros dados.

        Args:
            tipo: Tipo de entorno ("discreto", "rejilla", "dinamico")
            **kwargs: Parámetros específicos para cada tipo de entorno

        Returns:
            Instancia del entorno solicitado
        """
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

# Factory más avanzada con registro dinámico de entornos

class RegistroEntornos:
    """Factory con registro dinámico de tipos de entornos."""

    _entornos_registrados = {}

    @classmethod
    def registrar(cls, nombre, clase_entorno):
        """Registra un nuevo tipo de entorno."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    @classmethod
    def crear_entorno(cls, nombre, **kwargs):
        """Crea un entorno del tipo registrado."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    @classmethod
    def listar_entornos(cls):
        """Lista todos los tipos de entornos registrados."""
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

# Registramos los entornos disponibles
RegistroEntornos.registrar("discreto", EntornoDiscreto)
RegistroEntornos.registrar("rejilla", EntornoRejilla)
RegistroEntornos.registrar("dinamico", EntornoDinamico)

# Prueba la factory simple
entorno1 = EntornoFactory.crear_entorno("discreto", num_estados=10, num_acciones=3)
entorno2 = EntornoFactory.crear_entorno("rejilla", ancho=5, alto=5)

print(f"Entorno 1: {entorno1.obtener_nombre()}, acciones: {entorno1.obtener_num_acciones()}")
print(f"Entorno 2: {entorno2.obtener_nombre()}, acciones: {entorno2.obtener_num_acciones()}")

# Prueba el registro dinámico
entornos_disponibles = RegistroEntornos.listar_entornos()
print(f"Entornos registrados: {entornos_disponibles}")

entorno3 = RegistroEntornos.crear_entorno("dinamico", dificultad="alta")
print(f"Entorno 3: {entorno3.obtener_nombre()}")

# Intenta crear un tipo no registrado
try:
    entorno_invalido = RegistroEntornos.crear_entorno("inexistente")
except Exception as e:
    print(f"Error esperado: {e}")

"""# Ejercicio 18: Patrón Strategy para Políticas de Exploración
**Dificultad: Avanzada**

**Objetivo**: Implementar el patrón de diseño Strategy para crear comportamientos intercambiables.

**Descripción**: Crea un sistema basado en el patrón Strategy para políticas de exploración intercambiables en un agente de aprendizaje por refuerzo.

El sistema debe incluir:
1. Interfaz `EstrategiaExploracion` con métodos comunes
2. Implementaciones concretas de estrategias:
   - `EpsilonGreedy`: Basada en probabilidad epsilon para exploración
   - `Softmax`: Usa distribución de probabilidad basada en valores Q
   - `UCB`: Balance exploración/explotación basado en Upper Confidence Bound
3. Clase `AgenteConEstrategia` que utilice estas estrategias de forma intercambiable

Este patrón permite cambiar algoritmos "en tiempo de ejecución", ideal para experimentar
con diferentes políticas de exploración.
"""

import random
import math
import numpy as np
from abc import ABC, abstractmethod

# Completa la jerarquía de estrategias de exploración

# Interfaz para estrategias de exploración
class EstrategiaExploracion(ABC):
    """Interfaz para implementar diferentes estrategias de exploración."""

    @abstractmethod
    def seleccionar_accion(self, estado, valores_q):
        """
        Selecciona una acción basada en los valores Q y la estrategia.

        Args:
            estado: El estado actual
            valores_q: Lista/array de valores Q para cada acción posible

        Returns:
            Índice de la acción seleccionada
        """
        pass

    @abstractmethod
    def actualizar(self, **kwargs):
        """Actualiza parámetros internos de la estrategia."""
        pass

# Implementa estrategias concretas

class EpsilonGreedy(EstrategiaExploracion):
    """Estrategia epsilon-greedy con decaimiento opcional."""
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

class Softmax(EstrategiaExploracion):
    """Estrategia Softmax/Boltzmann usando distribución de probabilidad."""
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

class UCB(EstrategiaExploracion):
    """Estrategia Upper Confidence Bound para balance exploración/explotación."""
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

# Agente flexible que utiliza una estrategia de exploración
class AgenteConEstrategia:
    """Agente de RL que puede cambiar de estrategia de exploración."""

    def __init__(self, num_acciones, estrategia_exploracion):
        self.num_acciones = num_acciones
        self.q_table = {}  # Tabla de valores Q: {estado: [valores para cada acción]}
        self.estrategia = estrategia_exploracion

    def seleccionar_accion(self, estado):
        """Selecciona una acción usando la estrategia actual."""
        # Convertimos el estado a string para usarlo como clave
        estado_str = str(estado)

        # Si es la primera vez que vemos este estado, inicializamos valores
        if estado_str not in self.q_table:
            self.q_table[estado_str] = [0.0] * self.num_acciones

        # Delegamos la selección a la estrategia de exploración
        return self.estrategia.seleccionar_accion(estado, self.q_table[estado_str])

    def aprender(self, estado, accion, recompensa, siguiente_estado, alpha=0.1, gamma=0.9):
        """Actualiza tabla Q con aprendizaje básico."""
        estado_str = str(estado)
        sig_estado_str = str(siguiente_estado)

        # Inicializamos valores si es necesario
        if estado_str not in self.q_table:
            self.q_table[estado_str] = [0.0] * self.num_acciones
        if sig_estado_str not in self.q_table:
            self.q_table[sig_estado_str] = [0.0] * self.num_acciones

        # Actualización Q-Learning
        q_actual = self.q_table[estado_str][accion]
        max_q_siguiente = max(self.q_table[sig_estado_str])

        # Fórmula Q-Learning
        self.q_table[estado_str][accion] = q_actual + alpha * (
            recompensa + gamma * max_q_siguiente - q_actual)

    def cambiar_estrategia(self, nueva_estrategia):
        """Cambia la estrategia de exploración en tiempo de ejecución."""
        self.estrategia = nueva_estrategia
        return self

    def actualizar_estrategia(self, **kwargs):
        """Actualiza parámetros de la estrategia actual."""
        return self.estrategia.actualizar(**kwargs)

# Prueba el patrón Strategy con las diferentes estrategias
# Simulamos un estado y valores Q para pruebas
estado_prueba = [1, 2, 3]
valores_q_prueba = [0.1, 0.5, 0.3, 0.7]

# Creamos las estrategias
epsilon_greedy = EpsilonGreedy(epsilon=0.3, decaimiento=0.95)
softmax = Softmax(temperatura=1.0)
ucb = UCB(c=2.0)

# Creamos agente con estrategia epsilon-greedy
agente = AgenteConEstrategia(num_acciones=4, estrategia_exploracion=epsilon_greedy)

# Simulamos algunas selecciones con epsilon-greedy
print("Estrategia: Epsilon-Greedy")
for i in range(5):
    accion = agente.seleccionar_accion(estado_prueba)
    print(f"Selección {i+1}: Acción {accion}")
    agente.actualizar_estrategia()  # Aplica decaimiento

# Cambiamos a estrategia Softmax
agente.cambiar_estrategia(softmax)
print("\nEstrategia: Softmax")
for i in range(5):
    accion = agente.seleccionar_accion(estado_prueba)
    print(f"Selección {i+1}: Acción {accion}")
    agente.actualizar_estrategia(temperatura=max(0.1, softmax.temperatura * 0.9))

# Cambiamos a estrategia UCB
agente.cambiar_estrategia(ucb)
print("\nEstrategia: UCB")
for i in range(5):
    accion = agente.seleccionar_accion(estado_prueba)
    print(f"Selección {i+1}: Acción {accion}")

"""# Ejercicio 19: Desarrollo de una Función para Entrenar Agentes de RL
**Dificultad: Avanzada**

**Objetivo**: Integrar diferentes conceptos para crear una función completa de entrenamiento de agentes RL.

**Descripción**: Desarrolla una función completa para entrenar agentes de RL integrando varios conceptos vistos en el módulo:
- Manejo de excepciones
- Parámetros con valores por defecto
- Algoritmos de aprendizaje
- Control de progreso

La función `entrenar_agente` debe:
1. Entrenar un agente en un entorno durante múltiples episodios
2. Gestionar parámetros como tasas de aprendizaje, factores de descuento, etc.
3. Implementar decaimiento de epsilon para exploración
4. Registrar y retornar métricas de progreso
5. Incluir manejo de excepciones para errores durante el entrenamiento
6. Proporcionar evaluaciones periódicas para medir el progreso real

Este tipo de función es el corazón de cualquier sistema de aprendizaje por refuerzo.
"""

import time
import random
import numpy as np
from collections import deque
import matplotlib.pyplot as plt

def entrenar_agente(agente, entorno, num_episodios=500, max_pasos=1000,
                   gamma=0.99, alpha=0.1, epsilon_inicial=1.0, epsilon_final=0.01,
                   decaimiento_epsilon=0.995, intervalo_eval=10, mostrar_progreso=True):
    """
    Función completa para entrenar agentes de aprendizaje por refuerzo.

    Args:
        agente: Objeto agente con métodos seleccionar_accion y aprender
        entorno: Objeto entorno con métodos reset y paso
        num_episodios: Número de episodios de entrenamiento
        max_pasos: Número máximo de pasos por episodio
        gamma: Factor de descuento para el aprendizaje
        alpha: Tasa de aprendizaje
        epsilon_inicial: Valor inicial de epsilon para exploración
        epsilon_final: Valor mínimo de epsilon
        decaimiento_epsilon: Factor de decaimiento de epsilon
        intervalo_eval: Cada cuántos episodios evaluar sin exploración
        mostrar_progreso: Si mostrar información durante el entrenamiento

    Returns:
        dict: Diccionario con resultados del entrenamiento
    """
    # Estructuras para almacenar resultados
    todas_recompensas = []
    recompensas_evaluacion = []
    epsilon = epsilon_inicial
    tiempo_inicio = time.time()

    # Ventana deslizante para promediar recompensas recientes
    ventana_recompensas = deque(maxlen=50)

    # Función para ejecutar un episodio de evaluación (sin exploración)
    def ejecutar_evaluacion():
        estado = entorno.reset()
        recompensa_total = 0
        terminado = False

        for paso in range(max_pasos):
            # Durante evaluación, siempre seleccionamos la mejor acción
            # Tu código aquí
            pass

            if terminado:
                break

        return recompensa_total

    # Bucle principal de entrenamiento
    try:
        for episodio in range(1, num_episodios + 1):
            # Reiniciamos el entorno
            estado = entorno.reset()
            recompensa_episodio = 0
            pasos_episodio = 0
            terminado = False

            # Bucle del episodio
            for paso in range(max_pasos):
                # Seleccionamos acción según la política actual (con exploración)
                # Tu código aquí

                # Ejecutamos la acción en el entorno
                # Tu código aquí

                # El agente aprende de la experiencia
                # Tu código aquí

                # Actualizamos estado y contadores
                # Tu código aquí

                if terminado:
                    break

            # Guardamos estadísticas del episodio
            # Tu código aquí

            # Actualizamos épsilon con decaimiento
            # Tu código aquí

            # Evaluación periódica sin exploración
            if episodio % intervalo_eval == 0:
                # Tu código aquí
                pass

            # Mostramos progreso
            if mostrar_progreso and (episodio % 10 == 0 or episodio == 1):
                # Tu código aquí
                pass

    except KeyboardInterrupt:
        print("\nEntrenamiento interrumpido por el usuario.")
    except Exception as e:
        print(f"\nError durante el entrenamiento: {e}")
        raise

    # Calculamos estadísticas finales
    tiempo_total = time.time() - tiempo_inicio

    # Resultados finales
    resultados = {
        "tiempo_total": tiempo_total,
        "num_episodios": episodio,
        "recompensas": todas_recompensas,
        "recompensas_eval": recompensas_evaluacion,
        "epsilon_final": epsilon,
        "recompensa_mejor": max(ventana_recompensas) if ventana_recompensas else 0,
        "recompensa_promedio": np.mean(ventana_recompensas) if ventana_recompensas else 0
    }

    if mostrar_progreso:
        print(f"\nEntrenamiento completado en {tiempo_total:.2f} segundos")
        print(f"Recompensa promedio final: {resultados['recompensa_promedio']:.2f}")

    return resultados

# Ejemplo de uso
# Estas clases son solo para la demostración, deberían implementarse completamente
class AgenteDemo:
    def seleccionar_accion(self, estado, epsilon=0):
        # Implementación simplificada para demo
        return random.randint(0, 1)

    def aprender(self, estado, accion, recompensa, sig_estado, alpha, gamma):
        # Implementación simplificada para demo
        pass

class EntornoDemo:
    def reset(self):
        return [0, 0]

    def paso(self, accion):
        # Simplificado para demostración
        recompensa = random.random()
        sig_estado = [random.random(), random.random()]
        terminado = random.random() < 0.1
        return sig_estado, recompensa, terminado, {}

# Ejecutamos entrenamiento de demostración
agente_demo = AgenteDemo()
entorno_demo = EntornoDemo()

# Parámetros simplificados para la demo
resultados = entrenar_agente(
    agente_demo,
    entorno_demo,
    num_episodios=50,  # Número reducido para demostración
    max_pasos=50,      # Pasos reducidos para demostración
    intervalo_eval=10,
    mostrar_progreso=True
)

# Visualizamos resultados
plt.figure(figsize=(10, 5))
plt.plot(resultados["recompensas"])
plt.xlabel("Episodios")
plt.ylabel("Recompensa total")
plt.title("Curva de aprendizaje")
plt.grid(True)
plt.show()

"""# Ejercicio 20: Implementación Completa de un Sistema de Agentes y Entornos
**Dificultad: Avanzada**

**Objetivo**: Integrar todos los conceptos aprendidos en un sistema completo de aprendizaje por refuerzo.

**Descripción**: Crea un sistema completo que combine excepciones personalizadas, patrones de diseño,
y programación orientada a objetos para implementar un framework de aprendizaje por refuerzo.

El sistema debe incluir:
1. Jerarquía de excepciones para diferentes componentes
2. Estrategias de exploración usando el patrón Strategy
3. Clases de entorno y agente con interfaces bien definidas
4. Funciones de entrenamiento y evaluación
5. Visualización de resultados

Este ejercicio final integra todos los conceptos del módulo en un solo sistema coherente.
"""

import random
import time
import numpy as np
import matplotlib.pyplot as plt
from abc import ABC, abstractmethod

# ------------- Excepciones Personalizadas -------------
class RLError(Exception):
    """Error base para el framework de RL."""
    pass

class AgentError(RLError):
    """Error relacionado con agentes."""
    pass

class EnvironmentError(RLError):
    """Error relacionado con entornos."""
    pass

# ------------- Estrategias de Exploración (Patrón Strategy) -------------
class ExplorationStrategy(ABC):
    @abstractmethod
    def select_action(self, state, q_values):
        pass

    @abstractmethod
    def update(self, **kwargs):
        pass

class EpsilonGreedy(ExplorationStrategy):
    def __init__(self, epsilon=0.1, decay=0.995, epsilon_min=0.01):
        self.epsilon = epsilon
        self.decay = decay
        self.epsilon_min = epsilon_min

    def select_action(self, state, q_values):
        # Implementa la estrategia epsilon-greedy
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def update(self, **kwargs):
        # Actualiza epsilon con decaimiento
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

# ------------- Interfaces Base -------------
class Environment(ABC):
    @abstractmethod
    def reset(self):
        """Reinicia el entorno y retorna estado inicial."""
        pass

    @abstractmethod
    def step(self, action):
        """Ejecuta acción y retorna (next_state, reward, done, info)."""
        pass

    @property
    @abstractmethod
    def action_space(self):
        """Retorna el espacio de acciones."""
        pass

    @property
    @abstractmethod
    def observation_space(self):
        """Retorna el espacio de observaciones."""
        pass

class Agent(ABC):
    @abstractmethod
    def select_action(self, state):
        """Selecciona acción basada en estado actual."""
        pass

    @abstractmethod
    def learn(self, state, action, reward, next_state, done):
        """Actualiza conocimiento basado en experiencia."""
        pass

    @abstractmethod
    def save(self, path):
        """Guarda el modelo del agente."""
        pass

    @abstractmethod
    def load(self, path):
        """Carga el modelo del agente."""
        pass

# ------------- Implementaciones Concretas -------------

# GridWorld: un entorno de rejilla 2D simple
class GridWorld(Environment):
    def __init__(self, width=5, height=5, max_steps=100):
        self.width = width
        self.height = height
        self.max_steps = max_steps
        self.target_position = (height - 1, width - 1)  # Esquina inferior derecha
        self.agent_position = None
        self.steps = 0
        self._action_space = 4  # Arriba, Derecha, Abajo, Izquierda
        self._observation_space = width * height

    def reset(self):
        # Reinicia entorno y retorna estado inicial
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def step(self, action):
        # Ejecuta acción y retorna resultado
        # Tu código aquí: mueve al agente según la acción
        # y calcula recompensa y estado terminal
        pass  # Elimina esta línea al implementar tu solución

    @property
    def action_space(self):
        return self._action_space

    @property
    def observation_space(self):
        return self._observation_space

    def _get_state(self):
        """Convierte posición 2D a un índice de estado."""
        x, y = self.agent_position
        return x * self.width + y

    def render(self):
        """Visualiza el entorno en consola."""
        # Tu código aquí: muestra la rejilla con agente y objetivo
        pass  # Elimina esta línea al implementar tu solución

# QLearningAgent: agente que implementa el algoritmo Q-Learning
class QLearningAgent(Agent):
    def __init__(self, action_space, observation_space, **kwargs):
        self.action_space = action_space
        self.observation_space = observation_space
        self.q_table = {}

        # Hiperparámetros
        self.alpha = kwargs.get('alpha', 0.1)  # Tasa de aprendizaje
        self.gamma = kwargs.get('gamma', 0.99)  # Factor de descuento

        # Estrategia de exploración (patrón Strategy)
        self.exploration = kwargs.get('exploration',
                                     EpsilonGreedy(epsilon=kwargs.get('epsilon', 0.1)))

    def select_action(self, state):
        # Selecciona acción usando estrategia de exploración
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def learn(self, state, action, reward, next_state, done):
        # Actualiza tabla Q según algoritmo Q-Learning
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def save(self, path):
        # Guarda la tabla Q en un archivo
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

    def load(self, path):
        # Carga la tabla Q desde un archivo
        # Tu código aquí
        pass  # Elimina esta línea al implementar tu solución

# ------------- Funciones de Entrenamiento -------------
def train_agent(agent, env, episodes=1000, max_steps=100,
               verbose=True, eval_interval=100):
    """
    Entrena un agente en un entorno.

    Args:
        agent: Instancia de Agent
        env: Instancia de Environment
        episodes: Número de episodios
        max_steps: Pasos máximos por episodio
        verbose: Si mostrar progreso
        eval_interval: Intervalo para evaluación sin exploración

    Returns:
        dict: Resultados del entrenamiento
    """
    # Implementa la función de entrenamiento completa
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

def evaluate_agent(agent, env, episodes=10, max_steps=100):
    """
    Evalúa un agente sin exploración.

    Args:
        agent: Instancia de Agent
        env: Instancia de Environment
        episodes: Número de episodios
        max_steps: Pasos máximos por episodio

    Returns:
        float: Recompensa promedio
    """
    # Implementa la función de evaluación
    # Tu código aquí
    pass  # Elimina esta línea al implementar tu solución

# ------------- Función Principal -------------
def main():
    # Creación de entorno y agente
    env = GridWorld(width=5, height=5)

    # Estrategia de exploración
    exploration = EpsilonGreedy(epsilon=1.0, decay=0.995, epsilon_min=0.01)

    # Agente
    agent = QLearningAgent(
        action_space=env.action_space,
        observation_space=env.observation_space,
        alpha=0.1,
        gamma=0.99,
        exploration=exploration
    )

    # Entrenamiento
    results = train_agent(agent, env, episodes=500, max_steps=100)

    # Visualización de resultados
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(results['rewards'])
    plt.xlabel('Episodio')
    plt.ylabel('Recompensa total')
    plt.title('Curva de aprendizaje')
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(results['eval_rewards'])
    plt.xlabel('Evaluación')
    plt.ylabel('Recompensa promedio')
    plt.title('Desempeño en evaluación')
    plt.grid(True)

    plt.tight_layout()
    plt.show()

    # Demostración final
    print("\n--- Demostración Final ---")
    for episode in range(3):
        state = env.reset()
        env.render()

        total_reward = 0
        done = False

        while not done:
            action = agent.select_action(state)
            next_state, reward, done, info = env.step(action)
            total_reward += reward

            env.render()
            time.sleep(0.3)  # Pausa para visualización

            state = next_state

        print(f"Episodio {episode+1}: Recompensa total = {total_reward}")

# Ejecución del programa principal
if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"Error en la ejecución: {e}")

"FIN"